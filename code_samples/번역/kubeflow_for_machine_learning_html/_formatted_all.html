<html><head>
  <meta charset="utf-8"><meta name="generator" content="safari-dl">
  <meta charset="utf-8"><meta name="publisher" content="O'Reilly Media, Inc.">
  <link type="text/css" rel="stylesheet" href="css/@fy.css">
  <link type="text/css" rel="stylesheet" href="css/@fy_toc.css">
  <script>
  if(typeof InstallTrigger == 'undefined') {  //load mathjax if not firefox
  let sc = document.createElement('script');
  sc.type = 'text/javascript';
  sc.id = 'MathJax-script';
  sc.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/mml-chtml.js';
  document.head.appendChild(sc);
  }
  </script>
  <title>Kubeflow for Machine Learning</title></head>
  <body>
<section data-type="copyright-page" data-pdf-bookmark="Kubeflow for Machine Learning"><div class="preface" id="idm45831185772360">
<h1>Kubeflow for Machine Learning</h1>

<p class="author">by <span class="firstname">Trevor </span> <span class="surname">Grant</span>, <span class="firstname">Holden </span> <span class="surname">Karau</span>, <span class="firstname">Boris </span> <span class="surname">Lublinsky, <span class="firstname">Richard </span> <span class="surname">Liu</span>, and <span class="firstname">Ilan </span> <span class="surname">Filonenko</span></span></p>

<p class="copyright">Copyright © 2021 Trevor Grant, Holden Karau, Boris Lublinsky, Richard Liu, and Ilan Filonenko. All rights reserved.</p>

<p class="printlocation">Printed in the United States of America.</p>

<p class="publisher">Published by <span class="publishername">O’Reilly Media, Inc.</span>, 1005 Gravenstein Highway North, Sebastopol, CA 95472.</p>

<p>O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (<a href="http://oreilly.com">http://oreilly.com</a>). For more information, contact our corporate/institutional sales department: 800-998-9938 or <span data-type="email"><em>corporate@oreilly.com</em></span>.</p>

<ul class="stafflist">
	<li><span class="staffrole">Acquisitions Editor:</span> Jonathan Hassell</li>
	<li><span class="staffrole">Development Editor:</span> Amelia Blevins</li>
	<li><span class="staffrole">Production Editor:</span> Deborah Baker</li>
	<li><span class="staffrole">Copyeditor:</span> JM Olejarz</li>
	<li><span class="staffrole">Proofreader:</span> Justin Billing</li>
	<li><span class="staffrole">Indexer:</span> Sue Klefstad</li>
	<li><span class="staffrole">Interior Designer:</span> David Futato</li>
	<li><span class="staffrole">Cover Designer:</span> Karen Montgomery</li>
	<li><span class="staffrole">Illustrator:</span> Kate Dullea</li>
</ul>

<ul class="printings">
	<li><span class="printedition">November 2020:</span> First Edition</li>
</ul>
<!--Add additional revdate spans below as needed.-->

<div>
<h1 class="revisions">Revision History for the First Edition</h1>

<ul class="releases">
	<li><span class="revdate">2020-10-12:</span> First Release</li>
</ul>
</div>

<p class="errata">See <a href="http://oreilly.com/catalog/errata.csp?isbn=9781492050124">http://oreilly.com/catalog/errata.csp?isbn=9781492050124</a> for release details.</p>

<div class="legal">
<p>The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. <em>Kubeflow for Machine Learning</em>, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.</p>

<p>The views expressed in this work are those of the authors, and do not represent the publisher’s views. While the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.</p>
</div>

<div class="copyright-bottom">
<p class="isbn">978-1-492-05012-4</p>

<p class="printer">[LSI]</p>
</div>
</div></section>
<section data-type="preface" data-pdf-bookmark="Foreword"><div class="preface" id="_foreword" data-type="preface" title2="Foreword" no2="">
<h1>Foreword</h1>

<p>Occasionally over the years people will ask me what skills are most in demand in tech. Ten years ago I would tell them to study machine learning, which can scale automated decision making in ways previously impossible. However, these days I have a different answer: machine learning engineering.</p>

<p>Even just a few years ago if you knew machine learning and started at an organization, you would likely walk in the door as the only person with that skill set, allowing you to have an outsized impact. However, a side effect of the proliferation of books, tutorials, e-courses, and boot camps (some of which I have written myself) teaching an entire generation of technologists the skills required is that now machine learning is being used across tens of thousands of companies and organizations.</p>

<p>These days a more likely scenario is that, walking into your new job, you find an organization using machine learning locally but unable to deploy it to production or able to deploy models but unable to manage them effectively. In this setting, the most valuable skill is not being able to train a model, but rather to manage all those models and deploy them in ways that maximize their impact.</p>

<p>In this volume, Trevor Grant, Holden Karau, Boris Lublinsky, Richard Liu, and Ilan Filonenko have put together what I believe is an important cornerstone in the education of data scientists and machine learning engineers. 

For the foreseeable future the open source Kubeflow project will be a common tool in an organization’s toolkit for training, management, and deployment of machine learning models. This book represents the codification of a lot of knowledge that previously existed scattered around internal documentation, conference presentations, and blog posts.</p>

<p class="pagebreak-before less_space">If you believe, as I do, that machine learning is only as powerful as how we use it, then this book is for you.</p>

<p class="byline">Chris Albon</p>
<p class="byline cont">Director of Machine Learning,</p>
<p class="byline cont">The Wikimedia Foundation</p>
<p class="byline cont">https://chrisalbon.com</p>

</div></section>
<section data-type="preface" data-pdf-bookmark="Preface"><div class="preface" id="idm45831190160376" data-type="preface" title2="Preface" no2="">
<h1>Preface</h1>


<p>We wrote this book for data engineers and data scientists who are building machine learning systems/models they want to move to production. If you’ve ever had the experience of training an excellent model only to ask yourself how to deploy it into production or keep it up to date once it gets there, this is the book for you.
We hope this gives you the tools to replace <code>Untitled_5.ipynb</code> with something that works relatively reliably in production.</p>

<p>This book is not intended to serve as your first introduction to machine learning. The next section points to some resources that may be useful if you are just getting started on your machine learning journey.</p>






<section data-type="sect1" data-pdf-bookmark="Our Assumption About You"><div class="sect1" id="assumptions_about_the_audience" title2="Our Assumption About You" no2="">
<h1>Our Assumption About You</h1>

<p>This book assumes that you either understand how to train models locally, or are working with someone who does. If neither is true, there are many excellent <a data-type="indexterm" data-primary="ML" data-see="machine learning" id="idm45831189733448"></a><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="beginners’ resources" id="pref-rsrc3"></a><a data-type="indexterm" data-primary="getting started" data-secondary="machine learning" id="pref-rsrc2"></a><a data-type="indexterm" data-primary="beginners’ resources for ML" data-seealso="getting started" id="pref-rsrc"></a>introductory books on machine learning to get you started, including <a class="orm:hideurl" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632"><em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>, 2nd Edition, by Aurélien Géron (O’Reilly)</a>.</p>

<p>Our goal is to teach you how to do machine learning in a repeatable way, and how to automate the training and deployment of your models. A serious problem here is that this goal includes a wide range of topics, and it is more than reasonable that you may not be intimately familiar with all of them.</p>

<p>Since we can’t delve deeply into every topic, we would like to provide you a short list of our favorite primers on several of the topics you will see covered here:</p>
<ul>
<li><p><a class="orm:hideurl" href="https://learning.oreilly.com/library/view/python-for-data/9781491957653"><em>Python for Data Analysis</em></a>, 2nd Edition, by Wes McKinney (O’Reilly)</p></li>
<li><p><a class="orm:hideurl" href="https://learning.oreilly.com/library/view/data-science-from/9781492041122"><em>Data Science from Scratch</em></a>, 2nd Edition, by Joel Grus (O’Reilly)</p></li>
<li><p><a class="orm:hideurl" href="https://www.oreilly.com/library/view/introduction-to-machine/9781449369880"><em>Introduction to Machine Learning with Python</em></a> by Andreas C. Müller and Sarah Guido (O’Reilly)</p></li>
<li><p><a class="orm:hideurl" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632"><em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em></a>, 2nd Edition, by Aurélien Géron (O’Reilly)</p></li>
<li><p><a class="orm:hideurl" href="https://learning.oreilly.com/library/view/kubernetes-up-and/9781492046523"><em>Kubernetes: Up and Running</em></a> by Brendan Burns et al. (O’Reilly)</p></li>
<li><p><a class="orm:hideurl" href="https://learning.oreilly.com/library/view/learning-spark/9781449359034"><em>Learning Spark</em></a> by Holden Karau et al. (O’Reilly)</p></li>
<li><p><a class="orm:hideurl" href="https://learning.oreilly.com/library/view/feature-engineering-for/9781491953235"><em>Feature Engineering for Machine Learning</em></a> by Alice Zheng and Amanda Casari (O’Reilly)</p></li>
<li><p><a class="orm:hideurl" href="https://learning.oreilly.com/library/view/building-machine-learning/9781492053187"><em>Building Machine Learning Pipelines</em></a> by Hannes Hapke and Catherine Nelson (O’Reilly)</p></li>
<li><p><em>Apache Mahout: Beyond MapReduce</em> by Dmitriy Lyubimov and Andrew Palumbo (CreateSpace)</p></li>
<li><p><a class="orm:hideurl" href="https://learning.oreilly.com/library/view/r-cookbook-2nd/9781492040675"><em>R Cookbook</em></a>, 2nd Edition, by J. D. Long and Paul Teetor (O’Reilly)</p></li>
<li><p><a class="orm:hideurl" href="https://www.oreilly.com/library/view/serving-machine-learning/9781492024095"><em>Serving Machine Learning Models</em></a> by Boris Lublinsky (O’Reilly)</p></li>
<li><p><a href="https://oreil.ly/y59_n">“Continuous Delivery for Machine Learning”</a> by Danilo Sato et al.</p></li>
<li><p><a href="https://oreil.ly/hBiw1"><em>Interpretable Machine Learning</em></a> by Christoph Molnar (self-published)</p></li>
<li><p><a href="https://oreil.ly/KnJL0">“A Gentle Introduction to Concept Drift in Machine Learning”</a> by Jason Brownlee</p></li>
<li><p><a href="https://oreil.ly/q9o6P">“Model Drift and Ensuring a Healthy Machine Learning Lifecycle”</a> by A. Besir Kurtulmus</p></li>
<li><p><a href="https://oreil.ly/zvIyU">“The Rise of the Model Servers”</a> by Alex Vikati</p></li>
<li><p><a href="https://oreil.ly/lo36s">“An Overview of Model Explainability in Modern Machine Learning”</a> by Rui Aguiar</p></li>
<li><p><a class="orm:hideurl" href="https://learning.oreilly.com/library/view/machine-learning-with/9781491989371"><em>Machine Learning with Python Cookbook</em></a> by Chris Albon (O’Reilly)</p></li>
<li><p><a href="https://machinelearningflashcards.com">Machine Learning Flashcards</a> by Chris Albon</p></li>
</ul>

<p>Of course, there are many others, but those should get you started. Please don’t be overwhelmed by this list—you certainly don’t need to be an expert in each of these topics to effectively deploy and manage Kubeflow. In fact, Kubeflow exists to streamline many of these tasks. However, there may be some topic into which you wish to delve deeper—and so this should be thought of as a “getting started” list.</p>

<p>Containers and Kubernetes are a wide, rapidly evolving area of practice. <a data-type="indexterm" data-primary="containers" data-secondary="beginners’ resources" id="idm45831190140536"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="beginners’ resources" id="idm45831190139560"></a>If you want to deepen your knowledge of Kubernetes we recommend looking at the following:<a data-type="indexterm" data-startref="pref-rsrc" id="idm45831190138376"></a><a data-type="indexterm" data-startref="pref-rsrc2" id="idm45831190137704"></a><a data-type="indexterm" data-startref="pref-rsrc3" id="idm45831190137032"></a></p>
<ul>
<li><p><a class="orm:hideurl" href="https://learning.oreilly.com/library/view/cloud-native-infrastructure/9781491984291"><em>Cloud Native Infrastructure</em></a> by Justin Garrison and Kris Nova (O’Reilly)</p></li>
<li><p><a class="orm:hideurl" href="https://learning.oreilly.com/library/view/kubernetes-up-and/9781492046523"><em>Kubernetes: Up and Running</em></a> by Brendan Burns et al. (O’Reilly)</p></li>
</ul>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Your Responsibility as a Practitioner"><div class="sect1" id="idm45831190132248" title2="Your Responsibility as a Practitioner" no2="">
<h1>Your Responsibility as a Practitioner</h1>

<p>This book helps you put your machine learning models into production to solve real-world problems. <a data-type="indexterm" data-primary="models" data-secondary="about the impact of" id="idm45831190154680"></a>Solving real-world problems with machine learning is great, but as you go forth and apply your skills, remember to think about the impact.</p>

<p>First, it’s important to make sure your models are sufficiently accurate, and there are great tools for this in Kubeflow, covered in <a data-type="xref" href="#model_management">SECTION 2.2</a>. Even the best tools will not save you from all mistakes—for example, hyperparameter tuning on the same dataset to report final cross-validation results.</p>

<p>Even models with significant predictive power can have <a data-type="indexterm" data-primary="biases of machine learning" id="idm45831190151464"></a><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="biases" id="idm45831190150696"></a>unintended effects and biases that may not show up during the regular training-evaluation phase.
Unintended biases can be hard to discover, but there are many stories (e.g., <a href="https://oreil.ly/VekPG">the Amazon machine learning–based recruiting engine that turned out to have intense biases and decided to hire only men</a>) that demonstrate the profound potential implications of our work. Failing to address these issues early on can lead to having to abandon your entire work, as demonstrated by <a href="https://oreil.ly/WKUXl">IBM’s decision to stop its facial recognition program</a> and similar pauses across the industry after the implications of racial bias in facial recognition in the hands of law enforcement became clear.</p>

<p>Even seemingly unbiased data, like raw purchase records, can turn out to have intense biases resulting in incorrect recommendations or worse.
Just because a dataset is public and widely available does not mean it is unbiased. The well-known practice of <a href="https://oreil.ly/1dmOV">word embeddings</a> has been shown to have many types of bias, including sexism, anti-LGBTQ, and anti-immigrant.
When looking at a new dataset it is crucial to look for examples of bias in your data and attempt to mitigate it as much as possible. With the most popular public datasets, various techniques are often discussed in the research, and you can use these to guide your own work.</p>

<p>While this book does not have the tools to solve bias, we encourage you to think critically about potential biases in your system and explore solutions <em>before going into production</em>.
If you don’t know where to start, check out Katharine Jarmul’s <a href="https://oreil.ly/fiVYL">excellent introductory talk</a>.
IBM has a collection of tools and examples in its <a href="http://aif360.mybluemix.net">AI Fairness 360 open source toolkit</a> that can be a great place to start your exploration.
A critical step to reducing bias in your models is to have a diverse team to notice potential issues early.
As <a href="https://oreil.ly/PJNsF">Jeff Dean</a> said: “AI is full of promise, with the potential to revolutionize so many different areas of modern society. In order to realize its true potential, our field needs to be welcoming to all people. As it stands today, it is definitely not. Our field has a problem with inclusiveness.”</p>
<div data-type="tip"><h6>Tip</h6>
<p>It’s important to note that removing biases or validating accuracy in your results is not a “one and done”; model performance can degrade and biases can be introduced over time—even if you don’t personally change anything.<sup><a data-type="noteref" id="idm45831185818088-marker" href="#idm45831185818088">[1]</a></sup></p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Conventions Used in This Book"><div class="sect1" id="idm45831185817160" title2="Conventions Used in This Book" no2="">
<h1>Conventions Used in This Book</h1>

<p>The following typographical conventions are used in this book:</p>
<dl>
<dt><em>Italic</em></dt>
<dd>
<p>Indicates new terms, URLs, email addresses, filenames, and file extensions.</p>
</dd>
<dt><code>Constant width</code></dt>
<dd>
<p>Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.</p>
</dd>
<dt><strong><code>Constant width bold</code></strong></dt>
<dd>
<p>Shows commands or other text that should be typed literally by the user.</p>
</dd>
<dt><em><code>Constant width italic</code></em></dt>
<dd>
<p>Shows text that should be replaced with user-supplied values or by values determined by context.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>This element signifies a tip or suggestion.</p>
</div>
<div data-type="note"><h6>Note</h6>
<p>This element signifies a general note.</p>
</div>
<div data-type="warning"><h6>Warning</h6>
<p>This element indicates a warning or caution.</p>
</div>

<p>We will use warnings to indicate any situations where the resulting pipeline is likely to be nonportable and call out portable alternatives that you can use.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Code Examples"><div class="sect1" id="idm45831185804568" title2="Code Examples" no2="">
<h1>Code Examples</h1>

<p>Supplemental material (code examples, etc.) is available for download at <a href="https://oreil.ly/Kubeflow_for_ML"><em class="hyperlink">https://oreil.ly/Kubeflow_for_ML</em></a>. <a data-type="indexterm" data-primary="resources for Kubeflow" data-secondary="code examples for download" id="idm45831189726056"></a><a data-type="indexterm" data-primary="code examples in book" data-secondary="download link" id="idm45831189725048"></a>These code examples are available under an Apache 2 license, or as described in the next section.</p>

<p>There are additional examples under their own respective licenses that you may find useful.
The <a data-type="indexterm" data-primary="Kubeflow" data-secondary="Apache 2 license" id="idm45831189723368"></a><a data-type="indexterm" data-primary="Canonical resources" id="idm45831189722392"></a><a data-type="indexterm" data-primary="online resources" data-see="resources" id="idm45831189721720"></a>Kubeflow project has an <a href="https://oreil.ly/yslNT">example repo</a>, which at the time of writing is available under an Apache 2 license.
Canonical also has  <a href="https://oreil.ly/TOt_E">a set of resources</a> that may be of special interest to MicroK8s users.</p>








<section data-type="sect2" data-pdf-bookmark="Using Code Examples"><div class="sect2" id="using_code_examples" title2="Using Code Examples" no2="">
<h2>Using Code Examples</h2>

<p>If you have a technical question or a problem using the code examples, please send email to <a class="email" href="mailto:bookquestions@oreilly.com"><em>bookquestions@oreilly.com</em></a>.</p>

<p>This book is here to help you get your job done. In general, if example code is offered with this book, you may <a data-type="indexterm" data-primary="code examples in book" data-secondary="permission for use" id="idm45831189715352"></a>use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product’s documentation does require 
permission.</p>

<p>Additional details on license can be found in the repos.</p>

<p>We appreciate, but generally do not require, attribution. An attribution usually includes the <a data-type="indexterm" data-primary="attribution for code examples" id="idm45831189712136"></a>title, author, publisher, and ISBN. For example: “<em>Kubeflow for Machine Learning</em> by Holden Karau, Trevor Grant, Boris Lublinsky, Richard Liu, and Ilan Filonenko (O’Reilly). Copyright 2021 Holden Karau, Trevor Grant, Boris Lublinsky, Richard Liu, and Ilan Filonenko, 978-1-492-05012-4.”</p>

<p>If you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at <a class="email" href="mailto:permissions@oreilly.com"><em>permissions@oreilly.com</em></a>.</p>
</div></section>





</div></section>













<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="O’Reilly Online Learning"><div class="sect1" id="idm45831189708680" title2="O’Reilly Online Learning" no2="">
<h1>O’Reilly Online Learning</h1>
<div data-type="note" class="ormenabled"><h6>Note</h6>
<p>For more than 40 years, <a href="http://oreilly.com" class="orm:hideurl"><em class="hyperlink">O’Reilly Media</em></a> has provided technology and business training, knowledge, and insight to help companies succeed.</p>
</div>

<p>Our unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and 200+ other publishers. For more information, visit <a href="http://oreilly.com" class="orm:hideurl"><em>http://oreilly.com</em></a>.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="How to Contact the Authors"><div class="sect1" id="idm45831190065160" title2="How to Contact the Authors" no2="">
<h1>How to Contact the Authors</h1>

<p>For feedback, email us at <a class="email" href="mailto:intro-to-ml-kubeflow@googlegroups.com"><em>intro-to-ml-kubeflow@googlegroups.com</em></a>. <a data-type="indexterm" data-primary="author contact information" id="idm45831190062504"></a>For random ramblings, occasionally about Kubeflow, follow us online:</p>
<dl>
<dt>Trevor</dt>
<dd>

<ul>
<li>
<p><a href="https://twitter.com/rawkintrevo">Twitter</a></p>
</li>
<li>
<p><a href="https://rawkintrevo.org">Blog</a></p>
</li>
<li>
<p><a href="https://github.com/rawkintrevo">GitHub</a></p>
</li>
<li>
<p><a href="https://myspace.com/rawkintrevo">Myspace</a></p>
</li>
</ul>
</dd>
<dt>Holden</dt>
<dd>

<ul>
<li>
<p><a href="http://twitter.com/holdenkarau">Twitter</a></p>
</li>
<li>
<p><a href="https://www.youtube.com/user/holdenkarau">YouTube</a></p>
</li>
<li>
<p><a href="https://www.twitch.tv/holdenkarau">Twitch</a></p>
</li>
<li>
<p><a href="https://www.linkedin.com/in/holdenkarau">LinkedIn</a></p>
</li>
<li>
<p><a href="http://blog.holdenkarau.com">Blog</a></p>
</li>
<li>
<p><a href="https://github.com/holdenk">GitHub</a></p>
</li>
<li>
<p><a href="https://www.facebook.com/hkarau">Facebook</a></p>
</li>
</ul>
</dd>
<dt>Boris</dt>
<dd>

<ul>
<li>
<p><a href="https://www.linkedin.com/in/boris-lublinsky-b6a4a/">LinkedIn</a></p>
</li>
<li>
<p><a href="https://github.com/blublinsky">GitHub</a></p>
</li>
</ul>
</dd>
<dt>Richard</dt>
<dd>

<ul>
<li>
<p><a href="https://github.com/richardsliu">GitHub</a></p>
</li>
</ul>
</dd>
</dl>
<dl class="less_space pagebreak-before">
<dt>Ilan</dt>
<dd>

<ul>
<li>
<p><a href="https://www.linkedin.com/in/ifilonenko">LinkedIn</a></p>
</li>
<li>
<p><a href="https://github.com/ifilonenko">GitHub</a></p>
</li>
</ul>
</dd>
</dl>
</div></section>













<section data-type="sect1" data-pdf-bookmark="How to Contact Us"><div class="sect1" id="idm45831190089432" title2="How to Contact Us" no2="">
<h1>How to Contact Us</h1>

<p>Please address comments and questions concerning this book to the publisher:</p>
<ul class="simplelist">
  <li>O’Reilly Media, Inc.</li>
  <li>1005 Gravenstein Highway North</li>
  <li>Sebastopol, CA 95472</li>
  <li>800-998-9938 (in the United States or Canada)</li>
  <li>707-829-0515 (international or local)</li>
  <li>707-829-0104 (fax)</li>
</ul>

<p>You can access the web page for this book, where we list errata, examples, and any additional information, at <a href="https://oreil.ly/Kubeflow_for_Machine_Learning"><em class="hyperlink">https://oreil.ly/Kubeflow_for_Machine_Learning</em></a>.</p>

<p>Email <a class="email" href="mailto:bookquestions@oreilly.com"><em>bookquestions@oreilly.com</em></a> to comment or ask technical questions about this book.</p>

<p>For news and information about our books and courses, visit <a href="http://oreilly.com"><em class="hyperlink">http://oreilly.com</em></a>.</p>

<p>Find us on Facebook: <a href="http://facebook.com/oreilly"><em class="hyperlink">http://facebook.com/oreilly</em></a></p>

<p>Follow us on Twitter: <a href="http://twitter.com/oreillymedia"><em class="hyperlink">http://twitter.com/oreillymedia</em></a></p>

<p>Watch us on YouTube: <a href="http://www.youtube.com/oreillymedia"><em class="hyperlink">http://www.youtube.com/oreillymedia</em></a></p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Acknowledgments"><div class="sect1" id="idm45831190075080" title2="Acknowledgments" no2="">
<h1>Acknowledgments</h1>

<p>The authors would like to thank everyone at O’Reilly Media, especially our editors Amelia Blevins and Deborah Baker, as well as the Kubeflow community for making this book possible.
Clive Cox and Alejandro Saucedo from <a href="https://www.seldon.io">Seldon</a> made amazing contributions to <a data-type="xref" href="#inference_ch">CHAPTER 8</a>, without which this book would be missing key parts.
We’d like to thank Google Cloud Platform for resources that allowed us to ensure examples worked on GCP.
Perhaps most importantly, we’d like to thank our reviewers, without whom this book would not exist in its current form. This includes Taka Shinagawa, Pete MacKinnon, Kevin Haas, Chris Albon, Hannes Hapke, and more. To all early readers and reviewers of books, thank you for your contributions.</p>
<dl class="less_space pagebreak-before">
<dt>Holden</dt>
<dd>
<p>Would like to thank her girlfriend Kris Nóva for her help debugging her first Kubeflow PR,
as well as the entire Kubeflow community for being so welcoming.
She would also like to thank her wife Carolyn DeSimone, her puppy Timbit DeSimone-Karau (pictured in <a data-type="xref" href="#timbit_pic">FIGURE P-1</a>), and her stuffed animals for the support needed to write.
She would like to thank the doctors at SF General and UCSF for fixing up her hands so she could finish writing this book (although she does wish the hands did not hurt anymore) and everyone who came to visit her in the hospital and nursing home.
A special thank you to Ann Spencer, the first editor who showed her how to have fun writing.
Finally, she would like to thank her datefriend Els van Vessem for their support in recovering after her accident, especially reading stories and reminding her of her love of writing.</p>
</dd>
</dl>

<figure><div id="timbit_pic" class="figure" data-type="figure" title2="Timbit the dog" no2="P-1">
<img src="assets/kfml_0001.png" alt="Timbit" width="650" height="445">
<h6>Figure P-1. Timbit the dog</h6>
</div></figure>
<dl>
<dt>Ilan</dt>
<dd>
<p>Would like to thank all his colleagues at Bloomberg who took the time to review, mentor, and encourage him to write and contribute to open source.
The list includes but is not limited to: Kimberly Stoddard, Dan Sun, Keith Laban, Steven Bower, and Sudarshan Kadambi.
He would also like to thank his family—Galia, Yuriy, and Stan—for their unconditional love and support.</p>
</dd>
<dt>Richard</dt>
<dd>
<p>Would like to thank the Google Kubeflow team, including but not limited to: Jeremy Lewi, Abhishek Gupta, Thea Lamkin, Zhenghui Wang, Kunming Qu, Gabriel Wen, Michelle Casbon, and Sarah Maddox—without whose support none of this would have been possible. He would also like to thank his cat Tina (see <a data-type="xref" href="#tina_pic">FIGURE P-2</a>) for her support and understanding during COVID-19.</p>
</dd>
</dl>

<figure><div id="tina_pic" class="figure" data-type="figure" title2="Tina the cat" no2="P-2">
<img src="assets/kfml_0002.png" alt="Tina" width="825" height="1161">
<h6>Figure P-2. Tina the cat</h6>
</div></figure>
<dl>
<dt>Boris</dt>
<dd>
<p>Would like to thank his colleagues at Lightbend, especially Karl Wehden, for their support in writing the book,
their suggestions and proofreads of the early versions of the text, and his wife Marina for putting up with his long hours and feeding him during these hours.</p>
</dd>
<dt>Trevor</dt>
<dd>
<p>Trevor would like to thank his office mates Apache and Meowska (see <a data-type="xref" href="#apache_meowska">FIGURE P-3</a>) for reminding him of the importance of naps, and everyone who listened to him give a talk on Kubeflow last year (especially the people who listened to the bad versions, and especially especially people who listened to the bad versions but still are reading this book now—you’re the best). He’d also like to thank his mom, sister, and brother for tolerating his various shenanigans over the years.</p>
</dd>
</dl>

<figure><div id="apache_meowska" class="figure" data-type="figure" title2="Apache and Meowska" no2="P-3">
<img src="assets/kfml_0003.png" alt="Apache and Meowska" width="450" height="292">
<h6>Figure P-3. Apache and Meowska</h6>
</div></figure>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Grievances"><div class="sect1" id="idm45831188267096" title2="Grievances" no2="">
<h1>Grievances</h1>

<p>The authors would also like to acknowledge the struggles of API changes, which made writing this book so frustrating. If you ever struggle with API changes, know that you are not alone; they are annoying to almost everyone.</p>

<p>Holden would also like to acknowledge the times Timbit DeSimone-Karau was a little sh*t and dug up the yard while she was working. We have a special grievance to vent with the person who hit Holden with their car, slowing down the release of this book.</p>

<p>Trevor has a grievance to air with his girlfriend, who has been badgering him (with increasing persistence) to propose to her throughout this entire project, and while he has been “working on it”—if he hasn’t asked her to marry him by the time this book comes out: <strong>Katie, will you marry me?</strong></p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45831185818088"><sup><a href="#idm45831185818088-marker">[1]</a></sup> Remember the Twitter bot that through reinforcement learning became a neo-Nazi in less than a weekend?</p></div></div></section>
<section data-type="chapter" class="pagenumrestart" data-pdf-bookmark="Chapter 1. Kubeflow: What It Is and Who It Is For"><div class="chapter" id="who_is_kubeflow_for_ch" data-type="chapter" title2="Kubeflow: What It Is and Who It Is For" no2="1">
<h1>Chapter 1. Kubeflow: What It Is and Who It Is For</h1>


<p>If you are a data scientist trying to get your models into production, or a data engineer trying to make your models scalable and reliable, <a data-type="indexterm" data-primary="Kubeflow" data-secondary="about" id="idm45831188261320"></a>Kubeflow provides tools to help.
Kubeflow solves the problem of how to take machine learning from research to production.
Despite common misconceptions, Kubeflow is more than just Kubernetes and TensorFlow—you can use it for all sorts of machine learning tasks.
We hope Kubeflow is the right tool for you, as long as your organization is using Kubernetes.
<a data-type="xref" href="#brief_alternatives_to_kubeflow">SECTION 1.6</a> introduces some options you may wish to explore.</p>

<p>This chapter aims to help you decide if Kubeflow is the right tool for your use case.
We’ll cover the benefits you can expect from Kubeflow, some of the costs associated with it, and some of the alternatives.
After this chapter, we’ll dive into setting up Kubeflow and building an end-to-end solution to familiarize you with the basics.</p>






<section data-type="sect1" data-pdf-bookmark="Model Development Life Cycle"><div class="sect1" id="idm45831188258120" title2="Model Development Life Cycle" no2="1.1">
<h1>1.1. Model Development Life Cycle</h1>

<p>Machine learning or model development essentially follows the path: <a data-type="indexterm" data-primary="model development life cycle (MDLC)" id="idm45831188256824"></a>data →  information →  knowledge →  insight. This path of generating insight from data can be graphically described with <a data-type="xref" href="#mdlc_figure">FIGURE 1-1</a>.</p>

<p><em>Model development life cycle</em> (MDLC) is a term commonly used to describe the flow between training and inference. <a data-type="xref" href="#mdlc_figure">FIGURE 1-1</a> is a visual representation of this continuous interaction, where upon triggering a model update the whole cycle kicks off yet again.</p>

<figure><div id="mdlc_figure" class="figure" data-type="figure" title2="Model development life cycle" no2="1-1">
<img src="assets/kfml_0101.png" alt="Model Development Life-Cycle" width="1126" height="633">
<h6>Figure 1-1. Model development life cycle</h6>
</div></figure>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Where Does Kubeflow Fit In?"><div class="sect1" id="idm45831188251080" title2="Where Does Kubeflow Fit In?" no2="1.2">
<h1>1.2. Where Does Kubeflow Fit In?</h1>

<p>Kubeflow is a collection of cloud native tools for all of the stages of MDLC (data exploration, feature preparation, model training/tuning, model serving, model testing, and model versioning).
Kubeflow also has tooling that allows these traditionally separate tools to work seamlessly together.
An important part of this tooling is the pipeline system, which allows users to build integrated end-to-end pipelines that connect all components of their MDLC.</p>

<p>Kubeflow is for both data scientists and data engineers looking to build production-grade machine learning implementations.
Kubeflow can be run either locally in your development environment or on a production cluster.
Often pipelines will be developed locally and migrated once the pipelines are ready.
Kubeflow provides a unified system—leveraging Kubernetes for containerization and scalability, for the portability and repeatability of its pipelines.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Why Containerize?"><div class="sect1" id="idm45831188248120" title2="Why Containerize?" no2="1.3">
<h1>1.3. Why Containerize?</h1>

<p>The isolation provided by containers allows machine learning stages to be portable and reproducible. <a data-type="indexterm" data-primary="containers" data-secondary="why containerize" id="idm45831188246440"></a><a data-type="indexterm" data-primary="containers" data-secondary="resource about" id="idm45831188245464"></a>Containerized applications are isolated from the rest of your machine and have all their requirements included (from the operating system up).<sup><a data-type="noteref" id="idm45831188244232-marker" href="#idm45831188244232">[1]</a></sup>
Containerization means no more conversations that include “It worked on my machine” or “Oh yeah, we forgot about just one, you need this extra package.”</p>

<p>Containers are built in composable layers, allowing you to use another container as a base.<a data-type="indexterm" data-primary="containers" data-secondary="about" id="idm45831188242120"></a>
For example, if you have a new <a data-type="indexterm" data-primary="natural language processing (NLP)" id="idm45831188241000"></a>natural language processing (NLP) library you want to use, you can add it on top of the existing container—you don’t have to start from scratch each time.
The composability allows you to reuse a common base; for example, the R and Python containers we use both share a base Debian container.</p>

<p>A common worry about using containers is the overhead. The <a data-type="indexterm" data-primary="containers" data-secondary="overhead" id="idm45831190376136"></a>overhead of containers depends on your implementation, but a paper from IBM<sup><a data-type="noteref" id="idm45831190375064-marker" href="#idm45831190375064">[2]</a></sup> found the overhead to be quite low, and generally faster than virtualization.
With Kubeflow, there is some additional <a data-type="indexterm" data-primary="Kubeflow" data-secondary="overhead" id="idm45831190374024"></a>overhead of having operators installed that you may not use. This overhead is negligible on a production cluster but may be noticeable on a laptop.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Data scientists with Python experience can think of containers as a heavy-duty virtual environment. In addition to what you’re used to in a virtual environment, containers also include the operating system, the packages, and everything in between.</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Why Kubernetes?"><div class="sect1" id="idm45831190371288" title2="Why Kubernetes?" no2="1.4">
<h1>1.4. Why Kubernetes?</h1>

<p><a href="https://kubernetes.io">Kubernetes</a> is an open source system for automating the <a data-type="indexterm" data-primary="Kubernetes" data-secondary="about" id="idm45831190369448"></a>deployment, scaling, and management of containerized applications. It allows our pipelines to be scalable without sacrificing portability, enabling us to avoid becoming locked into a specific cloud provider.<sup><a data-type="noteref" id="idm45831190368120-marker" href="#idm45831190368120">[3]</a></sup>
In addition to being able to switch from a single machine to a distributed cluster, different stages of your machine learning pipeline can request different amounts or types of resources. For example, your data preparation step may benefit more from running on multiple machines, while your model training may benefit more from computing on top of GPUs or tensor processing units (TPUs).
This flexibility is especially useful in cloud environments, where you can reduce your costs by using expensive resources only when required.</p>

<p>You can, of course, build your own containerized machine learning pipelines on Kubernetes without using Kubeflow; <a data-type="indexterm" data-primary="Kubeflow" data-secondary="about" id="idm45831190365464"></a>however the goal of Kubeflow is to standardize this process and make it substantially easier and more efficient.<sup><a data-type="noteref" id="idm45831190364232-marker" href="#idm45831190364232">[4]</a></sup>
Kubeflow provides a common interface over the tools you would likely use for your machine learning implementations. It also makes it easier to configure your implementations to use hardware accelerators like TPUs without changing your code.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Kubeflow’s Design and Core Components"><div class="sect1" id="breif_kubeflow_design_and_core_components" title2="Kubeflow’s Design and Core Components" no2="1.5">
<h1>1.5. Kubeflow’s Design and Core Components</h1>

<p>In the machine learning landscape, there exists a diverse selection of libraries, tool sets, and frameworks.<a data-type="indexterm" data-primary="Kubeflow" data-secondary="core components" data-seealso="components" id="ch01-comp"></a><a data-type="indexterm" data-primary="components" data-secondary="about" id="ch01-comp2"></a>
Kubeflow does not seek to reinvent the wheel or provide a “one size fits all” solution—instead, it allows machine learning practitioners to compose and customize their own stacks based on specific needs. It is designed to simplify the process of building and deploying machine learning systems at scale. This allows data scientists to focus their energies on model development instead of infrastructure.</p>

<p>Kubeflow seeks to tackle the problem of simplifying machine learning through three features: composability, portability, and scalability.</p>
<dl>
<dt>Composability</dt>
<dd>
<p>The core components of Kubeflow <a data-type="indexterm" data-primary="composability of Kubeflow" id="idm45831190354584"></a>come from data science tools that are already familiar to machine learning practitioners. They can be used independently to facilitate specific stages of machine learning, or composed together to form end-to-end pipelines.</p>
</dd>
<dt>Portability</dt>
<dd>
<p>By having a container-based design and taking advantage of <a data-type="indexterm" data-primary="portability of Kubeflow" data-secondary="Kubernetes foundation" id="idm45831190352168"></a>Kubernetes and its cloud native architecture, Kubeflow does not require you to anchor to any particular developer environment. You can experiment and prototype on your laptop, and deploy to production effortlessly.</p>
</dd>
<dt>Scalability </dt>
<dd>
<p>By using Kubernetes, Kubeflow can dynamically <a data-type="indexterm" data-primary="scalability of Kubeflow" data-secondary="Kubernetes foundation" id="idm45831190349480"></a>scale according to the demand on your cluster, by changing the number and size of the underlying containers and machines.<sup><a data-type="noteref" id="idm45831190348232-marker" href="#idm45831190348232">[5]</a></sup></p>
</dd>
</dl>

<p>These features are critical for different parts of MDLC.
Scalability is important as your dataset grows. Portability is important to avoid vendor lock-in. Composability gives you the freedom to mix and match the best tools for the job.</p>

<p>Let’s take a quick look at some of Kubeflow’s components and how they support these features.</p>








<section data-type="sect2" data-pdf-bookmark="Data Exploration with Notebooks"><div class="sect2" id="idm45831190346120" title2="Data Exploration with Notebooks" no2="1.5.1">
<h2>1.5.1. Data Exploration with Notebooks</h2>

<p>MDLC always begins with data exploration—plotting,<a data-type="indexterm" data-primary="model development life cycle (MDLC)" id="ch01-mdlc"></a><a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="Kubeflow support for" id="idm45831190343768"></a><a data-type="indexterm" data-primary="notebooks" data-see="Jupyter notebooks" id="idm45831190342824"></a> segmenting, and manipulating your data to understand where possible insight might exist. One powerful tool that provides the tools and environment for such data exploration is Jupyter.
Jupyter is an open source web application that allows users to create and share data, code snippets, and experiments. Jupyter is popular among machine learning practitioners due to its simplicity and portability.</p>

<p>In Kubeflow, you can spin up instances of Jupyter that directly interact with your cluster and its other components, as shown in <a data-type="xref" href="#ch1_jupyter_notebook">FIGURE 1-2</a>. For example, you can write snippets of TensorFlow distributed training code on your laptop, and bring up a training cluster with just a few clicks.</p>

<figure><div id="ch1_jupyter_notebook" class="figure" data-type="figure" title2="Jupyter notebook running in Kubeflow" no2="1-2">
<img src="assets/kfml_0102.png" alt="A Jupyter notebook running in Kubeflow" width="696" height="451">
<h6>Figure 1-2. Jupyter notebook running in Kubeflow</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Data/Feature Preparation"><div class="sect2" id="idm45831190337608" title2="Data/Feature Preparation" no2="1.5.2">
<h2>1.5.2. Data/Feature Preparation</h2>

<p>Machine learning algorithms require good data to be effective, and often special <a data-type="indexterm" data-primary="data" data-secondary="preparation of" id="idm45831190336024"></a><a data-type="indexterm" data-primary="Kubeflow" data-secondary="dataset tools" id="idm45831190335048"></a>tools are needed to effectively extract, transform, and load data. One typically filters, normalizes, and prepares one’s input data in order to extract insightful features from otherwise unstructured, noisy data. Kubeflow <a data-type="indexterm" data-primary="Apache Spark" data-secondary="using with Kubeflow" id="idm45831190333736"></a><a data-type="indexterm" data-primary="TensorFlow Transform (TFT)" data-secondary="Kubeflow support for" id="idm45831190332792"></a>supports a few different tools for this:</p>

<ul>
<li>
<p>Apache Spark (one of the most popular big data tools)</p>
</li>
<li>
<p>TensorFlow Transform (integrated with TensorFlow Serving for easier inference)</p>
</li>
</ul>

<p>These distinct data preparation components can handle a variety of formats and data sizes and are designed to play nicely with your data exploration environment.<sup><a data-type="noteref" id="idm45831189959752-marker" href="#idm45831189959752">[6]</a></sup></p>
<div data-type="note"><h6>Note</h6>
<p>Support for Apache Beam with Apache Flink in Kubeflow Pipelines is an area of active development.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Training"><div class="sect2" id="idm45831189956888" title2="Training" no2="1.5.3">
<h2>1.5.3. Training</h2>

<p>Once your features are prepped, you are ready to build and train your model.<a data-type="indexterm" data-primary="Kubeflow" data-secondary="training frameworks" id="idm45831189955560"></a><a data-type="indexterm" data-primary="training" data-secondary="frameworks supported" id="idm45831189954488"></a>
Kubeflow supports a variety of distributed training frameworks. As of the time of writing, Kubeflow has support for:</p>

<ul>
<li>
<p><a href="https://www.tensorflow.org">TensorFlow</a></p>
</li>
<li>
<p><a href="https://pytorch.org">PyTorch</a></p>
</li>
<li>
<p><a href="https://mxnet.apache.org">Apache MXNet</a></p>
</li>
<li>
<p><a href="https://github.com/dmlc/xgboost">XGBoost</a></p>
</li>
<li>
<p><a href="https://chainer.org">Chainer</a></p>
</li>
<li>
<p><a href="https://caffe2.ai">Caffe2</a></p>
</li>
<li>
<p><a href="https://oreil.ly/0zln4">Message passing interface (MPI)</a></p>
</li>
</ul>

<p>In <a data-type="xref" href="#tf_ch">CHAPTER 7</a> we will examine how Kubeflow trains a TensorFlow model in greater detail and <a data-type="xref" href="#beyond_tf">CHAPTER 9</a> will explore other options.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Hyperparameter Tuning"><div class="sect2" id="idm45831189941304" title2="Hyperparameter Tuning" no2="1.5.4">
<h2>1.5.4. Hyperparameter Tuning</h2>

<p>How do you optimize your model architecture and training? In machine learning, <a data-type="indexterm" data-primary="Kubeflow" data-secondary="hyperparameter tuning" data-seealso="hyperparameters" id="idm45831189940008"></a><a data-type="indexterm" data-primary="hyperparameters" data-secondary="definition" id="idm45831189938760"></a><a data-type="indexterm" data-primary="hyperparameters" data-secondary="tuning supported by Kubeflow" id="idm45831189937816"></a>hyperparameters are variables that govern the training process. For example, what should the model’s learning rate be? How many hidden layers and neurons should be in the neural network? These parameters are not part of the training data, but they can have a significant effect on the performance of the training models.</p>

<p>With Kubeflow, users can begin with a training model that they are unsure about, define the hyperparameter search space, and Kubeflow will take care of the rest—spin up training jobs using different hyperparameters, collect the metrics, and save the results to a model database so their performance can be compared.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Model Validation"><div class="sect2" id="idm45831189935368" title2="Model Validation" no2="1.5.5">
<h2>1.5.5. Model Validation</h2>

<p>Before you put your model into production, it’s important to know how it’s likely to perform.<a data-type="indexterm" data-primary="models" data-secondary="validation" id="idm45831189933928"></a><a data-type="indexterm" data-primary="validation of models" data-secondary="Kubeflow support for" id="idm45831189932952"></a>
The same tool used for hyperparameter tuning can perform cross-validation for model validation.
When you’re updating existing models, techniques like A/B testing and multi-armed bandit can be used in model inference to validate your model online.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Inference/Prediction"><div class="sect2" id="idm45831189931352" title2="Inference/Prediction" no2="1.5.6">
<h2>1.5.6. Inference/Prediction</h2>

<p>After training your model, the next step is to serve the model in your cluster so it can handle prediction requests.<a data-type="indexterm" data-primary="model inference" data-secondary="Kubeflow model inference" id="idm45831189929928"></a><a data-type="indexterm" data-primary="prediction" data-see="model inference" id="idm45831189928936"></a>
Kubeflow makes it easy for data scientists to deploy machine learning models in production environments at scale.
Currently Kubeflow provides a multiframework component for model serving (KFServing), in addition to existing solutions like TensorFlow Serving and Seldon Core.</p>

<p>Serving many types of models on Kubeflow is fairly straightforward. In most situations, there is no need to build or customize a container yourself—simply point Kubeflow to where your model is stored, and a server will be ready to service requests.</p>

<p>Once the model is served, it needs to be monitored for performance and possibly updated. This monitoring and updating is possible via the cloud native design of Kubeflow and will be further expanded upon in<a data-type="indexterm" data-startref="ch01-mdlc" id="idm45831189926280"></a> <a data-type="xref" href="#inference_ch">CHAPTER 8</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Pipelines"><div class="sect2" id="idm45831189924360" title2="Pipelines" no2="1.5.7">
<h2>1.5.7. Pipelines</h2>

<p>Now that we have completed all aspects of MDLC, we wish to enable reusability and governance of these experiments. <a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="about" id="idm45831189922904"></a>To do this, Kubeflow treats MDLC as a machine learning pipeline and implements it as a graph, where each node is a stage in a workflow, as seen in <a data-type="xref" href="#ch1_kubeflow_pipeline">FIGURE 1-3</a>. Kubeflow Pipelines is a component that allows users to compose reusable workflows at ease. Its features include:</p>

<ul>
<li>
<p>An orchestration engine for multistep workflows</p>
</li>
<li>
<p>An SDK to interact with pipeline components</p>
</li>
<li>
<p>A user interface that allows users to visualize and track experiments, and to share results with collaborators</p>
</li>
</ul>

<figure><div id="ch1_kubeflow_pipeline" class="figure" data-type="figure" title2="A Kubeflow pipeline" no2="1-3">
<img src="assets/kfml_0103.png" alt="A Kubeflow pipeline" width="600" height="665">
<h6>Figure 1-3. A Kubeflow pipeline</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Component Overview"><div class="sect2" id="idm45831189915032" title2="Component Overview" no2="1.5.8">
<h2>1.5.8. Component Overview</h2>

<p>As you can see, Kubeflow has built-in components for all parts of MDLC: data preparation, feature preparation, model training, data exploration, hyperparameter tuning, and model inference, as well as pipelines to coordinate everything. However, you are not limited to just the components shipped as part of Kubeflow. You can build on top of the components or even replace them. This can be OK for occasional components, but if you find yourself wanting to replace many parts of Kubeflow, you may want to explore some of the alternatives available.<a data-type="indexterm" data-startref="ch01-comp" id="idm45831189913112"></a><a data-type="indexterm" data-startref="ch01-comp2" id="idm45831189912408"></a></p>
</div></section>





</div></section>













<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Alternatives to Kubeflow"><div class="sect1" id="brief_alternatives_to_kubeflow" title2="Alternatives to Kubeflow" no2="1.6">
<h1>1.6. Alternatives to Kubeflow</h1>

<p>Within the research community, various alternatives exist that provide uniquely different functionality to that of Kubeflow.<a data-type="indexterm" data-primary="Kubeflow" data-secondary="alternatives to" id="ch01-alt"></a>
Most recent research has focused around model development and training, with large improvements being made in infrastructure, theory, and systems.</p>

<p>Prediction and model serving, on the other hand, have received relatively less attention. As such, data science practitioners often end up hacking together an amalgam of critical systems components that are integrated to support serving and inference across various workloads and continuously evolving frameworks.</p>

<p>Given the demand for constant availability and horizontal scalability, solutions like Kubeflow and various others are gaining traction throughout the industry, as powerful architectural abstraction tools, and as convincing research scopes.</p>








<section data-type="sect2" data-pdf-bookmark="Clipper (RiseLabs)"><div class="sect2" id="idm45831189906328" title2="Clipper (RiseLabs)" no2="1.6.1">
<h2>1.6.1. Clipper (RiseLabs)</h2>

<p>One interesting alternative to Kubeflow is Clipper,<a data-type="indexterm" data-primary="Clipper" id="idm45831189904760"></a> a general-purpose low-latency prediction serving system developed by
RiseLabs. In an attempt to simplify deployment, optimization, and inference, Clipper has a layered architecture system. Through various optimizations and its modular design, Clipper, achieves low latency and high-throughput predictions at levels comparable to TensorFlow Serving, on three TensorFlow models of varying inference costs.</p>

<p>Clipper is divided across two abstractions, aptly named <em>model selection</em> and <em>model abstraction</em> layers. The model selection layer is quite sophisticated in that it uses an adaptive online model selection policy and various ensemble techniques. Since the model is continuously learning from feedback throughout the lifetime of the application, the model selection layer self-calibrates failed models without needing to interact directly with the policy layer.</p>

<p>Clipper’s modular architecture and focus on containerization, similar to Kubeflow, enables caching and batching mechanisms to be shared across frameworks while also reaping the benefits of scalability, concurrency, and flexibility in adding new model frameworks.</p>

<p>Graduating from theory into a functional end-to-end system, Clipper has gained traction within the scientific community and has had various parts of its architectural designs incorporated into recently introduced machine learning systems. Nonetheless, we have yet to see if it will be adopted in the industry at scale.</p>
</div></section>













<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="MLflow (Databricks)"><div class="sect2" id="idm45831189900296" title2="MLflow (Databricks)" no2="1.6.2">
<h2>1.6.2. MLflow (Databricks)</h2>

<p>MLflow was developed by Databricks as an open source machine learning development platform.<a data-type="indexterm" data-primary="MLflow (Databricks)" data-secondary="about" id="idm45831189898440"></a><a data-type="indexterm" data-primary="Databricks MLflow" data-secondary="about" id="idm45831189897464"></a>
The architecture of MLflow leverages a lot of the same architectural paradigms as Clipper, including its framework-agnostic nature, while focusing on three major components that it calls Tracking, Projects, and Models.</p>

<p>MLflow Tracking functions as an API with a complementing UI for logging parameters, code versions, metrics, and output files. This is quite powerful in machine learning as tracking parameters, metrics, and artifacts is of paramount importance.</p>

<p>MLflow Projects provides a standard format for packaging reusable data science code, defined by a YAML file that can leverage source-controlled code and dependency management via Anaconda. The project format makes it easy to share reproducible data science code, as reproducibility is critical for machine learning practitioners.</p>

<p>MLflow Models are a convention for packaging machine learning models in multiple formats.
Each MLflow Model is saved as a directory containing arbitrary files and an MLmodel descriptor file. MLflow also provides the model’s registry, showing lineage between deployed models and their creation metadata.</p>

<p>Like Kubeflow, MLflow is still in active development, and has an active community.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Others"><div class="sect2" id="idm45831189893464" title2="Others" no2="1.6.3">
<h2>1.6.3. Others</h2>

<p>Because of the challenges presented in machine learning development, many organizations have started to build internal platforms to manage their machine learning life cycle. <a data-type="indexterm" data-primary="resources for Kubeflow" data-secondary="alternatives" id="idm45831189891944"></a>For example: Bloomberg, Facebook, Google, Uber, and IBM have built, respectively, the Data Science Platform, FBLearner Flow, TensorFlow Extended, Michelangelo, and Watson Studio to manage data preparation, model training, and deployment.<sup><a data-type="noteref" id="idm45831189890584-marker" href="#idm45831189890584">[7]</a></sup></p>

<p>With the machine learning infrastructure landscape always evolving and maturing, we are excited to see how open source projects, like Kubeflow, will bring much-needed simplicity and abstraction to machine learning development.<a data-type="indexterm" data-startref="ch01-alt" id="idm45831189887880"></a></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Introducing Our Case Studies"><div class="sect1" id="case_studies" title2="Introducing Our Case Studies" no2="1.7">
<h1>1.7. Introducing Our Case Studies</h1>

<p>Machine learning can use many different types of data, and the approaches and tools you use may vary. In order to showcase Kubeflow’s capabilities, we’ve chosen case studies with very different data and best practices.
When possible, we will use data from these case studies to explore Kubeflow and some of its components.</p>








<section data-type="sect2" data-pdf-bookmark="Modified National Institute of Standards and Technology"><div class="sect2" id="idm45831185502792" title2="Modified National Institute of Standards and Technology" no2="1.7.1">
<h2>1.7.1. Modified National Institute of Standards and Technology</h2>

<p>In ML, Modified National Institute of Standards and Technology (MNIST) <a data-type="indexterm" data-primary="MNIST (Modified National Institute of Standards and Technology)" data-secondary="about" id="idm45831185501496"></a><a data-type="indexterm" data-primary="Modified National Institute of Standards and Technology" data-see="MNIST" id="idm45831185500392"></a>commonly refers to the dataset of handwritten digits for classification. The relatively small data size of digits, as well as its common use as an example, allows us to explore a variety of tools.
In some ways, MNIST has become one of the standard “hello world” examples for machine learning. We use MNIST as our first example in <a data-type="xref" href="#simple_training_ch">CHAPTER 2</a> to illustrate Kubeflow end-to-end.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Mailing List Data"><div class="sect2" id="idm45831185497768" title2="Mailing List Data" no2="1.7.2">
<h2>1.7.2. Mailing List Data</h2>

<p>Knowing how to ask good questions is something of an art. Have you ever posted a message to a mailing list, asking for help, only for no one to respond? What are the different types of questions?<a data-type="indexterm" data-primary="Apache Software Foundation" data-see="mailing list data preparation" id="idm45831185496264"></a><a data-type="indexterm" data-primary="mailing list data preparation" data-secondary="about mailing list data" id="idm45831185495208"></a>
We’ll look at some of the public Apache Software Foundation mailing list data and try to create a model that predicts if a message will be answered. This example is scaled up and down by choosing which projects and what time period we want to look at, so we can use a variety of tools to solve it.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Product Recommender"><div class="sect2" id="idm45831185493544" title2="Product Recommender" no2="1.7.3">
<h2>1.7.3. Product Recommender</h2>

<p>Recommendation systems are one of the most common and easily understood <a data-type="indexterm" data-primary="recommendation systems" data-secondary="about" id="idm45831185492248"></a><a data-type="indexterm" data-primary="product recommender" data-see="recommendation systems" id="idm45831185491272"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="recommender" data-tertiary="about" id="idm45831185490328"></a>applications of machine learning, with many examples from Amazon’s product recommender to Netflix’s movie suggestions. The majority of recommender implementations are based on collaborative filtering—an assumption that if person A has the same opinion as person B on a set of issues, A would be more likely to share B’s opinion on other issues than would a randomly chosen third person. This approach is built on a well-developed algorithm with quite a few implementations, including TensorFlow/Keras implementation.<sup><a data-type="noteref" id="idm45831185488440-marker" href="#idm45831185488440">[8]</a></sup></p>

<p>One of the problems with rating-based models is that they can’t be standardized easily for data with nonscaled target values, such as the purchase or frequency data. This <a href="https://oreil.ly/LncEo">excellent Medium post</a> shows how to convert such data into a rating matrix that can be used for collaborative filtering. Our example leverages <a href="https://oreil.ly/LncEo">data and code from Data Driven Investor</a> and code described on <a href="https://oreil.ly/p3TB_">Piyushdharkar’s GitHub</a>.
We’ll use this example to explore how to build an initial model in Jupyter and move on to building a production 
pipeline.</p>
</div></section>













<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="CT Scans"><div class="sect2" id="idm45831185483304" title2="CT Scans" no2="1.7.4">
<h2>1.7.4. CT Scans</h2>

<p>As we were writing this book, the world was going through the COVID-19 pandemic. <a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="about data" id="idm45831185481640"></a><a data-type="indexterm" data-primary="COVID-19 pandemic" id="idm45831185480664"></a><a data-type="indexterm" data-primary="denoising data" data-secondary="about CT scan data" id="idm45831185479992"></a><a data-type="indexterm" data-primary="data cleaning" data-secondary="about CT scan data" id="idm45831185479048"></a>AI researchers were being called on to
apply methods and techniques to assist medical providers with understanding the disease. Some research
showed that CT scans were more effective at early detection than RT-PCR tests (the traditional COVID test). However, diagnostic
CT scans use low dosages of radiation and are therefore “noisy”—that is to say, CT scans are more clear when more radiation
is used.</p>

<p>A <a href="https://oreil.ly/OXrFs">new paper proposes</a> an open source solution for denoising CT scans with off-the-shelf methods available entirely from open
source projects (as opposed to proprietary FDA-approved solutions). We implement this approach to illustrate how one might
go from academic article to real-world solution, to show the value of Kubeflow for creating reproducible and sharable
research, and to provide a starting off point for any reader who might want to contribute to the fight against COVID-19.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45831185475704" title2="Conclusion" no2="1.8">
<h1>1.8. Conclusion</h1>

<p>We are so glad you’ve decided to use this book to start your adventures into Kubeflow.
This introduction should have given you a feel for Kubeflow and its capabilities.
However, like all adventures, there may come a point when your guidebook isn’t enough to carry you through.
Thankfully, there is a collection of community resources where you can interact with others on similar paths.<a data-type="indexterm" data-primary="Kubeflow" data-secondary="online community" id="idm45831185473752"></a><a data-type="indexterm" data-primary="resources for Kubeflow" data-secondary="online community" id="idm45831185472776"></a><a data-type="indexterm" data-primary="online community for Kubeflow" id="idm45831185471832"></a><a data-type="indexterm" data-primary="Kubeflow Slack workspace" id="idm45831185471192"></a>
We encourage you to sign up for the <a href="http://kubeflow.slack.com">Kubeflow Slack workspace</a>, one
of the more active areas of discussion.
There is also a <a href="https://oreil.ly/Ca6R3">Kubeflow discussion mailing list</a>.
There is a <a href="https://www.kubeflow.org">Kubeflow project page</a> as well.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you want to quickly explore Kubeflow end-to-end,<a data-type="indexterm" data-primary="Google codelabs" id="idm45831185467256"></a> there are some
<a href="https://oreil.ly/YRfkm">Google codelabs</a> that can help you.</p>
</div>

<p>In <a data-type="xref" href="#simple_training_ch">CHAPTER 2</a>, we’ll install Kubeflow and use it to train and serve a relatively simple machine learning
model to give you an idea of the basics.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45831188244232"><sup><a href="#idm45831188244232-marker">[1]</a></sup> For more on containers, see <a href="">this Google cloud resource</a>. In situations with GPUs or TPUs, the details of isolation become more complicated.</p><p data-type="footnote" id="idm45831190375064"><sup><a href="#idm45831190375064-marker">[2]</a></sup> W. Felter et al., “An Updated Performance Comparison of Virtual Machines and Linux Containers,” 2015 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), March 29-31, 2015, doi: 10.1109/ISPASS.2015.7095802.</p><p data-type="footnote" id="idm45831190368120"><sup><a href="#idm45831190368120-marker">[3]</a></sup> Kubernetes does this by providing a container orchestration layer. For more information about Kubernetes, check out <a href="">its documentation</a>.</p><p data-type="footnote" id="idm45831190364232"><sup><a href="#idm45831190364232-marker">[4]</a></sup> Spotify was able to increase the rate of experiments ~7x; see this  <a href="">Spotify Engineering blog post</a>.</p><p data-type="footnote" id="idm45831190348232"><sup><a href="#idm45831190348232-marker">[5]</a></sup> Local clusters like Minikube are limited to one machine, but most cloud clusters can dynamically change the kind and number of machines as needed.</p><p data-type="footnote" id="idm45831189959752"><sup><a href="#idm45831189959752-marker">[6]</a></sup> There is still some setup work to make this function, which we cover in <a data-type="xref" href="#data_and_feature_prep">CHAPTER 5</a>.</p><p data-type="footnote" id="idm45831189890584"><sup><a href="#idm45831189890584-marker">[7]</a></sup> If you want to explore more of these tools, two good overviews are  <a href="">Ian Hellstrom’s 2020 blog post</a> and <a href="">this 2019 article by Austin Kodra</a>.</p><p data-type="footnote" id="idm45831185488440"><sup><a href="#idm45831185488440-marker">[8]</a></sup> For example, see the <a href="">Piyushdharkar’s GitHub</a>.</p></div></div></section>
<section data-type="chapter" data-pdf-bookmark="Chapter 2. Hello Kubeflow"><div class="chapter" id="simple_training_ch" data-type="chapter" title2="Hello Kubeflow" no2="2">
<h1>Chapter 2. Hello Kubeflow</h1>


<p>Welcome to your first steps into the exciting world of Kubeflow!</p>

<p>First off, we’ll set up Kubeflow on your machine, or on a cloud provider.
Then we’ll dive into a comprehensive example.
The goal of this example is to get a model trained and start serving as quickly as possible.
In some parts of the first section, it may seem like we are instructing you to mindlessly enter commands. While we want
you to follow along, we strongly encourage you to revisit this chapter after you’ve finished the book to reflect on the
commands you entered, and consider how much your understanding has grown while reading.</p>

<p>We’ll provide instructions for setting up and testing our example on a local machine and a link to instructions for performing the same on real clusters.
While we will point you to the config files and OCI containers that are driving all of this, they are not the focus of
this chapter; they will be covered in detail in subsequent chapters. The focus of this chapter is an end-to-end example that you can follow along with at home.</p>

<p>In future chapters we will dig into the “why” of everything we’re doing, we promise.</p>

<p>For now, just enjoy the ride.</p>






<section data-type="sect1" data-pdf-bookmark="Getting Set Up with Kubeflow"><div class="sect1" id="idm45831185459048" title2="Getting Set Up with Kubeflow" no2="2.1">
<h1>2.1. Getting Set Up with Kubeflow</h1>

<p>One of the great things about Kubeflow being built with Kubernetes is the ability to do our <a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="about" id="idm45831185457400"></a><a data-type="indexterm" data-primary="Kubeflow" data-secondary="local to distributed with ease" id="idm45831185456424"></a><a data-type="indexterm" data-primary="Kubeflow" data-secondary="installing" id="ch02-inst2"></a><a data-type="indexterm" data-primary="getting started" data-secondary="installing Kubeflow" id="ch02-inst"></a>initial development and exploration locally, moving into more powerful and distributed tools later on. Your same pipeline can be developed locally and moved into a cluster.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Though you could get started with Kubeflow locally, you don’t have to. You can just as easily do your initial work with
one of the cloud providers or on-premises Kubernetes clusters.</p>

<p>One of the faster ways to get started with Kubeflow is using the<a data-type="indexterm" data-primary="Google Cloud Platform (GCP)" data-secondary="click-to-deploy Kubeflow app" id="idm45831185451064"></a><a data-type="indexterm" data-primary="resources for Kubeflow" data-secondary="click-to-deploy Kubeflow app" id="idm45831185449976"></a><a data-type="indexterm" data-primary="deployment of Kubeflow" data-secondary="click-to-deploy on Google Cloud" id="idm45831185449016"></a> click-to-deploy app on Google Cloud Platform (GCP).
If you’re in a rush to get started, go ahead and check out <a href="https://oreil.ly/GBbsc">this Kubeflow documentation page</a>.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Installing Kubeflow and Its Dependencies"><div class="sect2" id="general_set_up_kubeflow" title2="Installing Kubeflow and Its Dependencies" no2="2.1.1">
<h2>2.1.1. Installing Kubeflow and Its Dependencies</h2>

<p>Before we approach the biggest requirement for Kubeflow, access to a Kubernetes cluster, let’s get the tools set up.
Kubeflow is fairly self-contained but does require <code>kubectl</code>.
The rest of the dependencies are inside containers, so you don’t have to worry about installing them.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Whether you use a local or a remote Kubernetes cluster, having the development tools installed locally will simplify your life.</p>
</div>

<p>Regardless of your cluster, you need to install Kubeflow’s core dependency <code>kubectl</code>, for communicating <a data-type="indexterm" data-primary="Kubernetes" data-secondary="installing Kubeflow" id="idm45831185441688"></a><a data-type="indexterm" data-primary="kubectl" data-secondary="installing" id="idm45831185440680"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="kubectl" data-tertiary="installing" id="idm45831185439736"></a>with Kubernetes.
<code>kubectl</code> is widely packaged, with the different installation options covered in the <a href="https://oreil.ly/tUpe0">Kubernetes documentation</a>.
If you want to use a package manager to install <code>kubectl</code>, Ubuntu users can use snap (see <a data-type="xref" href="#install_kubectl_snap">EXAMPLE 2-1</a>) and Mac users can use Homebrew (see <a data-type="xref" href="#install_kubectl_homebrew">EXAMPLE 2-2</a>); other installation options are covered in the <a href="https://oreil.ly/vQPYQ">Kubernetes documentation</a>. <code>kubectl</code> can also be installed as a local binary from this <a href="https://oreil.ly/iT5Pv">Kubernetes documentation page</a>.</p>
<div id="install_kubectl_snap" data-type="example" title2="Install kubectl with snap" no2="2-1">
<h5>Example 2-1. Install kubectl with snap</h5>

<pre data-type="programlisting" data-code-language="bash">sudo snap install kubectl --classic</pre></div>
<div id="install_kubectl_homebrew" data-type="example" title2="Install kubectl with Homebrew" no2="2-2">
<h5>Example 2-2. Install kubectl with Homebrew</h5>

<pre data-type="programlisting" data-code-language="bash">brew install kubernetes-cli</pre></div>

<p>Once you have the minimum dependencies installed, you can now install Kubeflow from <a href="https://oreil.ly/WTHLZ">this GitHub repo</a>, as in <a data-type="xref" href="#install_kf">EXAMPLE 2-3</a>.</p>
<div id="install_kf" data-type="example" class="less_space pagebreak-before" title2="Install Kubeflow" no2="2-3">
<h5>Example 2-3. Install Kubeflow</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">PLATFORM</code><code class="o">=</code><code class="k">$(</code>uname<code class="k">)</code> <code class="c"># Either Linux or Darwin</code>
<code class="nb">export </code>PLATFORM
mkdir -p ~/bin
<code class="c">#Configuration</code>
<code class="nb">export </code><code class="nv">KUBEFLOW_TAG</code><code class="o">=</code>1.0.1
<code class="c"># ^ You can also point this to a different version if you want to try</code>
<code class="nv">KUBEFLOW_BASE</code><code class="o">=</code><code class="s2">"https://api.github.com/repos/kubeflow/kfctl/releases"</code>
<code class="c"># Or just go to https://github.com/kubeflow/kfctl/releases</code>
<code class="nv">KFCTL_URL</code><code class="o">=</code><code class="k">$(</code>curl -s <code class="si">${</code><code class="nv">KUBEFLOW_BASE</code><code class="si">}</code> <code class="p">|</code><code class="se">\</code>
	      grep http <code class="p">|</code><code class="se">\</code>
	      grep <code class="s2">"</code><code class="si">${</code><code class="nv">KUBEFLOW_TAG</code><code class="si">}</code><code class="s2">"</code> <code class="p">|</code><code class="se">\</code>
	      grep -i <code class="s2">"</code><code class="si">${</code><code class="nv">PLATFORM</code><code class="si">}</code><code class="s2">"</code> <code class="p">|</code><code class="se">\</code>
	      cut -d : -f 2,3 <code class="p">|</code><code class="se">\</code>
	      tr -d <code class="s1">'\" '</code> <code class="k">)</code>
wget <code class="s2">"</code><code class="si">${</code><code class="nv">KFCTL_URL</code><code class="si">}</code><code class="s2">"</code>
<code class="nv">KFCTL_FILE</code><code class="o">=</code><code class="si">${</code><code class="nv">KFCTL_URL</code><code class="p">##*/</code><code class="si">}</code>
tar -xvf <code class="s2">"</code><code class="si">${</code><code class="nv">KFCTL_FILE</code><code class="si">}</code><code class="s2">"</code>
mv ./kfctl ~/bin/
rm <code class="s2">"</code><code class="si">${</code><code class="nv">KFCTL_FILE</code><code class="si">}</code><code class="s2">"</code>
<code class="c"># It's recommended that you add the scripts directory to your path</code>
<code class="nb">export </code><code class="nv">PATH</code><code class="o">=</code><code class="nv">$PATH</code>:~/bin</pre></div>

<p>You should now have Kubeflow installed on your machine.
To make sure it’s installed, run <code>kfctl version</code> and check that it returns the expected version.<a data-type="indexterm" data-startref="ch02-inst" id="idm45831190037640"></a><a data-type="indexterm" data-startref="ch02-inst2" id="idm45831190037032"></a>
Now let’s cover some optional tools that you can install to ease your future Kubeflowing.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Setting Up Local Kubernetes"><div class="sect2" id="idm45831185446424" title2="Setting Up Local Kubernetes" no2="2.1.2">
<h2>2.1.2. Setting Up Local Kubernetes</h2>

<p>Being able to have the same software running locally and in production is one of the great advantages of Kubeflow.<a data-type="indexterm" data-primary="Minikube" data-secondary="local installation of Kubeflow" id="idm45831181001016"></a><a data-type="indexterm" data-primary="Kubeflow" data-secondary="local installation" id="idm45831181000024"></a><a data-type="indexterm" data-primary="Minikube" data-secondary="about" id="idm45831180999080"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="local cluster via Minikube" id="idm45831180998136"></a>
To support this, you will need a local version of Kubernetes installed.
While there are several options, we find Minikube the simplest.
Minikube is a local version of Kubernetes that allows you to use your local computer to simulate a cluster.
Two other common options for a local version of Kubeflow are <code>microk8s</code>, supported on many Linux platforms, and <code>MiniKF</code>, which uses Vagrant to launch a VM to run Kubernetes with Kubeflow.</p>
<div data-type="tip"><h6>Tip</h6>
<p>A local Kubernetes cluster is not strictly required, but many data scientists and developers find it helpful to have a local cluster to test with.</p>
</div>










<section data-type="sect3" data-pdf-bookmark="Minikube"><div class="sect3" id="setting_up_minikube" title2="Minikube" no2="2.1.2.1">
<h3>2.1.2.1. Minikube</h3>

<p>Minikube is a local version of Kubernetes that can run Kubeflow. There are installation guides for Minikube on the<a data-type="indexterm" data-primary="Minikube" data-secondary="resources online" id="idm45831180992600"></a>  <a href="https://oreil.ly/lNeon">main Kubernetes documentation page</a> as well as the <a href="https://oreil.ly/B17Wp">Kubeflow-specific page</a>.</p>

<p>The most common failure in the automatic setup of Minikube is missing a hypervisor or Docker.<a data-type="indexterm" data-primary="Docker" data-secondary="installing Minikube" id="idm45831180989560"></a> Regardless of your OS, you should be able to use <a href="https://oreil.ly/h1uoS">VirtualBox</a>; however, other options like KVM2 on Linux, Hyper-V on Windows, and HyperKit on macOS all work as well.</p>
<div data-type="tip"><h6>Tip</h6>
<p>When starting Minikube make sure to give it plenty of memory and disk space,<a data-type="indexterm" data-primary="Minikube" data-secondary="memory for" id="idm45831180986664"></a><a data-type="indexterm" data-primary="disk space for Minikube" id="idm45831180973320"></a><a data-type="indexterm" data-primary="storage" data-secondary="Minikube requirements" id="idm45831180972712"></a> e.g., <code>minikube start --cpus 16 --memory 12g --disk-size 15g</code>. Note: you don’t need 16 CPU cores to run this; this is just the number of virtual CPUs Minikube will use.</p>
</div>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Setting Up Your Kubeflow Development Environment"><div class="sect2" id="idm45831180970840" title2="Setting Up Your Kubeflow Development Environment" no2="2.1.3">
<h2>2.1.3. Setting Up Your Kubeflow Development Environment</h2>

<p>Kubeflow’s pipeline system is built in Python, and having the SDK installed locally will allow you to build pipelines faster.<a data-type="indexterm" data-primary="Kubeflow" data-secondary="installing development environment" id="idm45831180969320"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="SDK installation" id="idm45831180968472"></a><a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="Kubeflow support for" id="idm45831180967624"></a>
However, if you can’t install software locally, you can still use Kubeflow’s Jupyter environment to build your pipelines.</p>










<section data-type="sect3" data-pdf-bookmark="Setting up the Pipeline SDK"><div class="sect3" id="install_pipeline_sdk" title2="Setting up the Pipeline SDK" no2="2.1.3.1">
<h3>2.1.3.1. Setting up the Pipeline SDK</h3>

<p>To begin setting up the Pipeline SDK you will need to have <a href="https://oreil.ly/IbfY2">Python</a> installed.
Many people find it useful to create isolated<a data-type="indexterm" data-primary="Python" data-secondary="installing" id="idm45831180964136"></a><a data-type="indexterm" data-primary="Python" data-secondary="virtual environments for projects" id="idm45831180963288"></a><a data-type="indexterm" data-primary="virtual environments in Python" id="idm45831180962440"></a> virtual environments for their different projects; see how in <a data-type="xref" href="#make_venv">EXAMPLE 2-4</a>.</p>
<div id="make_venv" data-type="example" title2="Create a virtual environment" no2="2-4">
<h5>Example 2-4. Create a virtual environment</h5>

<pre data-type="programlisting" data-code-language="bash">virtualenv kfvenv --python python3
<code class="nb">source </code>kfvenv/bin/activate</pre></div>

<p>Now you can use the pip command to install the Kubeflow Pipelines package and its requirements, as in <a data-type="xref" href="#install_sdk">EXAMPLE 2-5</a>.</p>
<div id="install_sdk" data-type="example" title2="Install Kubeflow Pipeline SDK" no2="2-5">
<h5>Example 2-5. Install Kubeflow Pipeline SDK</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">URL</code><code class="o">=</code>https://storage.googleapis.com/ml-pipeline/release/latest/kfp.tar.gz
pip install <code class="s2">"</code><code class="si">${</code><code class="nv">URL</code><code class="si">}</code><code class="s2">"</code> --upgrade</pre></div>

<p>If you use a virtual environment you will need to activate it whenever you want to use the Pipeline SDK.</p>

<p>In addition to the SDK, Kubeflow ships a number of components. Checking out a fixed version of the standard components, as in <a data-type="xref" href="#install_kf_pl_sdk">EXAMPLE 2-6</a>, allows us to create more reliable pipelines.</p>
<div id="install_kf_pl_sdk" data-type="example" title2="Clone the Kubeflow Pipelines repo" no2="2-6">
<h5>Example 2-6. Clone the Kubeflow Pipelines repo</h5>

<pre data-type="programlisting" data-code-language="bash">  git clone --single-branch --branch 0.3.0 https://github.com/kubeflow/pipelines.git</pre></div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Setting up Docker"><div class="sect3" id="idm45831180966072" title2="Setting up Docker" no2="2.1.3.2">
<h3>2.1.3.2. Setting up Docker</h3>

<p><a href="https://www.docker.com">Docker</a> is an important part of the minimum requirements, allowing you to customize<a data-type="indexterm" data-primary="Docker" data-secondary="installing" id="idm45831180946472"></a>
and add libraries and other functionality to your own custom containers. We’ll cover more on Docker in <a data-type="xref" href="#kubeflow_design_beyond_basics">CHAPTER 3</a>. Docker can be installed from the standard package managers in Linux or with Homebrew on macOS.</p>

<p>In addition to installing Docker, you will want a place to store the container images, called a container registry.<a data-type="indexterm" data-primary="Docker" data-secondary="container registry" id="idm45831180916280"></a><a data-type="indexterm" data-primary="containers" data-secondary="container registry" id="idm45831180915304"></a><a data-type="indexterm" data-primary="container registry" id="idm45831180914360"></a>
The container registry will be accessed by your Kubeflow cluster. The company behind Docker offers
<a href="https://hub.docker.com">Docker Hub</a> and RedHat offers <a href="https://quay.io">Quay</a>, a cloud neutral platform you can use.
Alternatively, you can also use your cloud provider’s container registry.<sup><a data-type="noteref" id="idm45831180911944-marker" href="#idm45831180911944">[1]</a></sup>
A cloud vendor’s specific container registry often offers greater security on images stored there and can configure your Kubernetes cluster automatically with the permissions required to fetch those images. In our examples, we’ll assume that you’ve set your container registry
via an environment variable 
$CONTAINER_REGISTRY, in your shell.</p>
<div data-type="tip"><h6>Tip</h6>
<p>If you use a registry that isn’t on the Google Cloud Platform, you will need to configure Kubeflow Pipelines container builder to have access to your registry by following the <a href="https://oreil.ly/88Ep-">Kaniko configuration guide</a>.</p>
</div>

<p>To make sure your Docker installation is properly configured, you can write a one-line <code>Dc</code> and push it to your<a data-type="indexterm" data-primary="containers" data-secondary="tag for pushing" id="idm45831180906648"></a><a data-type="indexterm" data-primary="tag for pushing containers" id="idm45831180905640"></a><a data-type="indexterm" data-primary="testing" data-secondary="Docker installation" id="idm45831180905000"></a>
registry. For the <code>Dockerfile</code> we’ll use the <code>FROM</code> command to indicate we are based on top of Kubeflow’s TensorFlow
notebook container image, as in <a data-type="xref" href="#trivial_docker">EXAMPLE 2-7</a> (we’ll talk more about this in <a data-type="xref" href="#beyond_tf">CHAPTER 9</a>). When you push a container, you need to specify the <code>tag</code>, which determines the image name, version, and where it is stored—as shown in <a data-type="xref" href="#trivial_build_and_push">EXAMPLE 2-8</a>.</p>
<div id="trivial_docker" data-type="example" title2="Specify the new container is built on top of Kubeflow’s container" no2="2-7">
<h5>Example 2-7. Specify the new container is built on top of Kubeflow’s container</h5>

<pre data-type="programlisting" data-code-language="dockerfile"><code class="k">FROM</code><code class="s"> gcr.io/kubeflow-images-public/tensorflow-2.1.0-notebook-cpu:1.0.0</code></pre></div>
<div id="trivial_build_and_push" data-type="example" title2="Build the new container and push to a registry for use" no2="2-8">
<h5>Example 2-8. Build the new container and push to a registry for use</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">IMAGE</code><code class="o">=</code><code class="s2">"</code><code class="si">${</code><code class="nv">CONTAINER_REGISTRY</code><code class="si">}</code><code class="s2">/kubeflow/test:v1"</code>
docker build  -t <code class="s2">"</code><code class="si">${</code><code class="nv">IMAGE</code><code class="si">}</code><code class="s2">"</code> -f Dockerfile .
docker push <code class="s2">"</code><code class="si">${</code><code class="nv">IMAGE</code><code class="si">}</code><code class="s2">"</code></pre></div>

<p>With this setup, you’re now ready to start customizing the containers and components in Kubeflow to meet your needs.
We’ll do a deeper dive into building containers from scratch in <a data-type="xref" href="#beyond_tf">CHAPTER 9</a>. As we move forward in future chapters we’ll use this pattern to add tools when needed.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Editing YAML"><div class="sect3" id="idm45831180857848" title2="Editing YAML" no2="2.1.3.3">
<h3>2.1.3.3. Editing YAML</h3>

<p>While Kubeflow abstracts the details of Kubernetes away from us to a large degree, there are still times when looking at<a data-type="indexterm" data-primary="YAML" data-secondary="editing" id="idm45831180856504"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="YAML configuration editing" id="idm45831180855528"></a>
or modifying the configuration is useful. Most of Kubernetes configuration is represented in YAML, so having tools set up
to easily look at and edit YAMLs will be beneficial. Most integrated development environments (IDEs) offer some sort of tooling for editing YAML, but you may have to install these separately.</p>
<div data-type="tip"><h6>Tip</h6>
<p>For IntelliJ there is a <a href="https://oreil.ly/Awmeq">YAML plugin</a>.
For emacs there are many modes available for YAML editing, including <a href="https://oreil.ly/lWZE5">yaml-mode</a> (which is installable from <a href="https://melpa.org">Milkypostman’s Emacs Lisp Package Archive (MELPA)</a>).
Atom has syntax highlighting available as a package <a href="https://oreil.ly/z47Sa">YAML</a>.
If you use a different IDE, don’t throw it away just for better YAML editing before you explore the plugin available.
Regardless of IDE you can also use the <a href="http://www.yamllint.com">YAMLlint website</a> to check your YAML.</p>
</div>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Creating Our First Kubeflow Project"><div class="sect2" id="idm45831180817352" title2="Creating Our First Kubeflow Project" no2="2.1.4">
<h2>2.1.4. Creating Our First Kubeflow Project</h2>

<p>First, we need to make a Kubeflow project to work in.<a data-type="indexterm" data-primary="Kubeflow" data-secondary="first project" id="ch02-frst"></a><a data-type="indexterm" data-primary="getting started" data-secondary="installing Kubeflow" data-tertiary="first project" id="ch02-frst3"></a><a data-type="indexterm" data-primary="hello world project" id="ch02-hllo"></a><a data-type="indexterm" data-primary="MNIST (Modified National Institute of Standards and Technology)" data-secondary="hello world project" id="ch02-frst4"></a><a data-type="indexterm" data-primary="handwriting recognition via RandomForestClassifier" id="ch02-rand2"></a>
To create a Kubeflow deployment we use the <code>kfctl</code> program.<sup><a data-type="noteref" id="idm45831180809144-marker" href="#idm45831180809144">[2]</a></sup>
When using Kubeflow you need to specify a manifest file that configures what is built and how there are various manifests for different cloud providers.</p>

<p>We’ll start with an example project using a vanilla configuration, as seen in <a data-type="xref" href="#create_example_project">EXAMPLE 2-9</a>. In this project we’ll build a simple end-to-end pipeline for our MNIST example. We chose this example because it’s the standard “hello world” of machine learning.</p>
<div id="create_example_project" data-type="example" title2="Create first example project" no2="2-9">
<h5>Example 2-9. Create first example project</h5>

<pre data-type="programlisting" data-code-language="shell"><code class="c"># Pick the correct config file for your platform from</code>
<code class="c"># https://github.com/kubeflow/manifests/tree/[version]/kfdef</code>
<code class="c"># You can download and edit the configuration at this point if you need to.</code>
<code class="c"># For generic Kubernetes with Istio:</code>
<code class="nv">MANIFEST_BRANCH</code><code class="o">=</code><code class="si">${</code><code class="nv">MANIFEST_BRANCH</code><code class="k">:-</code><code class="nv">v1</code><code class="p">.0-branch</code><code class="si">}</code>
<code class="nb">export </code>MANIFEST_BRANCH
<code class="nv">MANIFEST_VERSION</code><code class="o">=</code><code class="si">${</code><code class="nv">MANIFEST_VERSION</code><code class="k">:-</code><code class="nv">v1</code><code class="p">.0.1</code><code class="si">}</code>
<code class="nb">export </code>MANIFEST_VERSION

<code class="nv">KF_PROJECT_NAME</code><code class="o">=</code><code class="si">${</code><code class="nv">KF_PROJECT_NAME</code><code class="k">:-</code><code class="nv">hello</code><code class="p">-kf-</code><code class="si">${</code><code class="nv">PLATFORM</code><code class="si">}}</code>
<code class="nb">export </code>KF_PROJECT_NAME
mkdir <code class="s2">"</code><code class="si">${</code><code class="nv">KF_PROJECT_NAME</code><code class="si">}</code><code class="s2">"</code>
<code class="nb">pushd</code> <code class="s2">"</code><code class="si">${</code><code class="nv">KF_PROJECT_NAME</code><code class="si">}</code><code class="s2">"</code>

<code class="nv">manifest_root</code><code class="o">=</code>https://raw.githubusercontent.com/kubeflow/manifests/
<code class="c"># On most environments this will create a "vanilla" Kubeflow install using Istio.</code>
<code class="nv">FILE_NAME</code><code class="o">=</code>kfctl_k8s_istio.<code class="si">${</code><code class="nv">MANIFEST_VERSION</code><code class="si">}</code>.yaml
<code class="nv">KFDEF</code><code class="o">=</code><code class="si">${</code><code class="nv">manifest_root</code><code class="si">}${</code><code class="nv">MANIFEST_BRANCH</code><code class="si">}</code>/kfdef/<code class="si">${</code><code class="nv">FILE_NAME</code><code class="si">}</code>
kfctl apply -f <code class="nv">$KFDEF</code> -V
<code class="nb">echo</code> <code class="nv">$?</code>

<code class="nb">popd</code></pre></div>

<p><a data-type="xref" href="#create_example_project">EXAMPLE 2-9</a> assumes you’re using an existing Kubernetes cluster (like local Minikube).
While your running <code>kfctl apply</code> you will see lots of status messages and maybe even some error messages. Provided it prints out a 0 at the end you can safely ignore most errors as they are automatically retried.</p>
<div data-type="warning"><h6>Warning</h6>
<p>This deployment process can take up to <em>30 minutes</em>.</p>
</div>

<p>If you’ve decided to go straight ahead with a cloud provider,<a data-type="indexterm" data-primary="Kubeflow" data-secondary="installing" id="idm45831180722008"></a><a data-type="indexterm" data-primary="getting started" data-secondary="installing Kubeflow" id="idm45831180721032"></a><a data-type="indexterm" data-primary="resources for Kubeflow" data-secondary="installation guide" id="idm45831180720088"></a> <a href="https://oreil.ly/EMRVV">the Kubeflow installation guide</a> has information on how to get started.</p>
<div data-type="warning"><h6>Warning</h6>
<p>The Kubeflow user interface can come up before Kubeflow is fully deployed,<a data-type="indexterm" data-primary="kubectl" data-secondary="deployment of Kubeflow" id="idm45831180717128"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="kubectl" data-tertiary="deployment of Kubeflow" id="idm45831180716152"></a> and accessing it then can mean you won’t have a proper namespace.<a data-type="indexterm" data-primary="namespaces" data-secondary="deployment of Kubeflow" id="idm45831180714808"></a><a data-type="indexterm" data-primary="deployment of Kubeflow" data-secondary="namespace" id="idm45831180713864"></a> To make sure Kubeflow is ready, run <code>kubectl get pods --all-namespaces -w</code> and wait for all of the <a data-type="indexterm" data-primary="Pods (Kubernetes)" data-secondary="deployment of Kubeflow" id="idm45831180712312"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="Pods" data-tertiary="deployment of Kubeflow" id="idm45831180711368"></a>pods to become RUNNING or COMPLETED. If you see pods being preempted, make sure you launched a cluster with enough RAM and disk space. If you can’t launch a large enough cluster locally, consider a cloud provider. (Ilan and Holden are currently working on a blog post on this topic.)</p>
</div>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Training and Deploying a Model"><div class="sect1" id="model_management" title2="Training and Deploying a Model" no2="2.2">
<h1>2.2. Training and Deploying a Model</h1>

<p>In traditional machine learning texts, the training phase is the one that is given the most attention, with a few simple
examples on deployment, and very little treatment of model management.
Throughout this book, we assume that you are a data scientist who knows how to select the correct model/algorithm or work with someone who does. We focus on the deployment and model management more than traditional ML texts.</p>








<section data-type="sect2" data-pdf-bookmark="Training and Monitoring Progress"><div class="sect2" id="idm45831180707320" title2="Training and Monitoring Progress" no2="2.2.1">
<h2>2.2.1. Training and Monitoring Progress</h2>

<p>The next step is to train the model using a Kubeflow Pipeline.<a data-type="indexterm" data-primary="training" data-secondary="first Kubeflow project" id="idm45831180705400"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="training first project" id="idm45831180704424"></a>
We will use a precreated training container<sup><a data-type="noteref" id="idm45831180703352-marker" href="#idm45831180703352">[3]</a></sup> that downloads the training data and trains the model.
For <a data-type="xref" href="#create_training_workflow">EXAMPLE 2-10</a>, we have a prebuilt workflow in<a data-type="indexterm" data-primary="Scikit-learn" data-secondary="RandomForestClassifier" id="ch02-rand"></a> <code>train_pipeline.py</code> that trains a <code>RandomForestClassifier</code> in <a href="https://oreil.ly/Kubeflow_for_ML">the ch2 folder on this book’s GitHub example repo</a>.</p>
<div id="create_training_workflow" data-type="example" title2="Create training workflow example" no2="2-10">
<h5>Example 2-10. Create training workflow example</h5>

<pre data-type="programlisting" data-code-language="shell">dsl-compile --py train_pipeline.py --output job.yaml</pre></div>

<p>If you run into problems here, you should check out the <a href="https://oreil.ly/nvNnC">Kubeflow troubleshooting guide</a>.</p>

<p>The Kubeflow UI, as seen in <a data-type="xref" href="#img-kubeflow-ui">FIGURE 2-1</a>, is accessed in a few different ways.<a data-type="indexterm" data-primary="Kubeflow" data-secondary="web UI installation" id="idm45831180694248"></a><a data-type="indexterm" data-primary="user interfaces (UI)" data-secondary="installation of Kubeflow web UI" id="idm45831180693304"></a><a data-type="indexterm" data-primary="Kubeflow" data-secondary="installing" id="idm45831180692392"></a>
For local deployments a quick port forward is the simplest way to get started: just run <code>kubectl port-forward svc/istio-ingressgateway -n istio-system 7777:80</code> and then go to <code>localhost:7777</code>.
If you have deployed on GCP you should go to <code>https://&lt;deployment_name&gt;.endpoints.&lt;project_name&gt;.cloud.goog</code>.
Otherwise, you can get the address of the gateway service by running <code>kubectl get ingress -n istio-system</code>.</p>

<figure><div id="img-kubeflow-ui" class="figure" data-type="figure" title2="Kubeflow web UI" no2="2-1">
<img src="assets/kfml_0201.png" alt="kubeflow-ui" width="1440" height="378">
<h6>Figure 2-1. Kubeflow web UI</h6>
</div></figure>

<p class="less_space pagebreak-before">Click pipelines, or add <code>_/pipeline/</code> to the root URL and you should see the Pipelines web UI, as in <a data-type="xref" href="#img-argo">FIGURE 2-2</a>.</p>

<figure><div id="img-argo" class="figure" data-type="figure" title2="Pipelines web UI" no2="2-2">
<img src="assets/kfml_0202.png" alt="argo-ui" width="1200" height="210">
<h6>Figure 2-2. Pipelines web UI</h6>
</div></figure>

<p>From here we can upload our pipeline. Once we’ve uploaded the pipeline we can use the same web UI to create a run of the pipeline. After you click the uploaded pipeline you’ll be able to create a run, as shown in <a data-type="xref" href="#img-pipeline-detail">FIGURE 2-3</a>.</p>

<figure><div id="img-pipeline-detail" class="figure" data-type="figure" title2="Pipeline detail page" no2="2-3">
<img src="assets/kfml_0203.png" alt="pipeline-detail" width="1150" height="293">
<h6>Figure 2-3. Pipeline detail page</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Test Query"><div class="sect2" id="idm45831180706696" title2="Test Query" no2="2.2.2">
<h2>2.2.2. Test Query</h2>

<p>Finally, let’s query our model and monitor the results. A “sanity check” is a simple test to ensure our model is making<a data-type="indexterm" data-primary="testing" data-secondary="first project test query" id="idm45831180686376"></a><a data-type="indexterm" data-primary="model inference" data-secondary="first project test query" id="idm45831180685432"></a>
predictions that are theoretically reasonable. For example—we’re attempting to guess what digit
was written. If our model comes back with answers like <code>77</code>, <code>orange Kool-Aid</code>, or <code>ERROR</code>, those would all fail the sanity
check. We expect to see digits between 0 and 9.  Sanity checking models before putting them into production is always a
wise choice.</p>

<p>The web UI and model serving are exposed through the same Istio gateway.
So, the model will be available at <em>http://&lt;WEBUI_URL&gt;/seldon&lt;mnist-classifier/api&lt;v0.1/predictions</em>.
If you’re using Google IAP, you may find the iap_curl project helpful for making requests.</p>

<p>There is a Python <a href="https://oreil.ly/Kubeflow_for_MLch02">script available</a> <a data-type="indexterm" data-primary="Python" data-secondary="MNIST image script" id="idm45831180680520"></a><a data-type="indexterm" data-primary="MNIST (Modified National Institute of Standards and Technology)" data-secondary="Python script for first project" id="idm45831180679512"></a>for pulling an image from the MNIST dataset, turning it into a vector, displaying the image, and sending it to the model.
Turning the image into a vector is normally part of the preprediction transformation; we’ll cover more of this in <a data-type="xref" href="#inference_ch">CHAPTER 8</a>.
<a data-type="xref" href="#model_serving">EXAMPLE 2-11</a> is a fairly clear Python example of how one can query the model. The model returns a JSON of the 10 digits and the probability of whether the submitted vector represents a specific digit.  Specifically, we need an image of a handwritten digit that we can turn into an array of values.</p>
<div id="model_serving" data-type="example" title2="Model query example" no2="2-11">
<h5>Example 2-11. Model query example</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">requests</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>

<code class="kn">from</code> <code class="nn">tensorflow.examples.tutorials.mnist</code> <code class="kn">import</code> <code class="n">input_data</code>
<code class="kn">from</code> <code class="nn">matplotlib</code> <code class="kn">import</code> <code class="n">pyplot</code> <code class="k">as</code> <code class="n">plt</code>

<code class="k">def</code> <code class="nf">download_mnist</code><code class="p">():</code>
    <code class="k">return</code> <code class="n">input_data</code><code class="o">.</code><code class="n">read_data_sets</code><code class="p">(</code><code class="s2">"MNIST_data/"</code><code class="p">,</code> <code class="n">one_hot</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>


<code class="k">def</code> <code class="nf">gen_image</code><code class="p">(</code><code class="n">arr</code><code class="p">):</code>
    <code class="n">two_d</code> <code class="o">=</code> <code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">arr</code><code class="p">,</code> <code class="p">(</code><code class="mi">28</code><code class="p">,</code> <code class="mi">28</code><code class="p">))</code> <code class="o">*</code> <code class="mi">255</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">uint8</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">two_d</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="n">plt</code><code class="o">.</code><code class="n">cm</code><code class="o">.</code><code class="n">gray_r</code><code class="p">,</code> <code class="n">interpolation</code><code class="o">=</code><code class="s1">'nearest'</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">plt</code>
<code class="n">mnist</code> <code class="o">=</code> <code class="n">download_mnist</code><code class="p">()</code>
<code class="n">batch_xs</code><code class="p">,</code> <code class="n">batch_ys</code> <code class="o">=</code> <code class="n">mnist</code><code class="o">.</code><code class="n">train</code><code class="o">.</code><code class="n">next_batch</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="n">chosen</code> <code class="o">=</code> <code class="mi">0</code>
<code class="n">gen_image</code><code class="p">(</code><code class="n">batch_xs</code><code class="p">[</code><code class="n">chosen</code><code class="p">])</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>
<code class="n">data</code> <code class="o">=</code> <code class="n">batch_xs</code><code class="p">[</code><code class="n">chosen</code><code class="p">]</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="mi">1</code><code class="p">,</code> <code class="mi">784</code><code class="p">))</code>
<code class="n">features</code> <code class="o">=</code> <code class="p">[</code><code class="s2">"X"</code> <code class="o">+</code> <code class="nb">str</code><code class="p">(</code><code class="n">i</code> <code class="o">+</code> <code class="mi">1</code><code class="p">)</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">784</code><code class="p">)]</code>
<code class="n">request</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"data"</code><code class="p">:</code> <code class="p">{</code><code class="s2">"names"</code><code class="p">:</code> <code class="n">features</code><code class="p">,</code> <code class="s2">"ndarray"</code><code class="p">:</code> <code class="n">data</code><code class="o">.</code><code class="n">tolist</code><code class="p">()}}</code>
<code class="n">deploymentName</code> <code class="o">=</code> <code class="s2">"mnist-classifier"</code>
<code class="n">uri</code> <code class="o">=</code> <code class="s2">"http://"</code> <code class="o">+</code> <code class="n">AMBASSADOR_API_IP</code> <code class="o">+</code> <code class="s2">"/seldon/"</code> <code class="o">+</code> \
    <code class="n">deploymentName</code> <code class="o">+</code> <code class="s2">"/api/v0.1/predictions"</code>

<code class="n">response</code> <code class="o">=</code> <code class="n">requests</code><code class="o">.</code><code class="n">post</code><code class="p">(</code><code class="n">uri</code><code class="p">,</code> <code class="n">json</code><code class="o">=</code><code class="n">request</code><code class="p">)</code></pre></div>

<p>For example, see the handwritten <em>3</em> in <a data-type="xref" href="#handwritten_3">FIGURE 2-4</a>.</p>

<figure><div id="handwritten_3" class="figure" data-type="figure" title2="Handwritten 3" no2="2-4">
<img src="assets/kfml_0204.png" alt="kfml 0204" width="299" height="293">
<h6>Figure 2-4. Handwritten <em>3</em></h6>
</div></figure>

<p class="less_space pagebreak-before">This returns the following:</p>

<pre data-type="programlisting" id="untitled_programlisting_1" title2="(no caption)" no2="">{'data': {'names': ['class:0',
		    'class:1',
		    'class:2',
		    'class:3',
		    'class:4',
		    'class:5',
		    'class:6',
		    'class:7',
		    'class:8',
		    'class:9'],
	  'ndarray':[[0.03333333333333333,
		      0.26666666666666666,
		      0.03333333333333333,
		      0.13333333333333333, ## It was actually this
		      0.1,
		      0.06666666666666667,
		      0.1,
		      0.26666666666666666,
		      0.0,
		      0.0]]},
 'meta': {'puid': 'tb02ff58vcinl82jmkkoe80u4r', 'routing': {}, 'tags': {}}}</pre>

<p>We can see that even though we wrote a pretty clear <em>3</em>, the model’s best guess was a tie between <em>1</em> and <em>7</em>. That
being said, <code>RandomForestClassifier</code> is a bad model for handwriting recognition—so this isn’t a surprising result.
We used <code>RandomForestClassifier</code> for two reasons: first, to illustrate model explainability in <a data-type="xref" href="#inference_ch">CHAPTER 8</a>, and second, so you can experiment with a more reasonable model and compare performance.<a data-type="indexterm" data-startref="ch02-frst" id="idm45831180284552"></a><a data-type="indexterm" data-startref="ch02-frst3" id="idm45831180283880"></a><a data-type="indexterm" data-startref="ch02-frst4" id="idm45831180283208"></a><a data-type="indexterm" data-startref="ch02-rand" id="idm45831180282536"></a><a data-type="indexterm" data-startref="ch02-rand2" id="idm45831180281864"></a><a data-type="indexterm" data-startref="ch02-hllo" id="idm45831180281192"></a></p>
<div data-type="note"><h6>Note</h6>
<p>While we’ve deployed our end-to-end example here without any real validation,<a data-type="indexterm" data-primary="validation of models" data-secondary="importance of" id="idm45831180279672"></a><a data-type="indexterm" data-primary="models" data-secondary="validation" id="idm45831180278696"></a> you should always validate before real production.</p>
</div>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Going Beyond a Local Deployment"><div class="sect1" id="idm45831180687624" title2="Going Beyond a Local Deployment" no2="2.3">
<h1>2.3. Going Beyond a Local Deployment</h1>

<p>Some of you have been trying this out on a local Kubernetes deployment.
One of the powers of Kubeflow is the ability to<a data-type="indexterm" data-primary="Kubeflow" data-secondary="local to distributed with ease" id="idm45831180275816"></a><a data-type="indexterm" data-primary="scalability of Kubeflow" data-secondary="Kubernetes foundation" id="idm45831180274824"></a><a data-type="indexterm" data-primary="Kubeflow" data-secondary="installing" id="idm45831180273880"></a><a data-type="indexterm" data-primary="getting started" data-secondary="installing Kubeflow" id="idm45831180272936"></a> scale using Kubernetes. Kubernetes can run on a single machine or many computers, and some environments can dynamically add more resources as needed.
While Kubernetes is an industry standard, there are variations in Kubeflow’s setup steps required depending on your provider.<a data-type="indexterm" data-primary="getting started" data-secondary="getting started guide" id="idm45831180271576"></a><a data-type="indexterm" data-primary="resources for Kubeflow" data-secondary="getting started guide" id="idm45831180270632"></a><a data-type="indexterm" data-primary="Kubeflow" data-secondary="getting started guide" id="idm45831180269688"></a> <a href="https://oreil.ly/eq6rC">Kubeflow’s getting started guide</a> has installation instructions for GCP, AWS, Azure, IBM Cloud, and OpenShift.
Once Kubeflow is installed on your Kubernetes cluster, you can try this same example again and see how the same code can run, or take our word for it and move on to more interesting problems.</p>
<div data-type="tip"><h6>Tip</h6>
<p>When deploying on cloud providers, Kubeflow can create more than just Kubernetes resources that should be deleted too. For example, on Google you can delete the ancillary services by going to the deployment manager.</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45831180266184" title2="Conclusion" no2="2.4">
<h1>2.4. Conclusion</h1>

<p>In this chapter, you got your first real taste of Kubeflow.
You now have your development environment properly configured and a Kubeflow deployment you can use throughout the rest of this book.
We covered a simple end-to-end example with the standard MNIST, allowing you to see the different core components of Kubeflow in action.
We introduced the pipeline, which ties all of Kubeflow together, and you used it to train your model.
In <a data-type="xref" href="#kubeflow_design_beyond_basics">CHAPTER 3</a> we will explore Kubeflow’s design and set up some optional components. Understanding the design will help you choose the right components.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45831180911944"><sup><a href="#idm45831180911944-marker">[1]</a></sup> Just search “cloudname” plus the container registry name for documentation.</p><p data-type="footnote" id="idm45831180809144"><sup><a href="#idm45831180809144-marker">[2]</a></sup> Not to be confused with the legacy <code>kfctl.sh</code> script.</p><p data-type="footnote" id="idm45831180703352"><sup><a href="#idm45831180703352-marker">[3]</a></sup> The container is from <a href="">this GitHub repo</a>.</p></div></div></section>
<section data-type="chapter" data-pdf-bookmark="Chapter 3. Kubeflow Design: Beyond the Basics"><div class="chapter" id="kubeflow_design_beyond_basics" data-type="chapter" title2="Kubeflow Design: Beyond the Basics" no2="3">
<h1>Chapter 3. Kubeflow Design: Beyond the Basics</h1>


<p>You made it through two chapters.  Well done.
So far you have decided to learn Kubeflow and worked through a simple example.
Now we want to take a step back and look at each component in detail. <a data-type="xref" href="#ch3_kf_arch">FIGURE 3-1</a> shows the main Kubeflow components and the role they play in the overall architecture.</p>

<figure><div id="ch3_kf_arch" class="figure" data-type="figure" title2="Kubeflow architecture" no2="3-1">
<img src="assets/kfml_0301.png" alt="Kubeflow Architecture" width="1444" height="1112">
<h6>Figure 3-1. Kubeflow architecture</h6>
</div></figure>

<p>Essentially, we’ll look at the core elements that make up our example deployment as well as the supporting pieces.
In the chapters that follow, we will dig into each of these sections in greater depth.</p>

<p>That said, let’s get started.</p>






<section data-type="sect1" data-pdf-bookmark="Getting Around the Central Dashboard"><div class="sect1" id="idm45831180257240" title2="Getting Around the Central Dashboard" no2="3.1">
<h1>3.1. Getting Around the Central Dashboard</h1>

<p>Your main interface to Kubeflow is the central dashboard (see <a data-type="xref" href="#central_dashboard">FIGURE 3-2</a>),<a data-type="indexterm" data-primary="components" data-secondary="central dashboard" id="idm45831180255000"></a><a data-type="indexterm" data-primary="central dashboard" id="idm45831180254024"></a><a data-type="indexterm" data-primary="user interfaces (UI)" data-secondary="central dashboard" id="idm45831180253352"></a> which allows you to access the majority of Kubeflow components.
Depending on your Kubernetes provider, it might take up to half an hour to have your ingress become available.</p>

<figure><div id="central_dashboard" class="figure" data-type="figure" title2="The central dashboard" no2="3-2">
<img src="assets/kfml_0302.png" alt="The Central Dashboard" width="880" height="544">
<h6>Figure 3-2. The central dashboard</h6>
</div></figure>
<div data-type="note"><h6>Note</h6>
<p>While it is meant to be automatic, if you don’t have a namespace created for your work, follow<a data-type="indexterm" data-primary="manual profile creation" id="idm45831180249000"></a><a data-type="indexterm" data-primary="namespaces" data-secondary="manual profile creation" id="idm45831180248296"></a><a data-type="indexterm" data-primary="profiles" data-secondary="manual creation" id="idm45831180247352"></a> <a href="https://oreil.ly/_6iC5">Kubeflow’s “Manual profile creation” instructions</a>.</p>
</div>

<p>From the home page of the central dashboard you can access Kubeflow’s Pipelines, Notebooks, Katib (hyperparameter tuning), and the artifact store.
We will cover the design of these components and how to use them next.</p>








<section data-type="sect2" data-pdf-bookmark="Notebooks (JupyterHub)"><div class="sect2" id="idm45831180244680" title2="Notebooks (JupyterHub)" no2="3.1.1">
<h2>3.1.1. Notebooks (JupyterHub)</h2>

<p>The first step of most projects is some form of prototyping and experimentation.<a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="JupyterHub" id="idm45831180242952"></a>
Kubeflow’s tool for this purpose is <a href="https://jupyter.org/hub">JupyterHub</a>—a multiuser hub that spawns, manages, and proxies multiple instances of a single-user <a href="https://oreil.ly/C4dtQ">Jupyter notebook</a>.
Jupyter notebooks support the whole computation process: developing, documenting, and executing code, as well as communicating the results.</p>

<p>To access JupyterHub, go to the main Kubeflow page and click the notebook button. On the notebook page, you can connect to existing servers or create a new one.</p>

<p>To create a new server, you need to specify the server name and namespace, pick an image (from CPU optimized, GPU optimized, or a custom image that you can create), and specify resource requirements—CPU/memory, workspace, data volumes, custom configuration, and so on.
Once the server is created, you can connect to it and start creating and editing notebooks.</p>

<p>In order to allow data scientists to do cluster operations without leaving the notebook’s environment, Kubeflow adds<a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="kubectl for Kubernetes management" id="idm45831180238296"></a><a data-type="indexterm" data-primary="kubectl" data-secondary="Jupyter notebook incorporation" id="idm45831180237256"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="kubectl" data-tertiary="Jupyter notebook incorporation" id="idm45831180236296"></a>
<a href="https://oreil.ly/i-PFC">kubectl</a> to the provided notebook images, which allows developers to use notebooks to create and manage Kubernetes resources.
The Jupyter notebook pods run under a special service account <code>default-editor</code>, which has namespace-scoped permissions to the following Kubernetes resources:</p>

<ul>
<li>
<p>Pods</p>
</li>
<li>
<p>Deployments</p>
</li>
<li>
<p>Services</p>
</li>
<li>
<p>Jobs</p>
</li>
<li>
<p>TFJobs</p>
</li>
<li>
<p>PyTorchJobs</p>
</li>
</ul>

<p>You can bind this account to a custom role, in order to limit/extend permissions of the notebook server.
This allows notebook developers to execute all of the (allowed by role) Kubernetes commands without leaving the notebook environment. For example, the <a data-type="indexterm" data-primary="Kubernetes" data-secondary="resource creation" id="idm45831180226936"></a>creation of a new Kubernetes resource can be done by running the following command directly in a Jupyter notebook:</p>

<pre data-type="programlisting" data-code-language="bash" id="untitled_programlisting_2" title2="(no caption)" no2="">!kubectl create -f myspec.yaml</pre>

<p>The contents of your <code>yaml</code> file will determine what resource is created.<a data-type="indexterm" data-primary="YAML" data-secondary="resource creation" id="idm45831180223800"></a> If you’re not used to making Kubernetes resources, don’t worry—Kubeflow’s pipelines include tools to make them for you.</p>

<p>To further increase Jupyter capabilities, Kubeflow also provides support in the notebooks<a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="Kubeflow component support via" id="idm45831180222152"></a> for such important Kubeflow components as Pipelines and metadata management (described later in <a data-type="xref" href="#metadata_manage">SECTION 3.1.6</a>). Jupyter notebooks can also directly launch distributed training jobs.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Training Operators"><div class="sect2" id="idm45831180244056" title2="Training Operators" no2="3.1.2">
<h2>3.1.2. Training Operators</h2>

<p>JupyterHub is a great tool for initial experimentation with the data and prototyping ML jobs.<a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="JupyterHub" id="idm45831180217112"></a><a data-type="indexterm" data-primary="components" data-secondary="training operators" id="idm45831180216136"></a><a data-type="indexterm" data-primary="training" data-secondary="Kubeflow components" id="idm45831180215192"></a> However, when moving to train in production, Kubeflow provides several training components to automate the execution of machine learning algorithms, including:</p>

<ul>
<li>
<p><a href="https://oreil.ly/AjfwS">Chainer training</a></p>
</li>
<li>
<p><a href="https://oreil.ly/SK19W">MPI training</a></p>
</li>
<li>
<p><a href="https://oreil.ly/FvDdQ">Apache MXNet training</a></p>
</li>
<li>
<p><a href="https://oreil.ly/0z4j6">PyTorch training</a></p>
</li>
<li>
<p><a href="https://oreil.ly/YMGKx">TensorFlow training</a></p>
</li>
</ul>

<p>In Kubeflow, distributed training jobs are managed by application-specific controllers,<a data-type="indexterm" data-primary="training" data-secondary="operators" id="idm45831180206456"></a> known as operators. These operators extend the Kubernetes APIs to create, manage, and manipulate the state of resources. For example, to run a distributed TensorFlow training job, the user just needs to provide a specification that describes the desired state (number of workers and parameter servers, etc.), and the TensorFlow operator component will take care of the rest and manage the life cycle of the training job.</p>

<p>These operators allow the automation of important deployment concepts such as<a data-type="indexterm" data-primary="scalability of Kubeflow" data-secondary="operators automating" id="idm45831180204536"></a><a data-type="indexterm" data-primary="observability automated by operators" id="idm45831180203560"></a><a data-type="indexterm" data-primary="failover" id="idm45831180202920"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="operators chaining execution" id="idm45831180202248"></a> scalability, observability, and failover. They can also be used by pipelines to chain their execution with the execution of other components of the system.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Kubeflow Pipelines"><div class="sect2" id="idm45831180184408" title2="Kubeflow Pipelines" no2="3.1.3">
<h2>3.1.3. Kubeflow Pipelines</h2>

<p>In addition to providing specialized parameters implementing specific functionality, Kubeflow has<a data-type="indexterm" data-primary="components" data-secondary="pipelines" data-seealso="Kubeflow Pipelines" id="idm45831180182536"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="about" id="idm45831180181288"></a> <a href="https://oreil.ly/QZjNV">Pipelines</a>, which allows you to orchestrate the execution of machine learning applications.
This implementation is based on<a data-type="indexterm" data-primary="Argo Workflows" data-secondary="Kubeflow Pipelines built on" id="idm45831180179400"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="Argo Workflows" id="idm45831180178456"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="Argo Workflows foundation" id="idm45831180177512"></a>
<a href="https://oreil.ly/6PsLK">Argo Workflows</a>, an open source, container-native workflow engine for Kubernetes. Kubeflow installs all of the Argo components.</p>

<p>At a high level, the execution of a pipeline contains the following<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="components of" id="idm45831180175368"></a>
<a href="https://oreil.ly/QZjNV">components</a>:</p>
<dl class="less_space pagebreak-before">
<dt>Python SDK</dt>
<dd>
<p>You create components or specify a pipeline using the Kubeflow Pipelines <a data-type="indexterm" data-primary="Python" data-secondary="pipeline components" id="idm45831180171368"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="Python SDK" id="idm45831180170392"></a> <a href="https://oreil.ly/c2DRj">domain-specific language</a> (DSL).</p>
</dd>
<dt>DSL compiler</dt>
<dd>
<p>The <a href="https://oreil.ly/5o2Yw">DSL compiler</a> <a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="compiler" id="idm45831180166600"></a><a data-type="indexterm" data-primary="Python" data-secondary="DSL compiler" id="idm45831180165592"></a><a data-type="indexterm" data-primary="YAML" data-secondary="DSL compiler producing" id="idm45831180164648"></a>transforms your pipeline’s Python code into a static configuration (YAML).</p>
</dd>
<dt>Pipeline Service</dt>
<dd>
<p>The Pipeline Service <a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="Service" id="idm45831180162296"></a>creates a pipeline run from the static configuration.</p>
</dd>
<dt>Kubernetes resources</dt>
<dd>
<p>The Pipeline Service calls the Kubernetes API server to create the necessary Kubernetes<a data-type="indexterm" data-primary="Kubernetes" data-secondary="Pipeline Service custom resource definitions" id="idm45831180159880"></a><a data-type="indexterm" data-primary="custom resource definitions (CRDs)" data-secondary="Pipeline Service" id="idm45831180158808"></a> <a href="https://oreil.ly/5wPjy">custom resource definitions</a> (CRDs) to run the pipeline.</p>
</dd>
<dt>Orchestration controllers</dt>
<dd>
<p>A set of orchestration controllers execute the containers needed to complete<a data-type="indexterm" data-primary="orchestration controllers" id="idm45831180155544"></a> the pipeline execution specified by the Kubernetes resources (CRDs). The containers execute within Kubernetes Pods on virtual machines. An example <a data-type="indexterm" data-primary="Argo Workflows" data-secondary="orchestration controller" id="idm45831180154488"></a>controller is the <a href="https://oreil.ly/leX50">Argo Workflow</a> controller, which orchestrates task-driven workflows.</p>
</dd>
<dt>Artifact storage</dt>
<dd>
<p>The Kubernetes Pods store two kinds of data:<a data-type="indexterm" data-primary="Pods (Kubernetes)" data-secondary="data stored by" id="idm45831180151240"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="Pods" data-tertiary="data stored by" id="idm45831180150264"></a><a data-type="indexterm" data-primary="data" data-secondary="Kubernetes Pods storing" id="idm45831180149048"></a></p>
<dl>
<dt>Metadata</dt>
<dd>
<p>Experiments, jobs, runs, single scalar metrics (generally aggregated<a data-type="indexterm" data-primary="metadata" data-secondary="Kubernetes Pods" id="idm45831180146536"></a> for the purposes of sorting and filtering), etc. Kubeflow Pipelines stores the metadata in a MySQL database.</p>
</dd>
<dt>Artifacts</dt>
<dd>
<p>Pipeline packages, views, large-scale metrics like time series (usually used for investigating an individual run’s performance and for debugging), etc. Kubeflow Pipelines stores the artifacts in an artifact store like
<a href="https://docs.minio.io">MinIO server</a>,
<a href="https://oreil.ly/k1bQz">Google Cloud Storage (GCS)</a>,
or <a href="https://aws.amazon.com/s3">Amazon S3</a>.</p>
</dd>
</dl>
</dd>
</dl>

<p>Kubeflow Pipelines gives you the ability to make your machine learning<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="about" id="idm45831180134616"></a> jobs repeatable and handle new data.
It provides an intuitive DSL in Python to write your pipelines with. Your pipelines are then compiled down to an existing Kubernetes workflow engine (currently Argo Workflows).
Kubeflow’s pipeline components make it easy to use and coordinate the different tools required to build an end-to-end machine learning project.
On top of that, Kubeflow can <a data-type="indexterm" data-primary="metadata" data-secondary="tracked by Kubeflow" id="idm45831180133176"></a><a data-type="indexterm" data-primary="tracking data and metadata" id="idm45831180132232"></a><a data-type="indexterm" data-primary="data" data-secondary="tracked by Kubeflow" id="idm45831180131592"></a>track both data and metadata, improving how we can understand our jobs. For example, in <a data-type="xref" href="#data_and_feature_prep">CHAPTER 5</a> we use these artifacts to understand the schema.
Pipelines can expose the parameters of the underlying machine learning algorithms, allowing Kubeflow to perform tuning.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Hyperparameter Tuning"><div class="sect2" id="idm45831180129288" title2="Hyperparameter Tuning" no2="3.1.4">
<h2>3.1.4. Hyperparameter Tuning</h2>

<p>Finding the right set of hyperparameters for your training model can be a challenging task.<a data-type="indexterm" data-primary="components" data-secondary="hyperparameter tuning" id="ch03-cpy"></a><a data-type="indexterm" data-primary="hyperparameters" data-secondary="Kubeflow Katib for tuning" id="ch03-cpy2"></a> Traditional methodologies such as grid search can be time-consuming and quite tedious. Most existing hyperparameter systems are tied to one machine learning framework and have only a few options for searching the parameter space.</p>

<p>Kubeflow provides a component (called Katib) that allows users to perform<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="about" id="idm45831180124504"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="about" id="idm45831180123528"></a><a data-type="indexterm" data-primary="Google Vizier" id="idm45831180122584"></a><a data-type="indexterm" data-primary="AutoML (automated machine learning)" data-secondary="Kubeflow Katib" id="idm45831180121912"></a><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="AutoML" data-tertiary="Kubeflow Katib" id="idm45831180120952"></a> hyperparameter optimizations easily on Kubernetes clusters. Katib is inspired by <a href="https://oreil.ly/UInbP">Google Vizier</a>, a black-box optimization framework. It leverages advanced searching algorithms such as Bayesian optimization to find optimal hyperparameter configurations.<a data-type="indexterm" data-primary="Bayesian optimization" data-secondary="in Katib" id="idm45831180118776"></a></p>

<p>Katib supports
<a href="https://oreil.ly/O5mC9">hyperparameter tuning</a> and can run with any deep learning framework, including TensorFlow, MXNet, and PyTorch.</p>

<p>As in Google Vizier, Katib is based on four main concepts:</p>
<dl>
<dt>Experiment</dt>
<dd>
<p>A single optimization run over a feasible space. Each experiment<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="experiments" id="idm45831180114440"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="experiments" id="idm45831180113464"></a><a data-type="indexterm" data-primary="experiments" data-secondary="Kubeflow Katib" id="idm45831180112520"></a> contains a configuration describing the feasible space, as well as a set of trials. It is assumed that objective function <em>f(x)</em> does not change in the course of the experiment.</p>
</dd>
<dt>Trial</dt>
<dd>
<p>A list of parameter values, <em>x</em>, that will lead to a single evaluation<a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="trials" id="idm45831180109128"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="trials" id="idm45831180108152"></a> of <em>f(x)</em>. A trial can be “completed,” which means that it has been evaluated and the objective value <em>f(x)</em> has been assigned to it, otherwise it is “pending.” One trial corresponds to one job.</p>
</dd>
<dt>Job</dt>
<dd>
<p>A process responsible for evaluating a pending trial and calculating its objective value.<a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="jobs" id="idm45831180104600"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="jobs" id="idm45831180103624"></a></p>
</dd>
<dt>Suggestion</dt>
<dd>
<p>An algorithm to construct a parameter set. Currently, Katib supports<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="suggestions" id="idm45831180101400"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="suggestions" id="idm45831180100424"></a> the following exploration algorithms:</p>

<ul>
<li>
<p>Random</p>
</li>
<li>
<p>Grid</p>
</li>
<li>
<p><a href="https://oreil.ly/LlCKw">Hyperband</a></p>
</li>
<li>
<p><a href="https://oreil.ly/Pa83u">Bayesian optimization</a></p>
</li>
</ul>
</dd>
</dl>

<p>Using these core concepts, you can increase your model’s performance. Since Katib is not tied to one machine learning library, you can explore new algorithms and tools with minimal modifications.<a data-type="indexterm" data-startref="ch03-cpy" id="idm45831180093592"></a><a data-type="indexterm" data-startref="ch03-cpy2" id="idm45831180092856"></a></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Model Inference"><div class="sect2" id="idm45831180092056" title2="Model Inference" no2="3.1.5">
<h2>3.1.5. Model Inference</h2>

<p>Kubeflow makes it easy to deploy machine learning models in production environments at scale.<a data-type="indexterm" data-primary="components" data-secondary="model inference" id="idm45831180090088"></a><a data-type="indexterm" data-primary="model inference" data-secondary="components" id="idm45831180089112"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Kubeflow" id="idm45831180088168"></a><a data-type="indexterm" data-primary="models" data-secondary="serving options" id="idm45831180087224"></a><a data-type="indexterm" data-primary="deployment of Kubeflow" data-secondary="model serving options" id="idm45831180086280"></a><a data-type="indexterm" data-primary="scalability of Kubeflow" data-secondary="inference component" id="idm45831180085336"></a> It provides several model serving options, including <a href="https://oreil.ly/Hp2sb">TFServing</a>, <a href="https://oreil.ly/sWc71">Seldon serving</a>, <a href="https://oreil.ly/bLJxg">PyTorch serving</a>, and <a href="https://oreil.ly/fuv-7">TensorRT</a>. It also provides an umbrella implementation, <a href="https://oreil.ly/qEvqq">KFServing</a>, which generalizes the model inference concerns of autoscaling, networking, health checking, and server 
configuration.</p>

<p>The overall implementation is based on leveraging <a href="https://istio.io">Istio</a> (covered later) and <a href="https://knative.dev">Knative serving</a>—serverless<a data-type="indexterm" data-primary="containers" data-secondary="serverless" id="idm45831180077960"></a><a data-type="indexterm" data-primary="serverless" data-secondary="containers on Kubernetes" id="idm45831180076952"></a><a data-type="indexterm" data-primary="Istio" data-secondary="model inference" id="idm45831180076040"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Istio" id="idm45831180075096"></a><a data-type="indexterm" data-primary="serverless" data-secondary="Knative serving and" id="idm45831180074152"></a> containers on Kubernetes. As defined in the <a href="https://oreil.ly/h6O1E">Knative documentation</a>, the Knative serving project provides middleware primitives that enable:</p>

<ul>
<li>
<p>Rapid deployment of serverless containers</p>
</li>
<li>
<p>Automatic scaling up and down to zero</p>
</li>
<li>
<p>Routing and network programming for Istio components</p>
</li>
</ul>

<p>Since model serving is inherently spiky, rapid scaling up and down is important.
Knative serving simplifies the support for continuous model updates, by automatically routing requests to newer model deployments. This requires scaling down to zero (minimizing resource utilization) for unused models while keeping them available for rollbacks. Since Knative is cloud native it benefits from its underlying infrastructure stack and therefore provides all the <a data-type="indexterm" data-primary="models" data-secondary="monitoring" data-tertiary="Knative serving project" id="idm45831180068200"></a><a data-type="indexterm" data-primary="model serving" data-secondary="monitoring" data-tertiary="Knative serving project" id="idm45831180066952"></a><a data-type="indexterm" data-primary="monitoring" data-secondary="Knative serving project" id="idm45831180065736"></a><a data-type="indexterm" data-primary="events via Knative Eventing" data-secondary="KFServing" id="idm45831180064792"></a><a data-type="indexterm" data-primary="Knative" data-secondary="Eventing" data-tertiary="KFServing" id="idm45831180063880"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="Knative Eventing" id="idm45831180062664"></a><a data-type="indexterm" data-primary="orchestration controllers" id="idm45831180061448"></a>monitoring capabilities that exist within Kubernetes, such as logging, tracing, and monitoring. KFServing also makes use of <a href="https://oreil.ly/fpLrH">Knative eventing</a> to give optional support for pluggable event sources.</p>

<p>Similar to Seldon, every KFServing deployment is an orchestrator, wiring together the following components:</p>
<dl>
<dt>Preprocessor</dt>
<dd>
<p>An optional component responsible for the transformation of the input data into content/format required for model serving</p>
</dd>
<dt>Predictor</dt>
<dd>
<p>A mandatory component responsible for an actual model serving</p>
</dd>
<dt>Postprocessor</dt>
<dd>
<p>An optional component responsible for the transformation/enriching of the model serving result into content/format required for output</p>
</dd>
</dl>

<p>Additional components can enhance one’s overall model serving implementation, but are outside of the main execution pipeline. Tools like outlier detection and model explainability can run in this environment without slowing down the overall system.</p>

<p>While all of these individual components and techniques have existed for a long time, having them integrated into the serving system of Kubeflow reduces the complexity involved in bringing new models into production.</p>

<p>In addition to the components directly supporting ML operations, Kubeflow also provides several supporting components.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Metadata"><div class="sect2" id="metadata_manage" title2="Metadata" no2="3.1.6">
<h2>3.1.6. Metadata</h2>

<p>An important component of Kubeflow is metadata management, providing capabilities<a data-type="indexterm" data-primary="components" data-secondary="metadata management" data-seealso="Kubeflow ML Metadata" id="idm45831180050952"></a><a data-type="indexterm" data-primary="metadata" data-secondary="tracked by Kubeflow" id="idm45831180049704"></a><a data-type="indexterm" data-primary="metadata" data-secondary="management component" id="idm45831180048760"></a> to capture and track information about a model’s creation. Many organizations build hundreds of models a day, but it’s very hard to manage all of a model’s related information. ML Metadata is both the infrastructure and a library for recording and retrieving metadata associated with an ML developer’s and data scientist’s workflow.
The information, which can be registered in the metadata component includes:</p>

<ul>
<li>
<p>Data sources used for the model’s creation</p>
</li>
<li>
<p>The artifacts generated through the components/steps of the pipeline</p>
</li>
<li>
<p>The executions of these components/steps</p>
</li>
<li>
<p>The pipeline and associated lineage information</p>
</li>
</ul>

<p>ML Metadata tracks the inputs and outputs of all components and steps in an ML workflow and their lineage. This data powers several important features listed in <a data-type="xref" href="#mlmd_features">TABLE 3-1</a> and shown in <a data-type="xref" href="#metadata_dia">FIGURE 3-3</a>.</p>
<table id="mlmd_features" data-type="table" title2="Examples of ML Metadata operations" no2="3-1">
<caption>Table 3-1. Examples of ML Metadata operations</caption>
<thead>
<tr>
<th>Operation</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>List all artifacts of a specific type.</p></td>
<td><p>All models that have been trained.</p></td>
</tr>
<tr>
<td><p>Compare two artifacts of the same type.</p></td>
<td><p>Compare results from two experiments.</p></td>
</tr>
<tr>
<td><p>Show a DAG of all related executions and their input and output artifacts.</p></td>
<td><p>Visualize the workflow of an experiment for debugging and discovery.</p></td>
</tr>
<tr>
<td><p>Display how an artifact was created.</p></td>
<td><p>See what data went into a model; enforce data retention plans.</p></td>
</tr>
<tr>
<td><p>Identify all artifacts that were created using a given artifact.</p></td>
<td><p>Mark all models trained from a specific dataset with bad data.</p></td>
</tr>
<tr>
<td><p>Determine if an execution has been run on the same inputs before.</p></td>
<td><p>Determine whether a component/step has already completed the same work and the previous output can just be reused.</p></td>
</tr>
<tr>
<td><p>Record and query context of workflow runs.</p></td>
<td><p>Track the owner and changes used for a workflow run; group the lineage by experiments; manage artifacts by projects.</p></td>
</tr>
</tbody>
</table>

<figure><div id="metadata_dia" class="figure" data-type="figure" title2="Metadata diagram" no2="3-3">
<img src="assets/kfml_0303.png" alt="Metadata Diagram" width="1445" height="1082">
<h6>Figure 3-3. Metadata diagram</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Component Summary"><div class="sect2" id="idm45831180023656" title2="Component Summary" no2="3.1.7">
<h2>3.1.7. Component Summary</h2>

<p>The magic of Kubeflow is making all of these traditionally distinct components work together.
While Kubeflow is certainly not the only system to bring together different parts of the machine learning landscape, it is unique in its flexibility in supporting a wide range of components. In addition to that, since it runs on standard Kubernetes, you can add your own components as desired.
Much of this magic of tool integration happens inside of Kubeflow’s pipelines, but some of the support components are essential to allowing these tools to interact.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Support Components"><div class="sect1" id="Supp_components" title2="Support Components" no2="3.2">
<h1>3.2. Support Components</h1>

<p>While these components aren’t explicitly exposed by Kubeflow, they play an important role in the overall Kubeflow ecosystem. Let’s briefly discuss each of them. We also encourage you to research them more on your own.</p>








<section data-type="sect2" data-pdf-bookmark="MinIO"><div class="sect2" id="minio_walkthrough" title2="MinIO" no2="3.2.1">
<h2>3.2.1. MinIO</h2>

<p>The foundation of the pipeline architecture is shared storage. A common practice<a data-type="indexterm" data-primary="object stores" data-secondary="distributed object storage server" id="idm45831180017544"></a><a data-type="indexterm" data-primary="MinIO" id="ch03-min2"></a><a data-type="indexterm" data-primary="MinIO" data-secondary="distributed object storage server" id="ch03-min"></a><a data-type="indexterm" data-primary="data" data-secondary="distributed object storage server" id="idm45831180014472"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="shared storage" id="idm45831180013512"></a><a data-type="indexterm" data-primary="storage" data-secondary="distributed object storage server" id="idm45831180012568"></a> today is to keep data in external storage. Different cloud providers have different solutions, like Amazon S3, Azure Data Storage, Google Cloud Storage, etc. The variety of solutions makes it complex to port solutions from one cloud provider to another.
To minimize this dependency, Kubeflow ships with MinIO, a high-performance distributed object storage server, designed for large-scale private cloud infrastructure. Not just for private clouds, MinIO can also act as a consistent gateway to public APIs.</p>

<p>MinIO can be deployed in several different configurations. The default with Kubeflow is as a single container mode when MinIO runs using the Kubernetes built-in persistent storage on one container.
Distributed MinIO lets you pool multiple volumes into a single object storage service.<sup><a data-type="noteref" id="idm45831180010264-marker" href="#idm45831180010264">[1]</a></sup> It can also withstand multiple node failures and yet ensure full data protection (the number of failures depends on your replication configuration). MinIO Gateway provides S3 APIs on top of Azure Blob storage, Google Cloud storage, Gluster, or NAS storage. The gateway option is the most flexible, and allows you to create cloud independent implementation without scale limits.</p>

<p>While Kubeflow’s default MinIO setup works, you will likely want to configure it further. <a data-type="indexterm" data-primary="user interfaces (UI)" data-secondary="MinIO to explore storage" id="idm45831180008680"></a><a data-type="indexterm" data-primary="storage" data-secondary="UI to explore" id="idm45831180007784"></a>Kubeflow installs both the MinIO server and UI. You can get access to the MinIO UI and explore what is stored, as seen in <a data-type="xref" href="#ex_minio_ui">FIGURE 3-4</a>, by using port-forwarding, as in <a data-type="xref" href="#ex_minio_fwd">EXAMPLE 3-1</a>, or exposing an ingress. You can log in using Kubeflow’s default minio/minio123 user.</p>
<div id="ex_minio_fwd" data-type="example" title2="Setting up port-forwarding" no2="3-1">
<h5>Example 3-1. Setting up port-forwarding</h5>

<pre data-type="programlisting" data-code-language="bash">kubectl port-forward -n kubeflow svc/minio-service 9000:9000 <code class="p">&amp;</code></pre></div>

<figure><div id="ex_minio_ui" class="figure" data-type="figure" title2="MinIO dashboard" no2="3-4">
<img src="assets/kfml_03in01.png" alt="Minio dashboard" width="1440" height="528">
<h6>Figure 3-4. MinIO dashboard</h6>
</div></figure>

<p>In addition, you can also install the <a href="https://oreil.ly/_AAEv">MinIO CLI (mc)</a> to access your MinIO installation using commands from your workstation. For macOS, use Homebrew, as in <a data-type="xref" href="#minio_homebrew">EXAMPLE 3-2</a>. For Linux Ubuntu, use snap, as in <a data-type="xref" href="#minio_linux">EXAMPLE 3-3</a>.</p>
<div id="minio_homebrew" data-type="example" title2="Install MinIO on Mac" no2="3-2">
<h5>Example 3-2. Install MinIO on Mac</h5>

<pre data-type="programlisting" data-code-language="bash">brew install minio/stable/minio</pre></div>
<div id="minio_linux" data-type="example" title2="Install MinIO on Linux" no2="3-3">
<h5>Example 3-3. Install MinIO on Linux</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="nb">pushd</code> ~/bin
wget https://dl.min.io/client/mc/release/linux-amd64/mc
chmod a+x mc</pre></div>

<p>You need to configure MinIO to talk to the correct endpoint, as in <a data-type="xref" href="#minio_endpoint">EXAMPLE 3-4</a>.</p>
<div id="minio_endpoint" data-type="example" title2="Configure MinIO client to talk to Kubeflow’s MinIO" no2="3-4">
<h5>Example 3-4. Configure MinIO client to talk to Kubeflow’s MinIO</h5>

<pre data-type="programlisting" data-code-language="bash">mc config host add minio http://localhost:9000 minio minio123</pre></div>

<p>Once you’ve configured the command line you can make new buckets, as in <a data-type="xref" href="#minio_bucket">EXAMPLE 3-5</a>, or change your setup.</p>
<div id="minio_bucket" data-type="example" title2="Create a bucket with MinIO" no2="3-5">
<h5>Example 3-5. Create a bucket with MinIO</h5>

<pre data-type="programlisting" data-code-language="bash">mc mb minio/kf-book-examples</pre></div>

<p>MinIO exposes both native and S3-compatible APIs. The S3-compatible APIs are most important since most of our software can talk to S3, like TensorFlow and Spark.</p>
<div data-type="warning"><h6>Warning</h6>
<p>Using MinIO with systems built on top of Hadoop (mostly Java-based)<a data-type="indexterm" data-primary="MinIO" data-secondary="Hadoop version for" id="idm45831179911048"></a> requires Hadoop 2.8 or higher.</p>
</div>

<p>Kubeflow installation hardcodes MinIO credentials—minio/minio123, which you can use directly in your applications—but<a data-type="indexterm" data-primary="credentials for MinIO" id="idm45831179909128"></a><a data-type="indexterm" data-primary="MinIO" data-secondary="secrets for credentials" id="idm45831179908424"></a><a data-type="indexterm" data-primary="YAML" data-secondary="secrets for MinIO credentials" id="idm45831179907480"></a> it’s generally a better practice to use a secret, especially if you might switch to regular S3.
Kubernetes secrets provide you with a way to store credentials on the cluster separate from your application.<sup><a data-type="noteref" id="idm45831179889512-marker" href="#idm45831179889512">[2]</a></sup> To set one up for MinIO or S3, create a secret file like in <a data-type="xref" href="#minio_secret">EXAMPLE 3-6</a>. In Kubernetes secret values for the ID and key have to be base64 encoded. To encode a value, run the command <code>echo -n <em>xxx</em> | base64</code>.</p>
<div id="minio_secret" data-type="example" title2="Sample MinIO secret" no2="3-6">
<h5>Example 3-6. Sample MinIO secret</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Secret</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">minioaccess</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">mynamespace</code>
<code class="nt">data</code><code class="p">:</code>
  <code class="nt">AWS_ACCESS_KEY_ID</code><code class="p">:</code> <code class="l-Scalar-Plain">xxxxxxxxxx</code>
  <code class="nt">AWS_SECRET_ACCESS_KEY</code><code class="p">:</code> <code class="l-Scalar-Plain">xxxxxxxxxxxxxxxxxxxxx</code></pre></div>

<p>Save this YAML to the file <em>minioaccess.yaml</em>, and deploy the secret using the command <code>kubectl apply -f minioaccess.yaml</code>. Now that we understand data communication between pipeline stages, let’s work to understand network communication between components.<a data-type="indexterm" data-startref="ch03-min" id="idm45831179861336"></a><a data-type="indexterm" data-startref="ch03-min2" id="idm45831179860728"></a></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Istio"><div class="sect2" id="idm45831179859864" title2="Istio" no2="3.2.2">
<h2>3.2.2. Istio</h2>

<p>Another supporting component of Kubeflow is <a href="https://istio.io">Istio</a>—a <a data-type="indexterm" data-primary="Istio" data-secondary="about" id="idm45831179855816"></a><a data-type="indexterm" data-primary="service mesh" data-secondary="components, with Istio" id="idm45831179854808"></a>service mesh providing such vital features as service discovery, load balancing, failure recovery, metrics, monitoring, rate limiting, access control, and end-to-end authentication. Istio, as a service mesh, layers transparently onto a Kubernetes cluster. It integrates into any logging platform, or telemetry or policy system and promotes a uniform way to secure, connect, and monitor microservices. Istio implementation co-locates each service instance with a sidecar network proxy. All network traffic (HTTP, REST, gRPC, etc.) from an individual service instance flows via its local sidecar proxy to the appropriate destination. Thus, the service instance is not aware of the network at large and only knows about its local proxy. In effect, the distributed system network has been abstracted away from the service programmer.</p>

<p>Istio implementation is logically split into a data plane and control plane. The data plane is composed of a set of intelligent proxies. These proxies mediate and control all network communication between pods. The control plane manages and configures the proxies to route traffic.</p>

<p class="less_space pagebreak-before">The main components of Istio are:</p>
<dl>
<dt><a href="https://oreil.ly/7i49v">Envoy</a></dt>
<dd>
<p>Istio data plane is based on Envoy proxy, which provides features like failure handling (for example, health checks and bounded retries), dynamic service discovery, and load balancing. Envoy has many built-in features, including:</p>

<ul>
<li>
<p>Dynamic service discovery</p>
</li>
<li>
<p>Load balancing</p>
</li>
<li>
<p>TLS termination</p>
</li>
<li>
<p>HTTP/2 and gRPC proxies</p>
</li>
<li>
<p>Circuit breakers</p>
</li>
<li>
<p>Health checks</p>
</li>
<li>
<p>Staged rollouts with percent-based traffic splitting</p>
</li>
<li>
<p>Fault injection</p>
</li>
<li>
<p>Rich metrics</p>
</li>
</ul>
</dd>
<dt><a href="https://oreil.ly/NV5xk">Mixer</a></dt>
<dd>
<p>Mixer enforces access control and usage policies across the service mesh, and collects telemetry data from the Envoy proxy and other services. The proxy extracts request-level attributes, and sends them to Mixer for evaluation.</p>
</dd>
<dt><a href="https://oreil.ly/lIAq_">Pilot</a></dt>
<dd>
<p>Pilot provides service discovery for the Envoy sidecars and traffic management capabilities for intelligent routing (e.g., A/B tests, canary rollouts) and resiliency (timeouts, retries, circuit breakers, etc.). This is done by converting high-level routing rules that control traffic behavior into Envoy-specific configurations, and propagating them to the sidecars at runtime. Pilot abstracts platform-specific service discovery mechanisms and synthesizes them into a standard format that any sidecar conforming with the Envoy data plane APIs can consume.</p>
</dd>
<dt><a href="https://oreil.ly/gZdIY">Galley</a></dt>
<dd>
<p>Galley is Istio’s configuration validation, ingestion, processing, and distribution component. It is responsible for insulating the rest of the Istio components from the details of obtaining user configuration from the underlying platform.</p>
</dd>
<dt><a href="https://oreil.ly/sLh70">Citadel</a></dt>
<dd>
<p>Citadel enables strong service-to-service and end-user authentication by providing identity and credential management. It allows for upgrading unencrypted traffic in the service mesh. Using Citadel, operators can enforce policies based on service identity rather than on relatively unstable layer 3 or layer 4 network 
identifiers.</p>
</dd>
</dl>

<p>Istio’s overall architecture is illustrated in <a data-type="xref" href="#fig_istio_arch">FIGURE 3-5</a>.</p>

<figure><div id="fig_istio_arch" class="figure" data-type="figure" title2="Istio architecture" no2="3-5">
<img src="assets/kfml_0304.png" alt="Istio Architecture" width="1402" height="1062">
<h6>Figure 3-5. Istio architecture</h6>
</div></figure>

<p>Kubeflow uses Istio to provide a proxy to the Kubeflow UI and to route requests appropriately and securely.
Kubeflow’s KFServing leverages Knative, which requires a service mesh, like Istio.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Knative"><div class="sect2" id="knative_serving" title2="Knative" no2="3.2.3">
<h2>3.2.3. Knative</h2>

<p>Another unseen support component used by Kubeflow is Knative. We will begin<a data-type="indexterm" data-primary="Knative" data-secondary="Serving" id="idm45831179799128"></a><a data-type="indexterm" data-primary="serverless applications" data-secondary="Knative Serving" id="idm45831179798152"></a><a data-type="indexterm" data-primary="Knative" data-secondary="components in Kubeflow" id="idm45831179797208"></a> by describing the most important part: Knative Serving. Built on Kubernetes and Istio, <a href="https://oreil.ly/fcndQ">Knative Serving</a> supports the deploying and serving of serverless applications. The Knative Serving project provides middleware primitives that enable:</p>

<ul>
<li>
<p>Rapid deployment of serverless containers</p>
</li>
<li>
<p>Automatic scaling up and down to zero</p>
</li>
<li>
<p>Routing and network programming for Istio components</p>
</li>
<li>
<p>Point-in-time snapshots of deployed code and configurations</p>
</li>
</ul>

<p class="less_space pagebreak-before">Knative Serving is implemented as a set of Kubernetes CRDs.<a data-type="indexterm" data-primary="custom resource definitions (CRDs)" data-secondary="Knative Serving" id="idm45831179790536"></a><a data-type="indexterm" data-primary="CRDs" data-see="custom resource definitions" id="idm45831179789544"></a> These objects are used to define and control behavior of a serverless workload:</p>
<dl>
<dt><a href="https://oreil.ly/EbQRg">Service</a></dt>
<dd>
<p>The <code>service.serving.knative.dev</code> resource manages the workload as a whole. It orchestrates the creation and execution of other objects to ensure that an app has a configuration, a route, and a new revision for each update of the service. Service can be defined to always route traffic to the latest revision or to a specified revision.</p>
</dd>
<dt><a href="https://oreil.ly/FH50y">Route</a></dt>
<dd>
<p>The <code>route.serving.knative.dev</code> resource maps a network endpoint to one or more revisions. This allows for multiple traffic management approaches, including fractional traffic and named routes.</p>
</dd>
<dt><a href="https://oreil.ly/cNsj3">Configuration</a></dt>
<dd>
<p>The <code>configuration.serving.knative.dev</code> resource maintains the desired state for deployment. It provides a clean separation between code and configuration and follows the Twelve-Factor App methodology. Modifying a configuration creates a new revision.</p>
</dd>
<dt><a href="https://oreil.ly/jxpW1">Revision</a></dt>
<dd>
<p>The <code>revision.serving.knative.dev</code> resource is a point-in-time snapshot of the code and configuration for each modification made to the workload. Revisions are immutable objects and can be retained for as long as is useful. Knative Serving Revisions can be automatically scaled up and down according to incoming traffic.</p>
</dd>
</dl>

<p>Knative’s overall architecture is illustrated in <a data-type="xref" href="#fig_knative_arch">FIGURE 3-6</a>.<a data-type="indexterm" data-primary="Knative" data-secondary="architecture" id="idm45831179777000"></a></p>

<figure><div id="fig_knative_arch" class="figure" data-type="figure" title2="Knative architecture" no2="3-6">
<img src="assets/kfml_0305.png" alt="Knative Architecture" width="785" height="846">
<h6>Figure 3-6. Knative architecture</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Apache Spark"><div class="sect2" id="idm45831179773960" title2="Apache Spark" no2="3.2.4">
<h2>3.2.4. Apache Spark</h2>

<p>A more visible supporting component in Kubeflow is Apache Spark.<a data-type="indexterm" data-primary="Apache Spark" data-secondary="about" id="idm45831179772632"></a><a data-type="indexterm" data-primary="Apache Spark in Kubeflow" data-secondary="components" id="idm45831179771656"></a> Starting in Kubeflow 1.0, Kubeflow has a built-in Spark operator for running Spark jobs. In addition to the Spark operator, Kubeflow provides integration for using Google’s Dataproc and Amazon’s Elastic Map Reduce (EMR), two managed cloud services for running Spark. The components and the operator are focused on production use and are not well suited to exploration. For exploration, you can use Spark inside of your Jupyter notebook.</p>

<p>Apache Spark allows you to handle larger datasets and scale problems that cannot fit on a single machine. While Spark does have its own machine learning libraries, it is more commonly used as part of a machine learning pipeline for data or feature preparation. We cover Spark in more detail in <a data-type="xref" href="#data_and_feature_prep">CHAPTER 5</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Kubeflow Multiuser Isolation"><div class="sect2" id="idm45831179768328" title2="Kubeflow Multiuser Isolation" no2="3.2.5">
<h2>3.2.5. Kubeflow Multiuser Isolation</h2>

<p>The latest version of Kubeflow introduced multiuser isolation, which allows<a data-type="indexterm" data-primary="multiuser isolation" id="idm45831179766824"></a><a data-type="indexterm" data-primary="components" data-secondary="multiuser isolation" id="idm45831179766120"></a><a data-type="indexterm" data-primary="profiles" data-secondary="multiuser isolation" id="idm45831179765176"></a> sharing the same pool of resources across different teams and users. Multiuser isolation provides users with a reliable way to isolate and protect their own resources, without accidentally viewing or changing each other’s resources. The key concepts of such isolation are:</p>
<dl>
<dt>Administrator</dt>
<dd>
<p>An administrator is someone who creates and maintains the Kubeflow<a data-type="indexterm" data-primary="administrator of Kubeflow cluster" id="idm45831179762120"></a> cluster. This person has permission to grant access permissions to others.</p>
</dd>
<dt>User</dt>
<dd>
<p>A user is someone who has access to some set of resources in the<a data-type="indexterm" data-primary="user of Kubeflow cluster" id="idm45831179760040"></a> cluster. A user needs to be granted access permissions by the administrator.</p>
</dd>
<dt>Profile</dt>
<dd>
<p>A profile is a grouping of all Kubernetes namespaces and resources<a data-type="indexterm" data-primary="profiles" data-secondary="definition" id="idm45831179757912"></a><a data-type="indexterm" data-primary="namespaces" data-secondary="profile definition" id="idm45831179756856"></a> owned by a user.</p>
</dd>
</dl>

<p>As of version 1.0, Kubeflow’s Jupyter notebook service is the first application<a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="multiuser isolation" id="idm45831179755272"></a> to be fully integrated with multiuser isolation. Notebooks and their creation are controlled by the profile access policies set by the administrator or the owners of the profiles. Resources created by the notebooks (e.g., training jobs and deployments) will also inherit the same access.
By default, Kubeflow provides automatic profile creation for authenticated users<a data-type="indexterm" data-primary="profiles" data-secondary="automatic creation" id="idm45831179753752"></a> on first login,<sup><a data-type="noteref" id="idm45831179752680-marker" href="#idm45831179752680">[3]</a></sup> which creates a new namespace. Alternatively, profiles for users can be created <a href="https://oreil.ly/6aklV">manually</a>. This means that every user can work independently in their own namespace and use their own Jupyter server and notebooks. To share access to your server/notebooks with others, go to the manage contributors page and add your collaborators’ emails.</p>
<aside data-type="sidebar"><div class="sidebar" id="idm45831179750472">
<h5>Kubeflow’s Repositories</h5>
<p>As you’ve seen, Kubeflow is comprised of a number of different components.<a data-type="indexterm" data-primary="components" data-secondary="repositories" id="idm45831179749176"></a><a data-type="indexterm" data-primary="repositories" data-secondary="components" id="idm45831179747848"></a><a data-type="indexterm" data-primary="resources for Kubeflow" data-secondary="component repositories" id="idm45831179746904"></a> These components are hosted under the <a href="https://oreil.ly/LxSB5">Kubeflow GitHub organization</a>.
The most important repositories to be familiar with are <code>kfctl</code>, which is hosted in the<a data-type="indexterm" data-primary="kfctl repository" id="idm45831179744520"></a><a data-type="indexterm" data-primary="repositories" data-secondary="kfctl" id="idm45831179743816"></a> <a href="https://oreil.ly/aGAsV">kfctl repo</a>, and<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="Service" data-tertiary="repository" id="idm45831179742056"></a><a data-type="indexterm" data-primary="repositories" data-secondary="Pipelines Service" id="idm45831179740776"></a> Kubeflow Pipelines, <a href="https://oreil.ly/SXu1d">in the pipelines repo</a>. The pipelines repo is especially important as its prebuilt components can save you time.
Using the other components does not require explicit installation, but looking at the components issues, <a href="https://oreil.ly/OGJYQ">like in Katib</a>, can be useful to check for known workarounds for any problems you encounter.</p>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45831179738104" title2="Conclusion" no2="3.3">
<h1>3.3. Conclusion</h1>

<p>You now know the different components of Kubeflow and how they fit together.
Kubeflow’s central dashboard gives you access to its web components.
You’ve seen that JupyterHub facilitates the explorative phase of model development.
We’ve covered the different built-in training operators for Kubeflow.
We revisited Kubeflow pipelines to discuss how they tie together all of Kubeflow’s other components.
We introduced Katib, Kubeflow’s tool for hyperparameter tuning that works on pipelines.
We talked about the different options for serving your models with Kubeflow (including KF Serving and Seldon).
We discussed Kubeflow’s system for tracking your machine learning metadata and artifacts.
Then we wrapped it up with some of Kubeflow’s supporting components that enable the rest, Knative and Istio.
By understanding the different parts of Kubeflow, as well as the overall design, you should now be able to start seeing how your machine learning tasks and workflow translate to 
Kubeflow.</p>

<p>The next few chapters will help you gain insights into these components and how to apply them to your use cases.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45831180010264"><sup><a href="#idm45831180010264-marker">[1]</a></sup> This can run on multiple servers while exposing a consistent endpoint.</p><p data-type="footnote" id="idm45831179889512"><sup><a href="#idm45831179889512-marker">[2]</a></sup> Storing credentials inside your application can lead to security breaches.</p><p data-type="footnote" id="idm45831179752680"><sup><a href="#idm45831179752680-marker">[3]</a></sup> To enable users to log in, they should be given minimal permission scope that allows them to connect to the Kubernetes cluster. For example, for GCP users, they can be granted IAM roles: Kubernetes Engine Cluster Viewer and IAP-secured Web App User.</p></div></div></section>
<section data-type="chapter" data-pdf-bookmark="Chapter 4. Kubeflow Pipelines"><div class="chapter" id="pipelines_ch" data-type="chapter" title2="Kubeflow Pipelines" no2="4">
<h1>Chapter 4. Kubeflow Pipelines</h1>


<p>In the previous chapter we described <a href="https://oreil.ly/387tH">Kubeflow Pipelines</a>,<a data-type="indexterm" data-primary="components" data-secondary="pipelines" id="idm45831179731576"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="about" id="idm45831179730568"></a>
 the component of Kubeflow that orchestrates machine learning applications. Orchestration is necessary
because a typical machine learning implementation uses a combination of tools to prepare data, train the
model, evaluate performance, and deploy. By formalizing the steps and their sequencing in code, pipelines allow users
to formally capture all of the data processing steps, ensuring their reproducibility and auditability, and training and
deployment steps.</p>

<p>We will start this chapter by taking a look at the Pipelines UI and showing how to start writing simple pipelines in
Python. We’ll explore how to transfer data between stages, then continue
by getting into ways of leveraging existing applications as part of a pipeline. We will also look at the underlying workflow
engine—Argo Workflows, a standard Kubernetes pipeline tool—that Kubeflow uses to run pipelines. Understanding the basics of
Argo Workflows allows you to gain a deeper understanding of Kubeflow Pipelines and will aid in debugging. We will then show what
Kubeflow Pipelines adds to Argo.</p>

<p>We’ll wrap up Kubeflow Pipelines by showing how to implement conditional execution in pipelines and how to run
pipelines execution on schedule. Task-specific components of pipelines will be covered in their respective chapters.</p>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Getting Started with Pipelines"><div class="sect1" id="idm45831179727224" title2="Getting Started with Pipelines" no2="4.1">
<h1>4.1. Getting Started with Pipelines</h1>

<p>The Kubeflow Pipelines platform consists of:</p>

<ul>
<li>
<p>A UI for managing and tracking pipelines and their execution</p>
</li>
<li>
<p>An engine for scheduling a pipeline’s execution</p>
</li>
<li>
<p>An SDK for defining, building, and deploying pipelines in Python</p>
</li>
<li>
<p>Notebook support for using the SDK and pipeline execution</p>
</li>
</ul>

<p>The easiest way to familiarize yourself with pipelines is to take a look at prepackaged examples.</p>








<section data-type="sect2" data-pdf-bookmark="Exploring the Prepackaged Sample Pipelines"><div class="sect2" id="idm45831179720536" title2="Exploring the Prepackaged Sample Pipelines" no2="4.1.1">
<h2>4.1.1. Exploring the Prepackaged Sample Pipelines</h2>

<p>To help users understand pipelines, Kubeflow installs with a few sample pipelines. You can find these prepackaged<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="exploring sample" id="idm45831179718488"></a><a data-type="indexterm" data-primary="user interfaces (UI)" data-secondary="Kubeflow Pipelines UI" id="idm45831179717512"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="UI" id="idm45831179716568"></a>
in the Pipeline web UI, as seen in<a data-type="indexterm" data-primary="web UI for Pipeline" id="idm45831179715496"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="generic versus Google Kubernetes Engine" id="idm45831179714824"></a><a data-type="indexterm" data-primary="Google Kubernetes Engine (GKE)" id="idm45831179713816"></a> <a data-type="xref" href="#kubeflow_pipelines_ui_prepackage_pipelines">FIGURE 4-1</a>. Note that at the time of writing, only the Basic to Conditional execution
pipelines are generic, while the rest of them will run only on Google Kubernetes Engine (GKE). If you try to run them on non-GKE environments, they will fail.</p>

<figure><div id="kubeflow_pipelines_ui_prepackage_pipelines" class="figure" data-type="figure" title2="Kubeflow pipelines UI: prepackaged pipelines" no2="4-1">
<img src="assets/kfml_0401.png" alt="Kubeflow Pipelines UI - Prepackaged Pipelines" width="1440" height="633">
<h6>Figure 4-1. Kubeflow pipelines UI: prepackaged pipelines</h6>
</div></figure>

<p class="less_space pagebreak-before">Clicking a specific pipeline will show its execution graph or source, as seen in <a data-type="xref" href="#kubeflow_pipelines_ui_pipeline_graph_view">FIGURE 4-2</a>.</p>

<figure><div id="kubeflow_pipelines_ui_pipeline_graph_view" class="figure" data-type="figure" title2="Kubeflow pipelines UI: pipeline graph view" no2="4-2">
<img src="assets/kfml_0402.png" alt="Kubeflow Pipelines UI - Pipeline Graph View" width="1440" height="1228">
<h6>Figure 4-2. Kubeflow pipelines UI: pipeline graph view</h6>
</div></figure>

<p>Clicking the source tab will show the pipeline’s compiled code,<a data-type="indexterm" data-primary="YAML" data-secondary="DSL compiler producing" id="idm45831179705912"></a><a data-type="indexterm" data-primary="Argo Workflows" data-secondary="YAML files for pipelines" id="idm45831179704856"></a> which is an Argo YAML file (this is covered in more detail in <a data-type="xref" href="#argo_foundation">SECTION 4.2.1</a>).</p>

<p>In this area you are welcome to experiment with running pipelines to get a better feel for their execution and the capabilities of the Pipelines UI.</p>

<p class="less_space pagebreak-before">To invoke a specific pipeline, simply click it; this will bring up Pipeline’s view as presented in <a data-type="xref" href="#kubeflow_pipelines_ui_pipeline_view">FIGURE 4-3</a>.</p>

<figure><div id="kubeflow_pipelines_ui_pipeline_view" class="figure" data-type="figure" title2="Kubeflow pipelines UI: pipeline view" no2="4-3">
<img src="assets/kfml_0403.png" alt="Kubeflow Pipelines UI - Pipeline View" width="1420" height="1025">
<h6>Figure 4-3. Kubeflow pipelines UI: pipeline view</h6>
</div></figure>

<p>To run the pipeline, click the “Create Run” button and follow the<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="running" id="idm45831179698232"></a> instructions on the screen.</p>
<div data-type="tip"><h6>Tip</h6>
<p>When running a pipeline you must choose an experiment.<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="experiments" id="idm45831179695960"></a><a data-type="indexterm" data-primary="experiments" id="idm45831179694984"></a> Experiment here is just a convenience grouping for pipeline
executions (runs). You can always use the “Default” experiment created by Kubeflow’s installation. Also, pick “One-off”
for the Run type to execute the pipeline once. We will talk about recurring execution in <a data-type="xref" href="#run_pipelines_onsched">SECTION 4.3.2</a>.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Building a Simple Pipeline in Python"><div class="sect2" id="idm45831179719912" title2="Building a Simple Pipeline in Python" no2="4.1.2">
<h2>4.1.2. Building a Simple Pipeline in Python</h2>

<p>We have seen how to execute precompiled Kubeflow Pipelines, now let’s investigate how to author our own new pipelines.<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="building examples in Python" id="ch04-ex"></a><a data-type="indexterm" data-primary="Python" data-secondary="building example pipelines" id="ch04-ex2"></a><a data-type="indexterm" data-primary="Argo Workflows" data-secondary="executing pipeline YAML files" id="idm45831179688344"></a>
Kubeflow Pipelines are stored as YAML files executed by a program called Argo (see <a data-type="xref" href="#argo_foundation">SECTION 4.2.1</a>).
Thankfully, Kubeflow exposes a Python <a href="https://oreil.ly/7LdzK">domain-specific language (DSL)</a> for authoring pipelines.
The DSL is a Pythonic representation of the operations performed in the ML workflow and built with ML workloads specifically in mind.
The DSL also allows for some simple Python functions to be used as pipeline stages without you having to explicitly build a container.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The Chapter 4 examples can be found in the notebooks in <a href="https://oreil.ly/Kubeflow_for_ML_ch04">this book’s GitHub repository</a>.</p>
</div>

<p>A pipeline is, in its essence, a graph of container execution.  In addition<a data-type="indexterm" data-primary="containers" data-secondary="building example pipeline" id="idm45831179682888"></a> to specifying which containers should run
in which order, it also allows the user to pass arguments to the entire pipeline and between participating containers.</p>

<p>For each container (when using the Python SDK), we must:</p>

<ul>
<li>
<p>Create the container—either as a simple Python function, or with any Docker container (read more in <a data-type="xref" href="#beyond_tf">CHAPTER 9</a>).</p>
</li>
<li>
<p>Create an operation that references that container as well as the command line arguments, data mounts, and variable to pass the container.</p>
</li>
<li>
<p>Sequence the operations, defining which may happen in parallel and which must complete before moving on to a further step.<sup><a data-type="noteref" id="idm45831179676952-marker" href="#idm45831179676952">[1]</a></sup></p>
</li>
<li>
<p>Compile this pipeline, defined in Python, into a YAML file that Kubeflow Pipelines can consume.</p>
</li>
</ul>

<p>Pipelines are a key feature of Kubeflow and you will see them again throughout the book. In this chapter we are going to
show the simplest examples possible to illustrate the basic principles of Pipelines. This won’t feel like “machine
learning” and that is by design.</p>

<p>For our first Kubeflow operation, we are going to use a technique known as <em>lightweight Python functions</em>.<a data-type="indexterm" data-primary="Python" data-secondary="lightweight Python functions" id="ch04-lit2"></a><a data-type="indexterm" data-primary="lightweight Python functions" id="ch04-lit"></a> We should not,
however, let the word <em>lightweight</em> deceive us. In a lightweight Python function, we define a Python function and
then let Kubeflow take care of packaging that function into a container and creating an operation.</p>

<p>For the sake of simplicity, let’s declare the simplest of functions an echo. That is a function that takes a single
input, an integer, and returns that input.</p>

<p>Let’s start by importing <code>kfp</code> and defining our function:</p>

<pre data-type="programlisting" data-code-language="python" id="untitled_programlisting_3" title2="(no caption)" no2=""><code class="kn">import</code> <code class="nn">kfp</code>
<code class="k">def</code> <code class="nf">simple_echo</code><code class="p">(</code><code class="n">i</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>
    <code class="k">return</code> <code class="n">i</code></pre>
<div data-type="warning"><h6>Warning</h6>
<p>Note that we use <code>snake_case</code>, not <code>camelCase</code>, for our function names. At the time of writing there exists a bug
(feature?)<a data-type="indexterm" data-primary="Python" data-secondary="camelCase function name bug" id="idm45831179649080"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="camelCase function name bug in DSL" id="idm45831179648136"></a> such that camel case names (for example: naming our function <code>simpleEcho</code>) will produce errors.</p>
</div>

<p>Next, we want to wrap our function <code>simple_echo</code> into a Kubeflow Pipeline operation. There’s a nice little
method to do this: <code>kfp.components.func_to_container_op</code>. This method returns a factory function with a strongly typed signature:</p>

<pre data-type="programlisting" data-code-language="python" id="untitled_programlisting_4" title2="(no caption)" no2=""><code class="n">simpleStronglyTypedFunction</code> <code class="o">=</code>
  <code class="n">kfp</code><code class="o">.</code><code class="n">components</code><code class="o">.</code><code class="n">func_to_container_op</code><code class="p">(</code><code class="n">deadSimpleIntEchoFn</code><code class="p">)</code></pre>

<p>When we create a pipeline in the next step, the factory function will construct a ContainerOp, which will run the original function (echo_fn) in a container:</p>

<pre data-type="programlisting" data-code-language="python" id="untitled_programlisting_5" title2="(no caption)" no2=""><code class="n">foo</code> <code class="o">=</code> <code class="n">simpleStronglyTypedFunction</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="nb">type</code><code class="p">(</code><code class="n">foo</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="n">kfp</code><code class="o">.</code><code class="n">dsl</code><code class="o">.</code><code class="n">_container_op</code><code class="o">.</code><code class="n">ContainerOp</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>If your code can be accelerated by a GPU it is easy to mark a stage as using <a data-type="indexterm" data-primary="GPUs" data-secondary="resource marking in code" id="idm45831179600824"></a><a data-type="indexterm" data-primary="Python" data-secondary="GPU resource marking" id="idm45831179599944"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="GPU resource marking in DSL" id="idm45831179599000"></a><a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="GPU resources" id="idm45831179598088"></a>GPU resources; simply add <code>.set_gpu_limit(NUM_GPUS)</code> to your <code>ContainerOp</code>.</p>
</div>

<p>Now let’s sequence the ContainerOp(s) (there is only one) into a pipeline. This pipeline will take one parameter (the
number we will echo). The pipeline also has a bit of metadata associated with it. While echoing numbers may be a
trivial use of parameters, in real-world use cases you would include variables you might want to tune later such as
hyperparameters for machine learning algorithms.</p>

<p>Finally, we compile our pipeline into a zipped YAML file,<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="compiler" id="idm45831179568968"></a><a data-type="indexterm" data-primary="YAML" data-secondary="DSL compiler producing" id="idm45831179567992"></a><a data-type="indexterm" data-primary="user interfaces (UI)" data-secondary="Kubeflow Pipelines UI" id="idm45831179567048"></a> which we can then upload to the Pipelines UI.</p>

<pre data-type="programlisting" data-code-language="python" id="untitled_programlisting_6" title2="(no caption)" no2=""><code class="nd">@kfp.dsl.pipeline</code><code class="p">(</code>
  <code class="n">name</code><code class="o">=</code><code class="s1">'Simple Echo'</code><code class="p">,</code>
  <code class="n">description</code><code class="o">=</code><code class="s1">'This is an echo pipeline. It echoes numbers.'</code>
<code class="p">)</code>
<code class="k">def</code> <code class="nf">echo_pipeline</code><code class="p">(</code><code class="n">param_1</code><code class="p">:</code> <code class="n">kfp</code><code class="o">.</code><code class="n">dsl</code><code class="o">.</code><code class="n">PipelineParam</code><code class="p">):</code>
  <code class="n">my_step</code> <code class="o">=</code> <code class="n">simpleStronglyTypedFunction</code><code class="p">(</code><code class="n">i</code><code class="o">=</code> <code class="n">param_1</code><code class="p">)</code>

<code class="n">kfp</code><code class="o">.</code><code class="n">compiler</code><code class="o">.</code><code class="n">Compiler</code><code class="p">()</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">echo_pipeline</code><code class="p">,</code>
  <code class="s1">'echo-pipeline.zip'</code><code class="p">)</code></pre>
<div data-type="tip"><h6>Tip</h6>
<p>It is also possible to run the pipeline directly from the notebook, which we’ll do in the next example.</p>
</div>

<p>A pipeline with only one component is not very interesting. For our next example, we will customize the containers of our lightweight Python functions. We’ll create a new pipeline that installs and imports additional Python libraries, builds from a specified base image, and passes output between 
containers.</p>

<p>We are going to create a pipeline that divides a number by another number, and then adds a third number. First let’s create our simple <code>add</code> function, as shown in <a data-type="xref" href="#simple_python_function">EXAMPLE 4-1</a>.<a data-type="indexterm" data-startref="ch04-ex" id="idm45831179485976"></a><a data-type="indexterm" data-startref="ch04-ex2" id="idm45831179485272"></a></p>
<div id="simple_python_function" data-type="example" title2="A simple Python function" no2="4-1">
<h5>Example 4-1. A simple Python function</h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">add</code><code class="p">(</code><code class="n">a</code><code class="p">:</code> <code class="nb">float</code><code class="p">,</code> <code class="n">b</code><code class="p">:</code> <code class="nb">float</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">float</code><code class="p">:</code>
   <code class="sd">'''Calculates sum of two arguments'''</code>
   <code class="k">return</code> <code class="n">a</code> <code class="o">+</code> <code class="n">b</code>

<code class="n">add_op</code> <code class="o">=</code> <code class="n">comp</code><code class="o">.</code><code class="n">func_to_container_op</code><code class="p">(</code><code class="n">add</code><code class="p">)</code></pre></div>

<p>Next, let’s create a slightly more complex function. Additionally, let’s have this function require and import from a nonstandard <a data-type="indexterm" data-primary="Python" data-secondary="library imports" id="idm45831179459544"></a><a data-type="indexterm" data-primary="libraries" data-secondary="importing" id="idm45831179469416"></a>Python library, <code>numpy</code>.  This must be done within the function. That is because global imports from the notebook will not be packaged into the containers we create. Of course, it is also important to make sure that our container has the libraries we are importing installed.</p>

<p>To do that we’ll pass the specific container we want to use as our base image to <code>.func_to_container(</code>, as in <a data-type="xref" href="#less_simple_pyfunction">EXAMPLE 4-2</a>.</p>
<div id="less_simple_pyfunction" data-type="example" title2="A less-simple Python function" no2="4-2">
<h5>Example 4-2. A less-simple Python function</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code><code> </code><code class="nn">typing</code><code> </code><code class="kn">import</code><code> </code><code class="n">NamedTuple</code><code>
</code><code class="k">def</code><code> </code><code class="nf">my_divmod</code><code class="p">(</code><code class="n">dividend</code><code class="p">:</code><code> </code><code class="nb">float</code><code class="p">,</code><code> </code><code class="n">divisor</code><code class="p">:</code><code class="nb">float</code><code class="p">)</code><code> </code><code class="o">-</code><code class="o">&gt;</code><code> </code><code>\
</code><code>       </code><code class="n">NamedTuple</code><code class="p">(</code><code class="s1">'</code><code class="s1">MyDivmodOutput</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="p">[</code><code class="p">(</code><code class="s1">'</code><code class="s1">quotient</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="nb">float</code><code class="p">)</code><code class="p">,</code><code> </code><code class="p">(</code><code class="s1">'</code><code class="s1">remainder</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="nb">float</code><code class="p">)</code><code class="p">]</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="sd">'''Divides two numbers and calculate  the quotient and remainder'''</code><code>
</code><code>    </code><code class="c1">#Imports inside a component function:</code><code>
</code><code>    </code><code class="kn">import</code><code> </code><code class="nn">numpy</code><code> </code><code class="kn">as</code><code> </code><code class="nn">np</code><code> </code><a class="co" id="co_kubeflow_pipelines_CO1-1" href="#callout_kubeflow_pipelines_CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"></a><code>
</code><code>
</code><code>    </code><code class="c1">#This function demonstrates how to use nested functions inside a</code><code>
</code><code>    </code><code class="c1"># component function:</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="nf">divmod_helper</code><code class="p">(</code><code class="n">dividend</code><code class="p">,</code><code> </code><code class="n">divisor</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" id="co_kubeflow_pipelines_CO1-2" href="#callout_kubeflow_pipelines_CO1-2"><img src="assets/2.png" alt="2" width="12" height="12"></a><code>
</code><code>	</code><code class="k">return</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">divmod</code><code class="p">(</code><code class="n">dividend</code><code class="p">,</code><code> </code><code class="n">divisor</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="p">(</code><code class="n">quotient</code><code class="p">,</code><code> </code><code class="n">remainder</code><code class="p">)</code><code> </code><code class="o">=</code><code> </code><code class="n">divmod_helper</code><code class="p">(</code><code class="n">dividend</code><code class="p">,</code><code> </code><code class="n">divisor</code><code class="p">)</code><code>
</code><code>
</code><code>    </code><code class="kn">from</code><code> </code><code class="nn">collections</code><code> </code><code class="kn">import</code><code> </code><code class="n">namedtuple</code><code>
</code><code>    </code><code class="n">divmod_output</code><code> </code><code class="o">=</code><code> </code><code class="n">namedtuple</code><code class="p">(</code><code class="s1">'</code><code class="s1">MyDivmodOutput</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="p">[</code><code class="s1">'</code><code class="s1">quotient</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">remainder</code><code class="s1">'</code><code class="p">]</code><code class="p">)</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">divmod_output</code><code class="p">(</code><code class="n">quotient</code><code class="p">,</code><code> </code><code class="n">remainder</code><code class="p">)</code><code>
</code><code>
</code><code class="n">divmod_op</code><code> </code><code class="o">=</code><code> </code><code class="n">comp</code><code class="o">.</code><code class="n">func_to_container_op</code><code class="p">(</code><code>
</code><code>                </code><code class="n">my_divmod</code><code class="p">,</code><code> </code><code class="n">base_image</code><code class="o">=</code><code class="s1">'</code><code class="s1">tensorflow/tensorflow:1.14.0-py3</code><code class="s1">'</code><code class="p">)</code><code> </code><a class="co" id="co_kubeflow_pipelines_CO1-3" href="#callout_kubeflow_pipelines_CO1-3"><img src="assets/3.png" alt="3" width="12" height="12"></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_kubeflow_pipelines_CO1-1" href="#co_kubeflow_pipelines_CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>Importing libraries inside the function.</p></dd>
<dt><a class="co" id="callout_kubeflow_pipelines_CO1-2" href="#co_kubeflow_pipelines_CO1-2"><img src="assets/2.png" alt="2" width="12" height="12"></a></dt>
<dd><p>Nested functions inside lightweight Python functions are also OK.</p></dd>
<dt><a class="co" id="callout_kubeflow_pipelines_CO1-3" href="#co_kubeflow_pipelines_CO1-3"><img src="assets/3.png" alt="3" width="12" height="12"></a></dt>
<dd><p>Calling for a specific base container.</p></dd>
</dl></div>

<p>Now we will build a pipeline. The pipeline in <a data-type="xref" href="#simple_pipeline">EXAMPLE 4-3</a> uses the functions defined previously, <code>my_divmod</code> and <code>add</code>, as stages.</p>
<div id="simple_pipeline" data-type="example" title2="A simple pipeline" no2="4-3">
<h5>Example 4-3. A simple pipeline</h5>

<pre data-type="programlisting">@dsl.pipeline(
   name='Calculation pipeline',
   description='A toy pipeline that performs arithmetic calculations.'
)
def calc_pipeline(
   a='a',
   b='7',
   c='17',
):
    #Passing pipeline parameter and a constant value as operation arguments
    add_task = add_op(a, 4) #Returns a dsl.ContainerOp class instance.

    #Passing a task output reference as operation arguments
    #For an operation with a single return value, the output
    # reference can be accessed using `task.output`
    # or `task.outputs['output_name']` syntax
    divmod_task = divmod_op(add_task.output, b) <a class="co" id="co_kubeflow_pipelines_CO2-1" href="#callout_kubeflow_pipelines_CO2-1"><img src="assets/1.png" alt="1" width="12" height="12"></a>

    #For an operation with multiple return values, the output references
    # can be accessed using `task.outputs['output_name']` syntax
    result_task = add_op(divmod_task.outputs['quotient'], c) <a class="co" id="co_kubeflow_pipelines_CO2-2" href="#callout_kubeflow_pipelines_CO2-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_kubeflow_pipelines_CO2-1" href="#co_kubeflow_pipelines_CO2-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>Values being passed between containers. Order of operations is inferred from this.</p></dd>
</dl></div>

<p>Finally, we use the client to submit the pipeline for execution, which returns the links to execution and experiment.<a data-type="indexterm" data-primary="experiments" id="idm45831179312920"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="experiments" id="idm45831179312280"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="running" id="idm45831179364952"></a>
Experiments group the executions together. You can also use <code>kfp.compiler.Compiler().compile</code> and upload the zip file as
in the first example if you prefer:</p>

<pre data-type="programlisting" data-code-language="python" id="untitled_programlisting_7" title2="(no caption)" no2=""><code class="n">client</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">Client</code><code class="p">()</code>
<code class="c1">#Specify pipeline argument values</code>
<code class="c1"># arguments = {'a': '7', 'b': '8'} #whatever makes sense for new version</code>
<code class="c1">#Submit a pipeline run</code>
<code class="n">client</code><code class="o">.</code><code class="n">create_run_from_pipeline_func</code><code class="p">(</code><code class="n">calc_pipeline</code><code class="p">,</code> <code class="n">arguments</code><code class="o">=</code><code class="n">arguments</code><code class="p">)</code></pre>

<p>Following the link returned by <code>create_run_from_pipeline_func</code>, we can get to the execution <a data-type="indexterm" data-primary="web UI for Pipeline" id="idm45831179320712"></a>web UI, which shows the
pipeline itself and intermediate results, as seen in <a data-type="xref" href="#pipeline_execution">FIGURE 4-4</a>.</p>

<figure><div id="pipeline_execution" class="figure" data-type="figure" title2="Pipeline execution" no2="4-4">
<img src="assets/kfml_0404.png" alt="Pipeline Execution" width="1921" height="987">
<h6>Figure 4-4. Pipeline execution</h6>
</div></figure>

<p>As we’ve seen, the <em>lightweight</em> in <em>lightweight Python functions</em> refers to the ease of making these steps in our process and not the power of the functions themselves. We can use custom imports, base images, and how to hand off small results between containers.<a data-type="indexterm" data-startref="ch04-lit" id="idm45831179285400"></a><a data-type="indexterm" data-startref="ch04-lit2" id="idm45831179284728"></a></p>

<p>In the next section, we’ll show how to hand larger data files between containers by mounting volumes to the containers.</p>
<aside data-type="sidebar"><div class="sidebar" id="idm45831179318344">
<h5>Using Annotations to Simplify Our Pipeline</h5>
<p>As you may have noticed, directly calling <code>comp.func_to_container_op</code> all the time can get kind of repetitive. To avoid<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="annotations" id="idm45831179316280"></a><a data-type="indexterm" data-primary="annotations on pipelines" id="idm45831179361288"></a> this, you can create a function that returns a <code>kfp.dsl.ContainerOp</code>. Since people don’t always like creating absurdly large and
fat functions to do everything in real life, we’ll leave this here as an aside in case the reader is interested in it.
It’s also worth noting that adding the <code>@kfp.dsl.component</code> annotation instructs the Kubeflow compiler to turn on static type checking:</p>

<pre data-type="programlisting" data-code-language="python" id="untitled_programlisting_8" title2="(no caption)" no2=""><code class="nd">@kfp.dsl.component</code>
<code class="k">def</code> <code class="nf">my_component</code><code class="p">(</code><code class="n">my_param</code><code class="p">):</code>
  <code class="o">...</code>
  <code class="k">return</code> <code class="n">kfp</code><code class="o">.</code><code class="n">dsl</code><code class="o">.</code><code class="n">ContainerOp</code><code class="p">(</code>
    <code class="n">name</code><code class="o">=</code><code class="s1">'My component name'</code><code class="p">,</code>
    <code class="n">image</code><code class="o">=</code><code class="s1">'gcr.io/path/to/container/image'</code>
  <code class="p">)</code></pre>

<p>Finally, when it comes to incorporating these components into pipelines, you would do something like this:</p>

<pre data-type="programlisting" data-code-language="python" id="untitled_programlisting_9" title2="(no caption)" no2=""><code class="nd">@kfp.dsl.pipeline</code><code class="p">(</code>
  <code class="n">name</code><code class="o">=</code><code class="s1">'My pipeline'</code><code class="p">,</code>
  <code class="n">description</code><code class="o">=</code><code class="s1">'My machine learning pipeline'</code>
<code class="p">)</code>
<code class="k">def</code> <code class="nf">my_pipeline</code><code class="p">(</code><code class="n">param_1</code><code class="p">:</code> <code class="n">PipelineParam</code><code class="p">,</code> <code class="n">param_2</code><code class="p">:</code> <code class="n">PipelineParam</code><code class="p">):</code>
  <code class="n">my_step</code> <code class="o">=</code> <code class="n">my_component</code><code class="p">(</code><code class="n">my_param</code><code class="o">=</code><code class="s1">'a'</code><code class="p">)</code></pre>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Storing Data Between Steps"><div class="sect2" id="idm45831179053480" title2="Storing Data Between Steps" no2="4.1.3">
<h2>4.1.3. Storing Data Between Steps</h2>

<p>In the previous example, the data passed between containers was small and of primitive types (such as numeric, string, list, and arrays).  In practice however, we will likely be passing much larger data (for instance, entire datasets).
In Kubeflow,<a data-type="indexterm" data-primary="storage" data-secondary="storing data between steps" id="ch04-stor"></a> there are two primary methods for doing this—persistent volumes inside the Kubernetes cluster, and cloud
storage options (such as S3), though each method has inherent problems.</p>

<p>Persistent volumes abstract the storage layer. Depending on the vendor,<a data-type="indexterm" data-primary="persistent volume storage" data-secondary="about" id="ch04-pvs2"></a><a data-type="indexterm" data-primary="data" data-secondary="persistent volumes" id="ch04-pvs3"></a><a data-type="indexterm" data-primary="storage" data-secondary="persistent volumes" id="ch04-pvs"></a> persistent volumes can be slow with provisioning
and have IO limits. Check to see if your vendor supports <a data-type="indexterm" data-primary="storage" data-secondary="storage classes" id="idm45831178995768"></a>read-write-many storage classes, allowing for storage access by multiple pods, which is required for some types of parallelism. Storage classes can be one of the following.<sup><a data-type="noteref" id="idm45831178994504-marker" href="#idm45831178994504">[2]</a></sup></p>
<dl>
<dt>ReadWriteOnce</dt>
<dd>
<p>The volume can be mounted as read-write by a single node.</p>
</dd>
<dt>ReadOnlyMany</dt>
<dd>
<p>The volume can be mounted read-only by many nodes.</p>
</dd>
<dt>ReadWriteMany</dt>
<dd>
<p>The volume can be mounted as read-write by many nodes.</p>
</dd>
</dl>

<p>Your system/cluster administrator may be able to add
read-write-many support.<sup><a data-type="noteref" id="idm45831178988264-marker" href="#idm45831178988264">[3]</a></sup> Additionally, many cloud providers include their proprietary read-write-many implementations, see for example <a href="https://oreil.ly/je18X">dynamic provisioning</a> on GKE. but make sure to ask if there is a single node bottleneck.</p>

<p>Kubeflow Pipelines’ <code>VolumeOp</code> allows you to create an automatically managed persistent volume, as shown in <a data-type="xref" href="#make_volume_ch4">EXAMPLE 4-4</a>. To add the volume to your operation you can just call <code>add_pvolumes</code> with a dictionary of mount points to volumes, e.g., <code>download_data_op(year).add_pvolumes({"/data_processing": dvop.volume})</code>.<a data-type="indexterm" data-startref="ch04-pvs" id="idm45831178983224"></a><a data-type="indexterm" data-startref="ch04-pvs2" id="idm45831178982520"></a><a data-type="indexterm" data-startref="ch04-pvs3" id="idm45831178981848"></a></p>
<div id="make_volume_ch4" data-type="example" class="less_space pagebreak-before" title2="Mailing list data prep" no2="4-4">
<h5>Example 4-4. Mailing list data prep</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">dvop</code> <code class="o">=</code> <code class="n">dsl</code><code class="o">.</code><code class="n">VolumeOp</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s2">"create_pvc"</code><code class="p">,</code>
                    <code class="n">resource_name</code><code class="o">=</code><code class="s2">"my-pvc-2"</code><code class="p">,</code>
                    <code class="n">size</code><code class="o">=</code><code class="s2">"5Gi"</code><code class="p">,</code>
                    <code class="n">modes</code><code class="o">=</code><code class="n">dsl</code><code class="o">.</code><code class="n">VOLUME_MODE_RWO</code><code class="p">)</code></pre></div>

<p>While less common in the Kubeflow examples, using an object storage solution,<a data-type="indexterm" data-primary="object stores" data-secondary="distributed object storage server" data-seealso="MinIO" id="idm45831178942072"></a><a data-type="indexterm" data-primary="storage" data-secondary="distributed object storage server" id="idm45831178940984"></a><a data-type="indexterm" data-primary="data" data-secondary="distributed object storage server" id="idm45831178940136"></a><a data-type="indexterm" data-primary="MinIO" data-secondary="file_output" id="idm45831178939224"></a> in some cases, may be more suitable. MinIO provides cloud native object storage by working either as a gateway to an existing object storage engine or on its
own.<sup><a data-type="noteref" id="idm45831178937864-marker" href="#idm45831178937864">[4]</a></sup> We covered how to configure MinIO back in <a data-type="xref" href="#kubeflow_design_beyond_basics">CHAPTER 3</a>.</p>

<p>Kubeflow’s built-in <code>file_output</code> mechanism automatically transfers the<a data-type="indexterm" data-primary="file_output mechanism" id="idm45831178935144"></a> specified local file into MinIO between pipeline steps for you. To use <code>file_output</code>, write your files locally in your container and specify the parameter in your <code>ContainerOp</code>, as shown in <a data-type="xref" href="#file_output_ex">EXAMPLE 4-5</a>.</p>
<div id="file_output_ex" data-type="example" title2="File output example" no2="4-5">
<h5>Example 4-5. File output example</h5>

<pre data-type="programlisting" data-code-language="python">    <code class="n">fetch</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">dsl</code><code class="o">.</code><code class="n">ContainerOp</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'download'</code><code class="p">,</code>
                                <code class="n">image</code><code class="o">=</code><code class="s1">'busybox'</code><code class="p">,</code>
                                <code class="n">command</code><code class="o">=</code><code class="p">[</code><code class="s1">'sh'</code><code class="p">,</code> <code class="s1">'-c'</code><code class="p">],</code>
                                <code class="n">arguments</code><code class="o">=</code><code class="p">[</code>
                                    <code class="s1">'sleep 1;'</code>
                                    <code class="s1">'mkdir -p /tmp/data;'</code>
                                    <code class="s1">'wget '</code> <code class="o">+</code> <code class="n">data_url</code> <code class="o">+</code>
                                    <code class="s1">' -O /tmp/data/results.csv'</code>
                                <code class="p">],</code>
                                <code class="n">file_outputs</code><code class="o">=</code><code class="p">{</code><code class="s1">'downloaded'</code><code class="p">:</code> <code class="s1">'/tmp/data'</code><code class="p">})</code>
    <code class="c1"># This expects a directory of inputs not just a single file</code></pre></div>

<p>If you don’t want to use MinIO, you can also directly use your provider’s<a data-type="indexterm" data-primary="portability of Kubeflow" data-secondary="object storage and" id="idm45831178870888"></a> object storage, but this may compromise some portability.</p>

<p>The ability to mount data locally is an essential task in any machine learning pipeline. Here we have briefly outlined multiple
 methods and provided examples for each.<a data-type="indexterm" data-startref="ch04-stor" id="idm45831178839960"></a></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Introduction to Kubeflow Pipelines Components"><div class="sect1" id="idm45831179002392" title2="Introduction to Kubeflow Pipelines Components" no2="4.2">
<h1>4.2. Introduction to Kubeflow Pipelines Components</h1>

<p>Kubeflow Pipelines builds on <a href="https://oreil.ly/S2GuQ">Argo Workflows</a>,<a data-type="indexterm" data-primary="Argo Workflows" data-secondary="Kubeflow Pipelines built on" id="ch04-pip2"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="Argo Workflows" id="idm45831178835816"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="Argo Workflows foundation" id="ch04-pip"></a> an open source, container-native workflow engine for
Kubernetes. In this section we will describe how Argo works, what it does, and how Kubeflow Pipeline supplements Argo to make it easier to use by data scientists.</p>








<section data-type="sect2" data-pdf-bookmark="Argo: the Foundation of Pipelines"><div class="sect2" id="argo_foundation" title2="Argo: the Foundation of Pipelines" no2="4.2.1">
<h2>4.2.1. Argo: the Foundation of Pipelines</h2>

<p>Kubeflow installs all of the Argo components. Though having Argo<a data-type="indexterm" data-primary="Argo Workflows" data-secondary="installing" id="idm45831178830840"></a> installed on your computer is not necessary to use Kubeflow Pipelines, having the Argo command-line tool makes it easier to understand and debug your pipelines.</p>
<div data-type="tip"><h6>Tip</h6>
<p>By default, Kubeflow configures Argo to use the Docker executor.<a data-type="indexterm" data-primary="Argo Workflows" data-secondary="Docker as executor" id="idm45831178828472"></a><a data-type="indexterm" data-primary="executors for Argo Workflows" id="idm45831178827496"></a><a data-type="indexterm" data-primary="Docker" data-secondary="Argo Workflows executor" id="idm45831178826808"></a><a data-type="indexterm" data-primary="Argo Workflows" data-secondary="executors" id="idm45831178825864"></a> If your platform does not support the Docker APIs, you need to switch your executor to a compatible one. This is done by changing the <code>containerRuntimeExecutor</code> value in the Argo <em>params</em> file. See <a data-type="xref" href="#appendix_executors">APPENDIX A</a> for details on the trade-offs. The majority of the examples in this book use the Docker executor but can be adapted to other executors.</p>
</div>

<p>On macOS, you can install Argo with Homebrew, as shown in <a data-type="xref" href="#argo_dl_linux">EXAMPLE 4-6</a>.<sup><a data-type="noteref" id="idm45831178821608-marker" href="#idm45831178821608">[5]</a></sup></p>
<div id="argo_dl_linux" data-type="example" title2="Argo installation" no2="4-6">
<h5>Example 4-6. Argo installation</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="c">#!/bin/bash</code>
<code class="c"># Download the binary</code>
curl -sLO https://github.com/argoproj/argo/releases/download/v2.8.1/argo-linux-amd64

<code class="c"># Make binary executable</code>
chmod +x argo-linux-amd64

<code class="c"># Move binary to path</code>
mv ./argo-linux-amd64 ~/bin/argo</pre></div>

<p>You can verify your Argo installation by running the Argo examples<a data-type="indexterm" data-primary="testing" data-secondary="Argo installation" id="idm45831178815384"></a> with the command-line tool in the Kubeflow namespace: follow <a href="https://oreil.ly/QFxv2">these Argo instructions</a>.
When you run the Argo examples the pipelines are visible<a data-type="indexterm" data-primary="Argo Workflows" data-secondary="pipelines visible" id="idm45831178813784"></a> with the <code>argo</code> command, as in <a data-type="xref" href="#List_Argo_executions">EXAMPLE 4-7</a>.</p>
<div id="List_Argo_executions" data-type="example" title2="Listing Argo executions" no2="4-7">
<h5>Example 4-7. Listing Argo executions</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>argo list -n kubeflow
NAME                STATUS      AGE   DURATION
loops-maps-4mxp5    Succeeded   30m   12s
hello-world-wsxbr   Succeeded   39m   15s</pre></div>

<p>Since pipelines are implemented with Argo, you can use the same technique to check on them as well. You can<a data-type="indexterm" data-primary="Argo Workflows" data-secondary="execution information" id="idm45831178810552"></a>
also get information about specific workflow execution, as shown in <a data-type="xref" href="#Get_Argo_execution_details">EXAMPLE 4-8</a>.</p>
<div id="Get_Argo_execution_details" data-type="example" title2="Getting Argo execution details" no2="4-8">
<h5>Example 4-8. Getting Argo execution details</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code><code>argo</code><code> </code><code>get</code><code> </code><code>hello-world-wsxbr</code><code> </code><code>-n</code><code> </code><code>kubeflow</code><code>  </code><a class="co" id="co_kubeflow_pipelines_CO3-1" href="#callout_kubeflow_pipelines_CO3-1"><img src="assets/1.png" alt="1" width="12" height="12"></a><code>
</code><code>Name:</code><code>                </code><code>hello-world-wsxbr</code><code>
</code><code>Namespace:</code><code>           </code><code>kubeflow</code><code>
</code><code>ServiceAccount:</code><code>      </code><code>default</code><code>
</code><code>Status:</code><code>              </code><code>Succeeded</code><code>
</code><code>Created:</code><code>             </code><code>Tue</code><code> </code><code>Feb</code><code> </code><code class="m">12</code><code> </code><code>10:05:04</code><code> </code><code>-0600</code><code> </code><code class="o">(</code><code class="m">2</code><code> </code><code>minutes</code><code> </code><code>ago</code><code class="o">)</code><code>
</code><code>Started:</code><code>             </code><code>Tue</code><code> </code><code>Feb</code><code> </code><code class="m">12</code><code> </code><code>10:05:04</code><code> </code><code>-0600</code><code> </code><code class="o">(</code><code class="m">2</code><code> </code><code>minutes</code><code> </code><code>ago</code><code class="o">)</code><code>
</code><code>Finished:</code><code>            </code><code>Tue</code><code> </code><code>Feb</code><code> </code><code class="m">12</code><code> </code><code>10:05:23</code><code> </code><code>-0600</code><code> </code><code class="o">(</code><code class="m">1</code><code> </code><code>minute</code><code> </code><code>ago</code><code class="o">)</code><code>
</code><code>Duration:</code><code>            </code><code class="m">19</code><code> </code><code>seconds</code><code>

</code><code>STEP</code><code>                  </code><code>PODNAME</code><code>            </code><code>DURATION</code><code>  </code><code>MESSAGE</code><code>
 </code><code>✔</code><code> </code><code>hello-world-wsxbr</code><code>  </code><code>hello-world-wsxbr</code><code>  </code><code>18s</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_kubeflow_pipelines_CO3-1" href="#co_kubeflow_pipelines_CO3-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p><code>hello-world-wsxbr</code> is the name that we got using <code>argo list -n kubeflow</code> above. In your case the name will be different.</p></dd>
</dl></div>

<p>We can also view the execution logs by using the command in <a data-type="xref" href="#Get_log_Argo_execution">EXAMPLE 4-9</a>.<a data-type="indexterm" data-primary="Argo Workflows" data-secondary="execution logs" id="idm45831178603352"></a></p>
<div id="Get_log_Argo_execution" data-type="example" title2="Getting the log of Argo execution" no2="4-9">
<h5>Example 4-9. Getting the log of Argo execution</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>argo logs hello-world-wsxbr -n kubeflow</pre></div>

<p>This produces the result shown in <a data-type="xref" href="#Argo_execution_log">EXAMPLE 4-10</a>.</p>
<div id="Argo_execution_log" data-type="example" title2="Argo execution log" no2="4-10">
<h5>Example 4-10. Argo execution log</h5>

<pre data-type="programlisting" data-code-language="bash">&lt; hello world &gt;
 -------------
    <code class="se">\</code>
     <code class="se">\</code>
      <code class="se">\</code>
		    <code class="c">##        .</code>
	      <code class="c">## ## ##       ==</code>
	   <code class="c">## ## ## ##      ===</code>
       /<code class="s2">""""""""""""""""</code>___/ <code class="o">===</code>
  ~~~ <code class="o">{</code>~~ ~~~~ ~~~ ~~~~ ~~ ~ /  <code class="o">===</code>- ~~~
       <code class="se">\_</code>_____ o          __/
	<code class="se">\ </code>   <code class="se">\ </code>       __/
	  <code class="se">\_</code>___<code class="se">\_</code>_____/</pre></div>

<p>You can also delete a specific workflow; see <a data-type="xref" href="#Deleting_Argo_execution">EXAMPLE 4-11</a>.<a data-type="indexterm" data-primary="Argo Workflows" data-secondary="deleting a workflow" id="idm45831178592296"></a></p>
<div id="Deleting_Argo_execution" data-type="example" title2="Deleting Argo execution" no2="4-11">
<h5>Example 4-11. Deleting Argo execution</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>argo delete hello-world-wsxbr -n kubeflow</pre></div>

<p class="less_space pagebreak-before">Alternatively, you can get pipeline execution information using the Argo<a data-type="indexterm" data-primary="Argo Workflows" data-secondary="UI for pipeline execution" id="idm45831178639432"></a><a data-type="indexterm" data-primary="user interfaces (UI)" data-secondary="Argo UI" id="idm45831178638584"></a> UI, as seen in <a data-type="xref" href="#argo_ui_for_pipeline_execution">FIGURE 4-5</a>.</p>

<figure><div id="argo_ui_for_pipeline_execution" class="figure" data-type="figure" title2="Argo UI for pipeline execution" no2="4-5">
<img src="assets/kfml_0405.png" alt="Argo UI for pipelines execution" width="1440" height="728">
<h6>Figure 4-5. Argo UI for pipeline execution</h6>
</div></figure>
<aside data-type="sidebar"><div class="sidebar" id="idm45831178684472">
<h5>Installing Argo UI</h5>
<p>By default, Kubeflow does not provide access to the Argo UI.<a data-type="indexterm" data-primary="user interfaces (UI)" data-secondary="Argo UI" data-tertiary="installation" id="idm45831178683256"></a><a data-type="indexterm" data-primary="Argo Workflows" data-secondary="UI installation" id="idm45831178682008"></a> To enable access, you have to do the following:</p>

<ul>
<li>
<p>Make sure that your Argo UI deployment corresponds to the UI provided in code <a href="https://oreil.ly/Kubeflow_for_ML_ch04_install">in this book’s GitHub repo</a>.</p>
</li>
<li>
<p>Create a virtual service by applying the YAML provided in code <a href="https://oreil.ly/Kubeflow_for_ML_ch04_vsyaml">in this book’s GitHub repo</a>.</p>
</li>
<li>
<p>Point your browser to <em>&lt;cluster main url&gt;/argo</em>.</p>
</li>
</ul>
</div></aside>

<p class="less_space pagebreak-before">You can also look at the details of the flow execution graph<a data-type="indexterm" data-primary="Argo Workflows" data-secondary="flow execution graph details" id="idm45831178693272"></a> by clicking a specific workflow, as seen in <a data-type="xref" href="#argo_ui_execution_graph">FIGURE 4-6</a>.</p>

<figure><div id="argo_ui_execution_graph" class="figure" data-type="figure" title2="Argo UI execution graph" no2="4-6">
<img src="assets/kfml_0406.png" alt="Argo UI - Execution Graph" width="1440" height="381">
<h6>Figure 4-6. Argo UI execution graph</h6>
</div></figure>

<p>For any Kubeflow pipeline you run, you can also view that pipeline<a data-type="indexterm" data-primary="Argo Workflows" data-secondary="pipelines visible" id="idm45831178666360"></a><a data-type="indexterm" data-primary="Argo Workflows" data-secondary="UI for pipeline execution" id="idm45831178665384"></a> in the Argo CLI/UI. Note that because ML pipelines
are using the Argo CRD, you can also see the result of the pipeline execution in the Argo UI (as in <a data-type="xref" href="#viewing_kuberflow_pipelines_in_argo_ui">FIGURE 4-7</a>).<a data-type="indexterm" data-startref="ch04-pip" id="idm45831178663304"></a><a data-type="indexterm" data-startref="ch04-pip2" id="idm45831178662600"></a></p>

<figure><div id="viewing_kuberflow_pipelines_in_argo_ui" class="figure" data-type="figure" title2="Viewing Kubeflow pipelines in Argo UI" no2="4-7">
<img src="assets/kfml_0407.png" alt="Viewing Kubeflow Pipelines in Argo UI" width="1350" height="883">
<h6>Figure 4-7. Viewing Kubeflow pipelines in Argo UI</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>Currently, the Kubeflow community is actively looking at alternative<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="Argo alternative" id="idm45831178658952"></a><a data-type="indexterm" data-primary="Tekton for running pipelines" id="idm45831178657976"></a> foundational technologies for running Kubeflow pipelines, one of which is <a href="https://tekton.dev">Tekton</a>. The paper by A. Singh et al.,  <a href="https://oreil.ly/rrg-V">“Kubeflow Pipelines with Tekton”</a>, gives “initial design, specifications, and code for enabling Kubeflow Pipelines to run on top of Tekton.” The basic idea here is to create an intermediate format that can be produced by pipelines and then executed using Argo, Tekton, or other runtimes. The initial code for this implementation is found in <a href="https://oreil.ly/nes4r">this Kubeflow GitHub repo</a>.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="What Kubeflow Pipelines Adds to Argo Workflow"><div class="sect2" id="idm45831178832232" title2="What Kubeflow Pipelines Adds to Argo Workflow" no2="4.2.2">
<h2>4.2.2. What Kubeflow Pipelines Adds to Argo Workflow</h2>

<p>Argo underlies the workflow execution; however, using it directly requires you to do awkward things. First, you must<a data-type="indexterm" data-primary="Argo Workflows" data-secondary="Kubeflow Pipelines enhancing" id="idm45831178653368"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="Argo Workflows enhanced by" id="idm45831178518088"></a> define your workflow in YAML, which can be difficult. Second, you must containerize your code, which can be tedious. The main advantage of KF Pipelines is that you can use Python APIs for defining/creating pipelines, which automates the generation of much of the YAML boilerplate for workflow definitions and is extremely friendly for data scientists/Python developers. Kubeflow Pipelines also has hooks that add building blocks for machine learning-specific 
components. These APIs not only generate the YAML but can also simplify container creation and resource usage. In addition to the APIs, Kubeflow adds a recurring scheduler and UI for configuration and execution.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Building a Pipeline Using Existing Images"><div class="sect2" id="idm45831178515528" title2="Building a Pipeline Using Existing Images" no2="4.2.3">
<h2>4.2.3. Building a Pipeline Using Existing Images</h2>

<p>Building pipeline stages directly from Python provides a straightforward entry point. It does limit our implementation<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="prebuilt Docker images" id="ch04-doc2"></a><a data-type="indexterm" data-primary="Docker" data-secondary="prebuilt Docker images" id="ch04-doc"></a>
to Python, though. Another feature of Kubeflow Pipelines is the ability to orchestrate the execution of a multilanguage
implementation leveraging prebuilt Docker images (see <a data-type="xref" href="#beyond_tf">CHAPTER 9</a>).</p>
<aside data-type="sidebar"><div class="sidebar" id="idm45831178510472">
<h5>Using Custom Code Inside Pipelines</h5>
<p>In order to use custom code and tools inside Kubeflow Pipelines, it needs<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="custom code and tools inside" id="idm45831178508872"></a><a data-type="indexterm" data-primary="containers" data-secondary="pipeline custom code and tools" id="idm45831178507944"></a><a data-type="indexterm" data-primary="Python" data-secondary="pipeline custom code and tools" id="idm45831178506984"></a> to be packaged into a container: see  <a href="https://oreil.ly/tTZBD">this Kubeflow documentation page</a>.
Once the container is uploaded to an accessible repository, it can be included in the pipeline. Pipelines allow the user to configure
some of the container execution through <a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="environment variables" id="idm45831178505080"></a><a data-type="indexterm" data-primary="environment variables for pipelines" id="idm45831178504232"></a><a data-type="indexterm" data-primary="data" data-secondary="environment variables for pipelines" id="idm45831178503576"></a><a data-type="indexterm" data-primary="libraries" data-secondary="Kubernetes Python library" id="idm45831178502568"></a>environment variables and pass data between pipeline components. Environment variables can be set using <a href="https://oreil.ly/WdJNl">Kubernetes Python library</a>.
Include the Kubernetes library and then implement the code:</p>

<pre data-type="programlisting" data-code-language="python" class="less_space pagebreak-before" id="untitled_programlisting_10" title2="(no caption)" no2=""><code class="kn">from</code><code> </code><code class="nn">kubernetes</code><code> </code><code class="kn">import</code><code> </code><code class="n">client</code><code> </code><code class="k">as</code><code> </code><code class="n">k8s_client</code><code>
</code><code>
</code><code class="n">data</code><code> </code><code class="o">=</code><code> </code><code class="n">dsl</code><code class="o">.</code><code class="n">ContainerOp</code><code class="p">(</code><code>
</code><code>      </code><code class="n">name</code><code class="o">=</code><code class="s1">'</code><code class="s1">updatedata</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>      </code><code class="n">image</code><code class="o">=</code><code class="s1">'</code><code class="s1">lightbend/recommender-data-update-publisher:0.2</code><code class="s1">'</code><code class="p">)</code><code> </code><code>\
</code><code>    </code><code class="o">.</code><code class="n">add_env_variable</code><code class="p">(</code><code class="n">k8s_client</code><code class="o">.</code><code class="n">V1EnvVar</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'</code><code class="s1">MINIO_KEY</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="n">value</code><code class="o">=</code><code class="s1">'</code><code class="s1">minio</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" id="co_kubeflow_pipelines_CO4-1" href="#callout_kubeflow_pipelines_CO4-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_kubeflow_pipelines_CO4-1" href="#co_kubeflow_pipelines_CO4-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>Here we set the environment variable <code>MINIO_KEY</code> to the value of <code>minio</code>.</p></dd>
</dl>

<p>The way you can pass parameters between steps (containers), depends on the Argo runtime that you are using.
For example, in the case of the <code>docker</code> runtime, you can pass<a data-type="indexterm" data-primary="Docker" data-secondary="parameters passed by value" id="idm45831178469928"></a><a data-type="indexterm" data-primary="Argo Workflows" data-secondary="parameter passing" id="idm45831178469208"></a> parameters by value. Those parameters are exposed by the image.
If you are using the <code>k8api</code> runtime, then the only way to pass parameters is through the file.</p>
</div></aside>

<p>In addition to our previous imports, we also want to import the <a data-type="indexterm" data-primary="Kubernetes" data-secondary="client" id="idm45831178447128"></a><a data-type="indexterm" data-primary="Python" data-secondary="Kubernetes client" id="idm45831178472872"></a>Kubernetes client, which allows us to use Kubernetes functions directly from Python code (see <a data-type="xref" href="#Export_kubernetes_cli">EXAMPLE 4-12</a>).</p>
<div id="Export_kubernetes_cli" data-type="example" title2="Exporting Kubernetes client" no2="4-12">
<h5>Example 4-12. Exporting Kubernetes client</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">kubernetes</code> <code class="kn">import</code> <code class="n">client</code> <code class="k">as</code> <code class="n">k8s_client</code></pre></div>

<p>Again, we create a client and experiment to run our pipeline. As mentioned earlier, <a data-type="indexterm" data-primary="experiments" id="idm45831178488616"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="experiments" id="idm45831178487432"></a>experiments group the runs of
pipelines. You can only create a given experiment once, so <a data-type="xref" href="#Obtain_pipeline_exper">EXAMPLE 4-13</a> shows how to either create a new experiment or
use an existing one.</p>
<div id="Obtain_pipeline_exper" data-type="example" title2="Obtaining pipeline experiment" no2="4-13">
<h5>Example 4-13. Obtaining pipeline experiment</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">client</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">Client</code><code class="p">()</code>
<code class="n">exp</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="n">get_experiment</code><code class="p">(</code><code class="n">experiment_name</code> <code class="o">=</code><code class="s1">'mdupdate'</code><code class="p">)</code></pre></div>

<p>Now we create our pipeline (<a data-type="xref" href="#ex_rec_pipeline">EXAMPLE 4-14</a>). The images used need to be accessible, and we’re specifying the full names, so they resolve.
Since these containers are prebuilt, we need to configure them for our pipeline.</p>

<p>The pre-built containers we are using have their storage configured by the <code>MINIO_*</code> environment variables. So we configure them to use our local MinIO install by calling  <code>add_env_variable</code>.</p>

<p>In addition to the automatic dependencies created when passing parameters between stages, you can also specify that a
stage requires a previous stage with <code>after</code>. This is most useful when there is an external side effect, like updating a database.</p>
<div id="ex_rec_pipeline" data-type="example" class="less_space pagebreak-before" title2="Example recommender pipeline" no2="4-14">
<h5>Example 4-14. Example recommender pipeline</h5>

<pre data-type="programlisting" data-code-language="python"><code class="nd">@dsl.pipeline</code><code class="p">(</code>
  <code class="n">name</code><code class="o">=</code><code class="s1">'Recommender model update'</code><code class="p">,</code>
  <code class="n">description</code><code class="o">=</code><code class="s1">'Demonstrate usage of pipelines for multi-step model update'</code>
<code class="p">)</code>
<code class="k">def</code> <code class="nf">recommender_pipeline</code><code class="p">():</code>
    <code class="c1"># Load new data</code>
  <code class="n">data</code> <code class="o">=</code> <code class="n">dsl</code><code class="o">.</code><code class="n">ContainerOp</code><code class="p">(</code>
      <code class="n">name</code><code class="o">=</code><code class="s1">'updatedata'</code><code class="p">,</code>
      <code class="n">image</code><code class="o">=</code><code class="s1">'lightbend/recommender-data-update-publisher:0.2'</code><code class="p">)</code> \
    <code class="o">.</code><code class="n">add_env_variable</code><code class="p">(</code><code class="n">k8s_client</code><code class="o">.</code><code class="n">V1EnvVar</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'MINIO_URL'</code><code class="p">,</code>
        <code class="n">value</code><code class="o">=</code><code class="s1">'http://minio-service.kubeflow.svc.cluster.local:9000'</code><code class="p">))</code> \
    <code class="o">.</code><code class="n">add_env_variable</code><code class="p">(</code><code class="n">k8s_client</code><code class="o">.</code><code class="n">V1EnvVar</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'MINIO_KEY'</code><code class="p">,</code> <code class="n">value</code><code class="o">=</code><code class="s1">'minio'</code><code class="p">))</code> \
    <code class="o">.</code><code class="n">add_env_variable</code><code class="p">(</code><code class="n">k8s_client</code><code class="o">.</code><code class="n">V1EnvVar</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'MINIO_SECRET'</code><code class="p">,</code> <code class="n">value</code><code class="o">=</code><code class="s1">'minio123'</code><code class="p">))</code>
    <code class="c1"># Train the model</code>
  <code class="n">train</code> <code class="o">=</code> <code class="n">dsl</code><code class="o">.</code><code class="n">ContainerOp</code><code class="p">(</code>
      <code class="n">name</code><code class="o">=</code><code class="s1">'trainmodel'</code><code class="p">,</code>
      <code class="n">image</code><code class="o">=</code><code class="s1">'lightbend/ml-tf-recommender:0.2'</code><code class="p">)</code> \
    <code class="o">.</code><code class="n">add_env_variable</code><code class="p">(</code><code class="n">k8s_client</code><code class="o">.</code><code class="n">V1EnvVar</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'MINIO_URL'</code><code class="p">,</code>
            <code class="n">value</code><code class="o">=</code><code class="s1">'minio-service.kubeflow.svc.cluster.local:9000'</code><code class="p">))</code> \
    <code class="o">.</code><code class="n">add_env_variable</code><code class="p">(</code><code class="n">k8s_client</code><code class="o">.</code><code class="n">V1EnvVar</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'MINIO_KEY'</code><code class="p">,</code> <code class="n">value</code><code class="o">=</code><code class="s1">'minio'</code><code class="p">))</code> \
    <code class="o">.</code><code class="n">add_env_variable</code><code class="p">(</code><code class="n">k8s_client</code><code class="o">.</code><code class="n">V1EnvVar</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'MINIO_SECRET'</code><code class="p">,</code> <code class="n">value</code><code class="o">=</code><code class="s1">'minio123'</code><code class="p">))</code>
  <code class="n">train</code><code class="o">.</code><code class="n">after</code><code class="p">(</code><code class="n">data</code><code class="p">)</code>
    <code class="c1"># Publish new model</code>
  <code class="n">publish</code> <code class="o">=</code> <code class="n">dsl</code><code class="o">.</code><code class="n">ContainerOp</code><code class="p">(</code>
      <code class="n">name</code><code class="o">=</code><code class="s1">'publishmodel'</code><code class="p">,</code>
      <code class="n">image</code><code class="o">=</code><code class="s1">'lightbend/recommender-model-publisher:0.2'</code><code class="p">)</code> \
    <code class="o">.</code><code class="n">add_env_variable</code><code class="p">(</code><code class="n">k8s_client</code><code class="o">.</code><code class="n">V1EnvVar</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'MINIO_URL'</code><code class="p">,</code>
            <code class="n">value</code><code class="o">=</code><code class="s1">'http://minio-service.kubeflow.svc.cluster.local:9000'</code><code class="p">))</code> \
    <code class="o">.</code><code class="n">add_env_variable</code><code class="p">(</code><code class="n">k8s_client</code><code class="o">.</code><code class="n">V1EnvVar</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'MINIO_KEY'</code><code class="p">,</code> <code class="n">value</code><code class="o">=</code><code class="s1">'minio'</code><code class="p">))</code> \
    <code class="o">.</code><code class="n">add_env_variable</code><code class="p">(</code><code class="n">k8s_client</code><code class="o">.</code><code class="n">V1EnvVar</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'MINIO_SECRET'</code><code class="p">,</code> <code class="n">value</code><code class="o">=</code><code class="s1">'minio123'</code><code class="p">))</code> \
    <code class="o">.</code><code class="n">add_env_variable</code><code class="p">(</code><code class="n">k8s_client</code><code class="o">.</code><code class="n">V1EnvVar</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'KAFKA_BROKERS'</code><code class="p">,</code>
            <code class="n">value</code><code class="o">=</code><code class="s1">'cloudflow-kafka-brokers.cloudflow.svc.cluster.local:9092'</code><code class="p">))</code> \
    <code class="o">.</code><code class="n">add_env_variable</code><code class="p">(</code><code class="n">k8s_client</code><code class="o">.</code><code class="n">V1EnvVar</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'DEFAULT_RECOMMENDER_URL'</code><code class="p">,</code>
            <code class="n">value</code><code class="o">=</code><code class="s1">'http://recommendermodelserver.kubeflow.svc.cluster.local:8501'</code><code class="p">))</code> \
    <code class="o">.</code><code class="n">add_env_variable</code><code class="p">(</code><code class="n">k8s_client</code><code class="o">.</code><code class="n">V1EnvVar</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'ALTERNATIVE_RECOMMENDER_URL'</code><code class="p">,</code>
            <code class="n">value</code><code class="o">=</code><code class="s1">'http://recommendermodelserver1.kubeflow.svc.cluster.local:8501'</code><code class="p">))</code>
  <code class="n">publish</code><code class="o">.</code><code class="n">after</code><code class="p">(</code><code class="n">train</code><code class="p">)</code></pre></div>

<p>Since the pipeline definition is just code, you can make it more compact by using a loop to set the MinIO parameters instead of doing it on each stage.</p>

<p>As before, we need to compile the pipeline, either explicitly with <code>compiler.Compiler().compile</code> or implicitly with
<code>create_run_from_pipeline_func</code>. Now go ahead and run the pipeline (as in <a data-type="xref" href="#execution_recomm_pipelines_ex">FIGURE 4-8</a>).<a data-type="indexterm" data-startref="ch04-doc" id="idm45831178342664"></a><a data-type="indexterm" data-startref="ch04-doc2" id="idm45831178088968"></a></p>

<figure><div id="execution_recomm_pipelines_ex" class="figure" data-type="figure" title2="Execution of recommender pipelines example" no2="4-8">
<img src="assets/kfml_0408.png" alt="Execution of Recommender Pipelines Example" width="1680" height="982">
<h6>Figure 4-8. Execution of recommender pipelines example</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Kubeflow Pipeline Components"><div class="sect2" id="idm45831178514936" title2="Kubeflow Pipeline Components" no2="4.2.4">
<h2>4.2.4. Kubeflow Pipeline Components</h2>

<p>In addition to container operations which we’ve just discussed, Kubeflow Pipelines also exposes additional operations
with components. Components expose different Kubernetes resources or external operations (like <code>dataproc</code>). Kubeflow components allow developers to package machine learning tools while abstracting away the specifics on the containers or CRDs used.</p>

<p>We have used Kubeflow’s building blocks fairly directly, and we have used<a data-type="indexterm" data-primary="components" data-secondary="repositories" id="idm45831178083432"></a><a data-type="indexterm" data-primary="repositories" data-secondary="components" id="idm45831178082456"></a><a data-type="indexterm" data-primary="resources for Kubeflow" data-secondary="component repositories" id="idm45831178081512"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="components of" id="idm45831178080568"></a> the <code>func_to_container</code> component.<sup><a data-type="noteref" id="idm45831178079080-marker" href="#idm45831178079080">[6]</a></sup>
Some components, like <code>func_to_container</code>, are available as Python code and can be imported like normal. Other components are specified using Kubeflow’s <code>component.yaml</code> system and need to be loaded. In our opinion, the best way to work with Kubeflow components is to download a specific tag of the repo, allowing us to use <code>load_component_from_file</code>, as shown in <a data-type="xref" href="#dl_pipeline_release">EXAMPLE 4-15</a>.</p>
<div id="dl_pipeline_release" data-type="example" title2="Pipeline release" no2="4-15">
<h5>Example 4-15. Pipeline release</h5>

<pre data-type="programlisting" data-code-language="bash">wget https://github.com/kubeflow/pipelines/archive/0.2.5.tar.gz
tar -xvf 0.2.5.tar.gz</pre></div>
<div data-type="warning"><h6>Warning</h6>
<p>There is a <code>load_component</code> function that takes a<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="load_component function warning" id="idm45831178050184"></a><a data-type="indexterm" data-primary="components" data-secondary="load_component function warning" id="idm45831178049240"></a> component’s name and attempts to resolve it. We don’t recommend
using this function since it defaults to a search path that includes fetching, from Github, the master branch of the pipelines library, which is unstable.</p>
</div>

<p>We explore data preparation components in depth in the next chapter; however, let’s quickly look at a <a data-type="indexterm" data-primary="components" data-secondary="file-fetching component" id="idm45831178071560"></a><a data-type="indexterm" data-primary="file-fetching component" id="idm45831178070584"></a><a data-type="indexterm" data-primary="data" data-secondary="file-fetching component" id="idm45831178069912"></a>file-fetching component as an example. In our recommender example earlier in the chapter, we used a special prebuilt container to fetch our data since it was not already in a persistent volume. Instead, we can use the Kubeflow GCS component <code>google-cloud/storage/download/</code> to download our data. Assuming you’ve downloaded the pipeline release as in <a data-type="xref" href="#dl_pipeline_release">EXAMPLE 4-15</a>, you can load the component with <code>load_component_from_file</code> as in <a data-type="xref" href="#ex_load_gcs">EXAMPLE 4-16</a>.</p>
<div id="ex_load_gcs" data-type="example" title2="Load GCS download component" no2="4-16">
<h5>Example 4-16. Load GCS download component</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">gcs_download_component</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">components</code><code class="o">.</code><code class="n">load_component_from_file</code><code class="p">(</code>
    <code class="s2">"pipelines-0.2.5/components/google-cloud/storage/download/component.yaml"</code><code class="p">)</code></pre></div>

<p>When a component is loaded, it returns a function that produces a pipeline<a data-type="indexterm" data-primary="components" data-secondary="about" id="idm45831178030952"></a><a data-type="indexterm" data-primary="YAML" data-secondary="component options" id="idm45831178030104"></a> stage when called. Most components take parameters to configure their behavior. You can get a list of the components’ options by calling <code>help</code> on the loaded component, or looking at the <em>component.yaml</em>. The GCS download component requires us to configure what we are downloading
with <code>gcs_path</code>, shown in <a data-type="xref" href="#ex_dl_gcs">EXAMPLE 4-17</a>.</p>
<div id="ex_dl_gcs" data-type="example" title2="Loading pipeline storage component from relative path and web link" no2="4-17">
<h5>Example 4-17. Loading pipeline storage component from relative path and web link</h5>

<pre data-type="programlisting" data-code-language="python">    <code class="n">dl_op</code> <code class="o">=</code> <code class="n">gcs_download_component</code><code class="p">(</code>
        <code class="n">gcs_path</code><code class="o">=</code>
        <code class="s2">"gs://ml-pipeline-playground/tensorflow-tfx-repo/tfx/components/testdata/external/csv"</code>
    <code class="p">)</code>  <code class="c1"># Your path goes here</code></pre></div>

<p>In <a data-type="xref" href="#data_and_feature_prep">CHAPTER 5</a>, we explore more common Kubeflow pipeline components for data and feature preparation.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Advanced Topics in Pipelines"><div class="sect1" id="idm45831178085656" title2="Advanced Topics in Pipelines" no2="4.3">
<h1>4.3. Advanced Topics in Pipelines</h1>

<p>All of the examples that we have shown so far are purely sequential. There are also cases in which we need the ability to check conditions and change the behavior of the pipeline accordingly.</p>








<section data-type="sect2" data-pdf-bookmark="Conditional Execution of Pipeline Stages"><div class="sect2" id="idm45831178017224" title2="Conditional Execution of Pipeline Stages" no2="4.3.1">
<h2>4.3.1. Conditional Execution of Pipeline Stages</h2>

<p>Kubeflow Pipelines allows conditional executions via <code>dsl.Condition</code>.<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="conditional execution" id="ch04-cond"></a><a data-type="indexterm" data-primary="conditional execution of pipelines" id="ch04-cond2"></a> Let’s look at a very simple example, where, depending on the value of a variable, different calculations are executed.</p>

<p>A simple notebook implementing this example follows. It starts with the imports necessary for this, in <a data-type="xref" href="#Import_req_components">EXAMPLE 4-18</a>.</p>
<div id="Import_req_components" data-type="example" title2="Importing required components" no2="4-18">
<h5>Example 4-18. Importing required components</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">kfp</code>
<code class="kn">from</code> <code class="nn">kfp</code> <code class="kn">import</code> <code class="n">dsl</code>
<code class="kn">from</code> <code class="nn">kfp.components</code> <code class="kn">import</code> <code class="n">func_to_container_op</code><code class="p">,</code> <code class="n">InputPath</code><code class="p">,</code> <code class="n">OutputPath</code></pre></div>

<p>Once the imports are in place, we can implement several simple functions, as shown in <a data-type="xref" href="#Functions_implement">EXAMPLE 4-19</a>.</p>
<div id="Functions_implement" data-type="example" title2="Functions implementation" no2="4-19">
<h5>Example 4-19. Functions implementation</h5>

<pre data-type="programlisting" data-code-language="python"><code class="nd">@func_to_container_op</code>
<code class="k">def</code> <code class="nf">get_random_int_op</code><code class="p">(</code><code class="n">minimum</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">maximum</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>
    <code class="sd">"""Generate a random number between minimum and maximum (inclusive)."""</code>
    <code class="kn">import</code> <code class="nn">random</code>
    <code class="n">result</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="n">minimum</code><code class="p">,</code> <code class="n">maximum</code><code class="p">)</code>
    <code class="k">print</code><code class="p">(</code><code class="n">result</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">result</code>

<code class="nd">@func_to_container_op</code>
<code class="k">def</code> <code class="nf">process_small_op</code><code class="p">(</code><code class="n">data</code><code class="p">:</code> <code class="nb">int</code><code class="p">):</code>
    <code class="sd">"""Process small numbers."""</code>
    <code class="k">print</code><code class="p">(</code><code class="s2">"Processing small result"</code><code class="p">,</code> <code class="n">data</code><code class="p">)</code>
    <code class="k">return</code>

<code class="nd">@func_to_container_op</code>
<code class="k">def</code> <code class="nf">process_medium_op</code><code class="p">(</code><code class="n">data</code><code class="p">:</code> <code class="nb">int</code><code class="p">):</code>
    <code class="sd">"""Process medium numbers."""</code>
    <code class="k">print</code><code class="p">(</code><code class="s2">"Processing medium result"</code><code class="p">,</code> <code class="n">data</code><code class="p">)</code>
    <code class="k">return</code>

<code class="nd">@func_to_container_op</code>
<code class="k">def</code> <code class="nf">process_large_op</code><code class="p">(</code><code class="n">data</code><code class="p">:</code> <code class="nb">int</code><code class="p">):</code>
    <code class="sd">"""Process large numbers."""</code>
    <code class="k">print</code><code class="p">(</code><code class="s2">"Processing large result"</code><code class="p">,</code> <code class="n">data</code><code class="p">)</code>
    <code class="k">return</code></pre></div>

<p>We implement all of the functions directly using Python (as in the previous example). The first function generates an integer between 0 and 100, and the next three constitute a simple skeleton for the actual processing. The pipeline is implemented as in <a data-type="xref" href="#Pipeline_implement">EXAMPLE 4-20</a>.</p>
<div id="Pipeline_implement" data-type="example" title2="Pipeline implementation" no2="4-20">
<h5>Example 4-20. Pipeline implementation</h5>

<pre data-type="programlisting" data-code-language="python"><code class="nd">@dsl.pipeline</code><code class="p">(</code><code>
</code><code>    </code><code class="n">name</code><code class="o">=</code><code class="s1">'</code><code class="s1">Conditional execution pipeline</code><code class="s1">'</code><code class="p">,</code><code>
</code><code>    </code><code class="n">description</code><code class="o">=</code><code class="s1">'</code><code class="s1">Shows how to use dsl.Condition().</code><code class="s1">'</code><code>
</code><code class="p">)</code><code>
</code><code class="k">def</code><code> </code><code class="nf">conditional_pipeline</code><code class="p">(</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">number</code><code> </code><code class="o">=</code><code> </code><code class="n">get_random_int_op</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code><code> </code><code class="mi">100</code><code class="p">)</code><code class="o">.</code><code class="n">output</code><code> </code><a class="co" id="co_kubeflow_pipelines_CO5-1" href="#callout_kubeflow_pipelines_CO5-1"><img src="assets/1.png" alt="1" width="12" height="12"></a><code>
</code><code>    </code><code class="k">with</code><code> </code><code class="n">dsl</code><code class="o">.</code><code class="n">Condition</code><code class="p">(</code><code class="n">number</code><code> </code><code class="o">&lt;</code><code> </code><code class="mi">10</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" id="co_kubeflow_pipelines_CO5-2" href="#callout_kubeflow_pipelines_CO5-2"><img src="assets/2.png" alt="2" width="12" height="12"></a><code>
</code><code>	</code><code class="n">process_small_op</code><code class="p">(</code><code class="n">number</code><code class="p">)</code><code>
</code><code>    </code><code class="k">with</code><code> </code><code class="n">dsl</code><code class="o">.</code><code class="n">Condition</code><code class="p">(</code><code class="n">number</code><code> </code><code class="o">&gt;</code><code> </code><code class="mi">10</code><code> </code><code class="ow">and</code><code> </code><code class="n">number</code><code> </code><code class="o">&lt;</code><code> </code><code class="mi">50</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" id="co_kubeflow_pipelines_CO5-3" href="#callout_kubeflow_pipelines_CO5-2"><img src="assets/2.png" alt="2" width="12" height="12"></a><code>
</code><code>	</code><code class="n">process_medium_op</code><code class="p">(</code><code class="n">number</code><code class="p">)</code><code>
</code><code>    </code><code class="k">with</code><code> </code><code class="n">dsl</code><code class="o">.</code><code class="n">Condition</code><code class="p">(</code><code class="n">number</code><code> </code><code class="o">&gt;</code><code> </code><code class="mi">50</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" id="co_kubeflow_pipelines_CO5-4" href="#callout_kubeflow_pipelines_CO5-2"><img src="assets/2.png" alt="2" width="12" height="12"></a><code>
</code><code>	</code><code class="n">process_large_op</code><code class="p">(</code><code class="n">number</code><code class="p">)</code><code>
</code><code>
</code><code class="n">kfp</code><code class="o">.</code><code class="n">Client</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">create_run_from_pipeline_func</code><code class="p">(</code><code class="n">conditional_pipeline</code><code class="p">,</code><code> </code><code class="n">arguments</code><code class="o">=</code><code class="p">{</code><code class="p">}</code><code class="p">)</code><code> </code><a class="co" id="co_kubeflow_pipelines_CO5-5" href="#callout_kubeflow_pipelines_CO5-3"><img src="assets/3.png" alt="3" width="12" height="12"></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_kubeflow_pipelines_CO5-1" href="#co_kubeflow_pipelines_CO5-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>Depending on the number we get here…</p></dd>
<dt><a class="co" id="callout_kubeflow_pipelines_CO5-2" href="#co_kubeflow_pipelines_CO5-2"><img src="assets/2.png" alt="2" width="12" height="12"></a></dt>
<dd><p>We will continue on to one of these operations.</p></dd>
<dt><a class="co" id="callout_kubeflow_pipelines_CO5-3" href="#co_kubeflow_pipelines_CO5-5"><img src="assets/3.png" alt="3" width="12" height="12"></a></dt>
<dd><p>Note here that we are specifying empty arguments—required parameter.</p></dd>
</dl></div>

<p>Finally, the execution graph, as shown in <a data-type="xref" href="#fig_conditional_pipelines_example">FIGURE 4-9</a>.</p>

<figure><div id="fig_conditional_pipelines_example" class="figure" data-type="figure" title2="Execution of conditional pipelines example" no2="4-9">
<img src="assets/kfml_0409.png" alt="Execution of conditional Pipelines Example" width="1680" height="603">
<h6>Figure 4-9. Execution of conditional pipelines example</h6>
</div></figure>

<p class="less_space pagebreak-before">From this graph, we can see that the pipeline really splits into three branches and process-large-op execution is selected in this run. To validate that this is correct, we look at the execution log, shown in <a data-type="xref" href="#fig_conditional_pipelines_log">FIGURE 4-10</a>.</p>

<figure><div id="fig_conditional_pipelines_log" class="figure" data-type="figure" title2="Viewing conditional pipeline log" no2="4-10">
<img src="assets/kfml_0410.png" alt="Viewing Conditional Pipeline Log" width="1680" height="605">
<h6>Figure 4-10. Viewing conditional pipeline log</h6>
</div></figure>

<p>Here we can see that the generated number is 67. This number is larger than 50, which means that the <em>process_large_op</em> branch should be <a data-type="indexterm" data-startref="ch04-cond" id="idm45831177533048"></a><a data-type="indexterm" data-startref="ch04-cond2" id="idm45831177532440"></a>executed.<sup><a data-type="noteref" id="idm45831177531704-marker" href="#idm45831177531704">[7]</a></sup></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Running Pipelines on Schedule"><div class="sect2" id="run_pipelines_onsched" title2="Running Pipelines on Schedule" no2="4.3.2">
<h2>4.3.2. Running Pipelines on Schedule</h2>

<p>We have run our pipeline manually. This is good for testing, but is often insufficient for production environments.
Fortunately, you can run pipelines on a schedule,<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="periodic execution of" id="idm45831177528680"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="running" data-tertiary="on a schedule" id="idm45831177527832"></a> as described on  <a href="https://oreil.ly/8v3fb">thisKubeflow documentation page</a>.
First, you need to upload a pipeline definition and specify a description. When this is done, you can create a periodic run by
creating a run and selecting a run type of “Recurring,” then following the instructions on the screen, as seen in <a data-type="xref" href="#setting_up_periodic_execution_of_a_pipeline">FIGURE 4-11</a>.</p>

<p>In this figure we are setting a pipeline to run every day.</p>
<div data-type="warning"><h6>Warning</h6>
<p>When creating a periodic run we are specifying how often to run a pipeline, not when to run it. In the
current implementation, the time of execution is defined by when the run is created. Once it is created, it is executed
immediately and then executed with the defined frequency. If, for example, a daily run is created at 10 am, it will be executed at 10 am daily.</p>
</div>

<p>Setting periodic execution of pipelines is an important functionality, allowing you to completely automate pipeline execution.</p>

<figure><div id="setting_up_periodic_execution_of_a_pipeline" class="figure" data-type="figure" title2="Setting up periodic execution of a pipeline" no2="4-11">
<img src="assets/kfml_0411.png" alt="Setting Up Periodic Execution of a Pipeline" width="1138" height="1112">
<h6>Figure 4-11. Setting up periodic execution of a pipeline</h6>
</div></figure>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45831177520696" title2="Conclusion" no2="4.4">
<h1>4.4. Conclusion</h1>

<p>You should now have the basics of how to build, schedule, and run some simple pipelines. You also learned about the
tools that pipelines use for when you need to debug. We showed how to integrate existing software into pipelines, how to implement conditional execution inside a pipeline, and how to run pipelines on a schedule.</p>

<p>In our next chapter, we look at how to use pipelines for data preparation with some examples.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45831179676952"><sup><a href="#idm45831179676952-marker">[1]</a></sup> This can often be automatically inferred when passing the result of one pipeline stage as the input to others. You can also specify additional dependencies manually.</p><p data-type="footnote" id="idm45831178994504"><sup><a href="#idm45831178994504-marker">[2]</a></sup> Kubernetes persistent volumes can provide different <a href="">access modes</a>.</p><p data-type="footnote" id="idm45831178988264"><sup><a href="#idm45831178988264-marker">[3]</a></sup> Generic read-write-many implementation is <a href="">NFS server</a>.</p><p data-type="footnote" id="idm45831178937864"><sup><a href="#idm45831178937864-marker">[4]</a></sup> Usage of the cloud native access storage can be handy if you need to ensure portability of your solution across multiple cloud providers.</p><p data-type="footnote" id="idm45831178821608"><sup><a href="#idm45831178821608-marker">[5]</a></sup> For installation of Argo Workflow on another OS, refer to <a href="">these Argo instructions</a>.</p><p data-type="footnote" id="idm45831178079080"><sup><a href="#idm45831178079080-marker">[6]</a></sup> Many of the standard components are in <a href="">this Kubeflow GitHub repo</a>.</p><p data-type="footnote" id="idm45831177531704"><sup><a href="#idm45831177531704-marker">[7]</a></sup> A slightly more complex example of conditional processing (with nested conditions) can be found <a href="">in this GitHub site</a>.</p></div></div></section>
<section data-type="chapter" data-pdf-bookmark="Chapter 5. Data and Feature Preparation"><div class="chapter" id="data_and_feature_prep" data-type="chapter" title2="Data and Feature Preparation" no2="5">
<h1>Chapter 5. Data and Feature Preparation</h1>


<p>Machine learning algorithms are only as good as their training data. Getting good data for training involves data and feature preparation.</p>

<p><em>Data preparation</em> is the process of sourcing the data and making sure it’s valid.<a data-type="indexterm" data-primary="data preparation" data-secondary="about" id="idm45831177515864"></a><a data-type="indexterm" data-primary="data" data-secondary="preparation of" data-seealso="data preparation" id="idm45831177515016"></a> This is a multistep process<sup><a data-type="noteref" id="idm45831177513800-marker" href="#idm45831177513800">[1]</a></sup> that can include data collection, augmentation, statistics calculation, schema validation, outlier pruning, and various validation techniques. Not having enough data can lead to overfitting, missing significant correlations, and more.
Putting in the effort to collect more records and information about each<a data-type="indexterm" data-primary="training" data-secondary="impact of using more data" id="idm45831177511736"></a> sample during data preparation can considerably improve the model.<sup><a data-type="noteref" id="idm45831177510760-marker" href="#idm45831177510760">[2]</a></sup></p>

<p><em>Feature preparation</em> (sometimes called <em>feature engineering</em>) refers to transforming<a data-type="indexterm" data-primary="data preparation" data-secondary="feature preparation" data-tertiary="about" id="idm45831177506920"></a><a data-type="indexterm" data-primary="feature preparation" data-secondary="about" id="idm45831177505832"></a> the raw input data into features that the machine learning model can use.<sup><a data-type="noteref" id="idm45831177504856-marker" href="#idm45831177504856">[3]</a></sup> Poor feature preparation can lead to losing out on important relations, such as a linear model with nonlinear terms not expanded, or a deep learning model with inconsistent image 
orientation.</p>

<p>Small changes in data and feature preparation can lead to significantly different model outputs. The iterative approach is the best for both feature and data preparation, revisiting them as your understanding of the problem and model changes. Kubeflow Pipelines makes it easier for us to iterate<a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="data and feature preparation" id="idm45831177502232"></a> our data and feature preparation. We will explore how to use hyperparameter tuning to iterate in <a data-type="xref" href="#hyperparameter_tuning">CHAPTER 10</a>.</p>

<p>In this chapter, we will cover different approaches to data and feature preparation and demonstrate how to make them repeatable by using pipelines. We assume you are already familiar with local tools. As such, we’ll start by covering how to structure our local code for pipelines, and then move on to more scalable distributed tools. Once we’ve explored the tools, we’ll put them together in a pipeline, using the examples from <a data-type="xref" href="#case_studies">SECTION 1.7</a>.</p>






<section data-type="sect1" data-pdf-bookmark="Deciding on the Correct Tooling"><div class="sect1" id="idm45831177498712" title2="Deciding on the Correct Tooling" no2="5.1">
<h1>5.1. Deciding on the Correct Tooling</h1>

<p>There are a wide variety of data and feature preparation<a data-type="indexterm" data-primary="data preparation" data-secondary="tools online resource" id="idm45831177497576"></a> tools.<sup><a data-type="noteref" id="idm45831177496600-marker" href="#idm45831177496600">[4]</a></sup> We can categorize them into<a data-type="indexterm" data-primary="data preparation" data-secondary="tools, local versus distributed" id="idm45831177495064"></a> distributed and local. Local tools run on a single machine and offer a great amount of flexibility. Distributed tools run on many machines so they can handle larger and more complex tasks. With two very distinct paths of tooling, making the wrong decision here can require substantial changes in code later.</p>

<p>If the input data size is relatively small, a single machine offers you all of the tools you are used to. Larger data sizes tend to require distributed tools for the entire pipeline or just as a sampling stage. Even with smaller datasets, distributed systems, like Apache Spark, Dask, or TFX with Beam, can be faster but may require learning new tools.<sup><a data-type="noteref" id="idm45831177492744-marker" href="#idm45831177492744">[5]</a></sup></p>

<p>Using the same tool for all of the data and feature preparation activities is not necessary. Using multiple tools is especially common when working with different datasets where using the same tools would be inconvenient. Kubeflow Pipelines allows you to split the implementation into multiple steps and connect them (even if they use different languages) into a cohesive system.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Local Data and Feature Preparation"><div class="sect1" id="idm45831177491000" title2="Local Data and Feature Preparation" no2="5.2">
<h1>5.2. Local Data and Feature Preparation</h1>

<p>Working locally limits the scale of data but offers the most comprehensive range of tools.<a data-type="indexterm" data-primary="data preparation" data-secondary="local" data-tertiary="about" id="idm45831177489416"></a><a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="data and feature preparation" data-tertiary="about" id="idm45831177488168"></a> A common way to implement data and feature preparation is with Jupyter notebooks. In <a data-type="xref" href="#pipelines_ch">CHAPTER 4</a>, we covered how to turn parts of the notebook into a pipeline, and here we’ll look at how to structure our data and feature prep code to make this easy.</p>

<p>Using notebooks for data preparation can be a great way to start exploring the data. Notebooks can be especially useful at this stage since we often have the least amount of understanding, and because using visualizations to understand our data can be quite beneficial.</p>








<section data-type="sect2" data-pdf-bookmark="Fetching the Data"><div class="sect2" id="idm45831177484984" title2="Fetching the Data" no2="5.2.1">
<h2>5.2.1. Fetching the Data</h2>

<p>For our mailing list example, we use data from public archives on the internet.<a data-type="indexterm" data-primary="data preparation" data-secondary="local" data-tertiary="fetching the data" id="idm45831177483480"></a><a data-type="indexterm" data-primary="mailing list data preparation" data-secondary="fetching the data" id="idm45831177482232"></a> Ideally, you want to connect to a database, stream, or other data repository. However, even in production, fetching web data can be necessary.
First, we’ll implement our data-fetching algorithm, which takes an Apache Software Foundation (ASF) project’s email list location along with the year from which to fetch messages.
<a data-type="xref" href="#scrape_mailing_list">EXAMPLE 5-1</a> returns the path to the records it fetches so we can use that as the input to the next pipeline stage.</p>
<div data-type="note"><h6>Note</h6>
<p>The function downloads at <em>most</em> one year of data, and it sleeps between calls. This is to prevent overwhelming the ASF mail archive servers. The ASF is a charity; please be mindful of that when downloading data and do not abuse this service.</p>
</div>
<div id="scrape_mailing_list" data-type="example" title2="Downloading the mailing list data" no2="5-1">
<h5>Example 5-1. Downloading the mailing list data</h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">download_data</code><code class="p">(</code><code class="n">year</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>

  <code class="c1"># The imports are inline here so Kubeflow can serialize the function correctly.</code>
  <code class="kn">from</code> <code class="nn">datetime</code> <code class="kn">import</code> <code class="n">datetime</code>
  <code class="kn">from</code> <code class="nn">lxml</code> <code class="kn">import</code> <code class="n">etree</code>
  <code class="kn">from</code> <code class="nn">requests</code> <code class="kn">import</code> <code class="n">get</code>
  <code class="kn">from</code> <code class="nn">time</code> <code class="kn">import</code> <code class="n">sleep</code>

 <code class="kn">import</code> <code class="nn">json</code>

  <code class="k">def</code> <code class="nf">scrapeMailArchives</code><code class="p">(</code><code class="n">mailingList</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">year</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">month</code><code class="p">:</code> <code class="nb">int</code><code class="p">):</code>
      <code class="c1">#Ugly xpath code goes here. See the example repo if you're curious.</code>

   <code class="n">datesToScrape</code> <code class="o">=</code>  <code class="p">[(</code><code class="n">year</code><code class="p">,</code> <code class="n">i</code><code class="p">)</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">2</code><code class="p">)]</code>

   <code class="n">records</code> <code class="o">=</code> <code class="p">[]</code>
   <code class="k">for</code> <code class="n">y</code><code class="p">,</code><code class="n">m</code> <code class="ow">in</code> <code class="n">datesToScrape</code><code class="p">:</code>
     <code class="k">print</code><code class="p">(</code><code class="n">m</code><code class="p">,</code><code class="s2">"-"</code><code class="p">,</code><code class="n">y</code><code class="p">)</code>
     <code class="n">records</code> <code class="o">+=</code> <code class="n">scrapeMailArchives</code><code class="p">(</code><code class="s2">"spark-dev"</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">m</code><code class="p">)</code>
   <code class="n">output_path</code> <code class="o">=</code> <code class="s1">'/data_processing/data.json'</code>
   <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">output_path</code><code class="p">,</code> <code class="s1">'w'</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
     <code class="n">json</code><code class="o">.</code><code class="n">dump</code><code class="p">(</code><code class="n">records</code><code class="p">,</code> <code class="n">f</code><code class="p">)</code>

   <code class="k">return</code> <code class="n">output_path</code></pre></div>

<p>This code downloads all of the mailing list data for a given year and saves it to a known path. In this example, a <a data-type="indexterm" data-primary="persistent volume storage" data-secondary="local data preparation" id="idm45831177474216"></a><a data-type="indexterm" data-primary="storage" data-secondary="persistent volumes" data-tertiary="local data preparation" id="idm45831177473368"></a><a data-type="indexterm" data-primary="data" data-secondary="persistent volumes" data-tertiary="local data preparation" id="idm45831177472248"></a>persistent volume needs to be mounted there to allow this data to move between stages, when we make our pipeline.</p>

<p>You may have a data dump as part of the machine learning pipeline, or a different system or team may provide one. For data on GCS or a PV, you can use the built-in components <code>google-cloud/storage/download</code> or <code>filesystem/get_subdirectory</code> to load the data instead of writing a custom function.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Data Cleaning: Filtering Out the Junk"><div class="sect2" id="idm45831177358696" title2="Data Cleaning: Filtering Out the Junk" no2="5.2.2">
<h2>5.2.2. Data Cleaning: Filtering Out the Junk</h2>

<p>Now that we’ve loaded our data, it’s time to do some simple data cleaning.<a data-type="indexterm" data-primary="data preparation" data-secondary="local" data-tertiary="filtering out junk" id="idm45831177357160"></a>
Local tools are more common, so we’ll focus on them first.
While data cleaning often depends on domain expertise, there are standard tools to assist with common tasks. A first step can be validating input records by checking the schema. That is to say, we check to see if the fields are present and are the right type.</p>

<p>To check the schema in the mailing list example, we ensure a sender,<a data-type="indexterm" data-primary="mailing list data preparation" data-secondary="filtering out junk" id="idm45831177355064"></a>  subject, and body all exist. To convert this into an independent component, we’ll make our function take a parameter for the input path and return the file path to the cleaned records. The amount of code it takes to do this is relatively small, shown in <a data-type="xref" href="#clean_single_machine_ml">EXAMPLE 5-2</a>.</p>
<div id="clean_single_machine_ml" data-type="example" title2="Data cleaning" no2="5-2">
<h5>Example 5-2. Data cleaning</h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">clean_data</code><code class="p">(</code><code class="n">input_path</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">str</code><code class="p">:</code>
    <code class="kn">import</code> <code class="nn">json</code>
    <code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>

    <code class="k">print</code><code class="p">(</code><code class="s2">"loading records..."</code><code class="p">)</code>
    <code class="k">with</code> <code class="nb">open</code><code class="p">(</code><code class="n">input_path</code><code class="p">,</code> <code class="s1">'r'</code><code class="p">)</code> <code class="k">as</code> <code class="n">f</code><code class="p">:</code>
        <code class="n">records</code> <code class="o">=</code> <code class="n">json</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">f</code><code class="p">)</code>
    <code class="k">print</code><code class="p">(</code><code class="s2">"records loaded"</code><code class="p">)</code>

    <code class="n">df</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">records</code><code class="p">)</code>
    <code class="c1"># Drop records without a subject, body, or sender</code>
    <code class="n">cleaned</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="n">dropna</code><code class="p">(</code><code class="n">subset</code><code class="o">=</code><code class="p">[</code><code class="s2">"subject"</code><code class="p">,</code> <code class="s2">"body"</code><code class="p">,</code> <code class="s2">"from"</code><code class="p">])</code>

    <code class="n">output_path_hdf</code> <code class="o">=</code> <code class="s1">'/data_processing/clean_data.hdf'</code>
    <code class="n">cleaned</code><code class="o">.</code><code class="n">to_hdf</code><code class="p">(</code><code class="n">output_path_hdf</code><code class="p">,</code> <code class="n">key</code><code class="o">=</code><code class="s2">"clean"</code><code class="p">)</code>

    <code class="k">return</code> <code class="n">output_path_hdf</code></pre></div>

<p>There are many other standard data quality techniques besides dropping missing fields.
Two of the more popular ones are imputing missing <a data-type="indexterm" data-primary="data preparation" data-secondary="local" data-tertiary="missing data" id="idm45831177348376"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="missing data" data-tertiary="local" id="idm45831177196952"></a>data<sup><a data-type="noteref" id="idm45831177195672-marker" href="#idm45831177195672">[6]</a></sup> and analyzing and removing outliers that may be the result of incorrect measurements.
Regardless of which additional general techniques you decide to perform, you can simply add them to your data-cleaning function.</p>

<p>Domain specific data cleaning tools can also be beneficial. In the mailing list example, one potential source of noise in our data could be <a data-type="indexterm" data-primary="Apache SpamAssassin" id="idm45831177193208"></a><a data-type="indexterm" data-primary="mailing list data preparation" data-secondary="Apache SpamAssassin package" id="idm45831177192504"></a><a data-type="indexterm" data-primary="SpamAssassin package" id="idm45831177191624"></a><a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="data and feature preparation" data-tertiary="adding system software" id="idm45831177190952"></a><a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="adding system software" id="idm45831177189768"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="local" data-tertiary="SpamAssassin package" id="idm45831177188824"></a><a data-type="indexterm" data-primary="containers" data-secondary="SpamAssassin package" id="idm45831177187608"></a>spam messages. One way to solve this would be by using SpamAssassin. We can add this package to our container as shown in <a data-type="xref" href="#install_spamassassin">EXAMPLE 5-3</a>. Adding system software, not managed by pip, on top of the notebook images is a bit more complicated because of permissions. Most containers run as root, making it simple to install new system packages. However, because of Jupyter, the notebook containers run as a less privileged user. Installing new packages like this requires switching to the root user and back, which is not common in other Dockerfiles.</p>
<div id="install_spamassassin" data-type="example" title2="Installing SpamAssassin" no2="5-3">
<h5>Example 5-3. Installing SpamAssassin</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">ARG</code> <code class="n">base</code>
<code class="n">FROM</code> <code class="err">$</code><code class="n">base</code>
<code class="c1"># Run as root for updates</code>
<code class="n">USER</code> <code class="n">root</code>
<code class="c1"># Install SpamAssassin</code>
<code class="n">RUN</code> <code class="n">apt</code><code class="o">-</code><code class="n">get</code> <code class="n">update</code> <code class="o">&amp;&amp;</code> \
    <code class="n">apt</code><code class="o">-</code><code class="n">get</code> <code class="n">install</code> <code class="o">-</code><code class="n">yq</code> <code class="n">spamassassin</code> <code class="n">spamc</code> <code class="o">&amp;&amp;</code> \
    <code class="n">apt</code><code class="o">-</code><code class="n">get</code> <code class="n">clean</code> <code class="o">&amp;&amp;</code> \
    <code class="n">rm</code> <code class="o">-</code><code class="n">rf</code> <code class="o">/</code><code class="n">var</code><code class="o">/</code><code class="n">lib</code><code class="o">/</code><code class="n">apt</code><code class="o">/</code><code class="n">lists</code><code class="o">/*</code> <code class="o">&amp;&amp;</code> \
    <code class="n">rm</code> <code class="o">-</code><code class="n">rf</code> <code class="o">/</code><code class="n">var</code><code class="o">/</code><code class="n">cache</code><code class="o">/</code><code class="n">apt</code>
<code class="c1"># Switch back to the expected user</code>
<code class="n">USER</code> <code class="n">jovyan</code></pre></div>

<p>After you created this Dockerfile, you’ll want to build it and push the resulting image somewhere that the Kubeflow cluster can access, as in <a data-type="xref" href="#trivial_build_and_push">EXAMPLE 2-8</a>.</p>

<p>Pushing a new container is not enough to let Kubeflow know that we want to use it. When constructing a pipeline stage with <code>func_to_container_op</code>, you then need to specify the <code>base_image</code> parameter to the <code>func_to_container_op</code> function call. We’ll show this when we bring the example together as a pipeline in <a data-type="xref" href="#use_spamassassin">EXAMPLE 5-35</a>.</p>

<p>Here we see the power of containers again. You can add the tools we need on top of the building blocks provided by Kubeflow rather than making everything from scratch.</p>

<p>Once the data is cleaned, it’s time to make sure you have enough of it, or if not, explore augmenting your data.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Formatting the Data"><div class="sect2" id="idm45831177358232" title2="Formatting the Data" no2="5.2.3">
<h2>5.2.3. Formatting the Data</h2>

<p>The correct format depends on which tool you’re using to do the feature preparation.<a data-type="indexterm" data-primary="data preparation" data-secondary="local" data-tertiary="formatting the data" id="idm45831177090808"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="feature preparation" data-tertiary="data formatting and" id="idm45831177089560"></a><a data-type="indexterm" data-primary="feature preparation" data-secondary="data formatting and" id="idm45831177088344"></a> If you’re sticking with the same tool you used for data preparation, an output can be the same as input. Otherwise, you might find this a good place to change formats. For example, when using Spark for data prep and TensorFlow for training, we often implement conversion to  TFRecords here.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Feature Preparation"><div class="sect2" id="idm45831177086712" title2="Feature Preparation" no2="5.2.4">
<h2>5.2.4. Feature Preparation</h2>

<p>How to do feature preparation depends on the problem. With the mailing list example,<a data-type="indexterm" data-primary="data preparation" data-secondary="feature preparation" data-tertiary="about" id="idm45831177085176"></a><a data-type="indexterm" data-primary="feature preparation" data-secondary="about" id="idm45831177083928"></a> we can write all kinds of text-processing functions and combine them into features, as shown in <a data-type="xref" href="#local_mailing_list_feature_prep_fun">EXAMPLE 5-4</a>.</p>
<div id="local_mailing_list_feature_prep_fun" data-type="example" title2="Writing and combining text-processing functions into features" no2="5-4">
<h5>Example 5-4. Writing and combining text-processing functions into features</h5>

<pre data-type="programlisting">    df['domains'] = df['links'].apply(extract_domains)
    df['isThreadStart'] = df['depth'] == '0'

    # Arguably, you could split building the dataset away from the actual witchcraft.
    from sklearn.feature_extraction.text import TfidfVectorizer

    bodyV = TfidfVectorizer()
    bodyFeatures = bodyV.fit_transform(df['body'])

    domainV = TfidfVectorizer()

    def makeDomainsAList(d):
        return ' '.join([a for a in d if not a is None])

    domainFeatures = domainV.fit_transform(
        df['domains'].apply(makeDomainsAList))

    from scipy.sparse import csr_matrix, hstack

    data = hstack([
        csr_matrix(df[[
            'containsPythonStackTrace', 'containsJavaStackTrace',
            'containsExceptionInTaskBody', 'isThreadStart'
        ]].to_numpy()), bodyFeatures, domainFeatures
    ])</pre></div>

<p>So far, the example code is structured to allow you to turn each function into a separate pipeline stage; however, other options exist. We’ll examine how to use the entire notebook as a pipeline stage in <a data-type="xref" href="#putting_it_in_a_pipeline">SECTION 5.4</a>.</p>

<p>There are data preparation tools beyond notebooks and Python, of course. Notebooks are not always the best tool as they can have difficulty with version control. Python doesn’t always have the libraries (or performance) you need. So we’ll now look at how to use other available tools.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Custom Containers"><div class="sect2" id="idm45831177076856" title2="Custom Containers" no2="5.2.5">
<h2>5.2.5. Custom Containers</h2>

<p>Pipelines are not just limited to notebooks or even to specific <a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="language capabilities" id="idm45831177075224"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="custom code and tools inside" id="idm45831177074248"></a><a data-type="indexterm" data-primary="containers" data-secondary="pipeline custom code and tools" id="idm45831177073336"></a><a data-type="indexterm" data-primary="Python" data-secondary="pipeline custom code and tools" id="idm45831177072376"></a><a data-type="indexterm" data-primary="containers" data-secondary="custom containers" id="idm45831177071416"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="local" data-tertiary="custom containers" id="idm45831177070472"></a><a data-type="indexterm" data-primary="custom containers" id="idm45831177069256"></a><a data-type="indexterm" data-primary="language capabilities of pipelines" id="idm45831177068584"></a>languages.<sup><a data-type="noteref" id="idm45831177067768-marker" href="#idm45831177067768">[7]</a></sup> Depending on the project, you may have a regular Python project, custom tooling, Python 2, or even FORTRAN code as an essential component.</p>

<p>For instance, in <a data-type="xref" href="#beyond_tf">CHAPTER 9</a> we will use Scala to perform one step in our pipeline. Also, in <a data-type="xref" href="#Use_rstats">‘Using RStats’</a>, we discuss how to get started with an RStats container.</p>

<p>Sometimes you won’t be able to find a container that so closely matches your needs as we did here. In these cases, you can take a generic base image and build on top of it, which we look at more in <a data-type="xref" href="#beyond_tf">CHAPTER 9</a>.</p>

<p>Beyond the need for custom containers, another reason you might choose to move beyond notebooks is to explore distributed tools.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Distributed Tooling"><div class="sect1" id="idm45831177062072" title2="Distributed Tooling" no2="5.3">
<h1>5.3. Distributed Tooling</h1>

<p>Using a distributed platform makes it possible to work with large datasets (beyond a single machine memory)<a data-type="indexterm" data-primary="data preparation" data-secondary="distributed" data-tertiary="about" id="idm45831177060376"></a> and can provide significantly better performance.
Often the time when we need to start using distributed tooling is when our problem has out-grown our initial notebook solution.</p>

<p>The two main data-parallel distributed systems in Kubeflow are Apache Spark<a data-type="indexterm" data-primary="Apache Spark" data-secondary="about" id="idm45831177058424"></a><a data-type="indexterm" data-primary="Apache Beam" data-secondary="about" id="idm45831177057448"></a><a data-type="indexterm" data-primary="Google Dataflow" data-secondary="Apache Beam for" id="idm45831177056504"></a><a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="about" id="idm45831177055560"></a><a data-type="indexterm" data-primary="model inference" data-secondary="TensorFlow Serving" data-tertiary="TensorFlow Extended and" id="idm45831177054648"></a><a data-type="indexterm" data-primary="TensorFlow Serving (TFServing)" data-secondary="TensorFlow Extended integrating with" id="idm45831177053432"></a> and Google’s Dataflow (via Apache Beam). Apache Spark has a larger install base and variety of formats and libraries supported. Apache Beam supports TensorFlow Extended (TFX), an end-to-end ML tool, which integrates smoothly into TFServing for model inference. As it’s the most tightly integrated, we’ll start with exploring TFX on Apache Beam and then continue to the more standard Apache Spark.</p>








<section data-type="sect2" data-pdf-bookmark="TensorFlow Extended"><div class="sect2" id="idm45831177051880" title2="TensorFlow Extended" no2="5.3.1">
<h2>5.3.1. TensorFlow Extended</h2>

<p>The TensorFlow community has created an excellent set of integrated tools for everything from data validation to model serving.<a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="about" id="idm45831177050200"></a><a data-type="indexterm" data-primary="Apache Beam" data-secondary="TensorFlow Extended on top of" id="idm45831177049208"></a>  At present, TFX’s data tools are all built on top of Apache Beam, which has the most support for distributed processing on Google Cloud. If you want to use Kubeflow’s TFX components, you are limited to a single node; this may change in future versions.</p>
<div data-type="note"><h6>Note</h6>
<p>Apache Beam’s Python support outside of Google Cloud’s Dataflow is not as mature.<a data-type="indexterm" data-primary="Apache Beam" data-secondary="Python support" id="idm45831177015992"></a><a data-type="indexterm" data-primary="Python" data-secondary="Apache Beam support of" id="idm45831177014920"></a><a data-type="indexterm" data-primary="Python" data-secondary="TensorFlow Extended as Python tool" id="idm45831177013976"></a><a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="Apache Beam Python support and" id="idm45831177013016"></a> TFX is a Python tool, so scaling it depends on Apache Beam’s Python support. You can scale the job by using the GCP only Dataflow component. As Apache Beam’s support for Apache Flink and Spark improves, support may be added for scaling the TFX components in a portable manner.<sup><a data-type="noteref" id="idm45831177011608-marker" href="#idm45831177011608">[8]</a></sup></p>
</div>

<p>Kubeflow includes many of the TFX components in its pipeline system. TFX also has its own concept of pipelines. These are separate from Kubeflow pipelines, and in some cases <a data-type="indexterm" data-primary="Kubeflow" data-secondary="alternatives to" id="idm45831177009272"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="and TensorFlow Extended" id="idm45831177008296"></a><a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="Kubeflow pipelines and TFX pipelines" id="idm45831177007352"></a>TFX can be an alternative to Kubeflow. Here we will focus on the data and feature preparation components, since those are the simplest to be used with the rest of the Kubeflow ecosystem.</p>










<section data-type="sect3" data-pdf-bookmark="Keeping your data quality: TensorFlow data validation"><div class="sect3" id="validating_data_sec" title2="Keeping your data quality: TensorFlow data validation" no2="5.3.1.1">
<h3>5.3.1.1. Keeping your data quality: TensorFlow data validation</h3>

<p>It’s crucial to make sure data quality doesn’t decline over time. Data validation<a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="TensorFlow Data Validation" id="ch05-val4"></a><a data-type="indexterm" data-primary="data" data-secondary="validation via TensorFlow Extended" id="ch05-val"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="distributed" data-tertiary="data validation" id="ch05-val2"></a><a data-type="indexterm" data-primary="validation of data" data-secondary="TensorFlow Extended" id="ch05-val5"></a><a data-type="indexterm" data-primary="TensorFlow Data Validation (TFDV)" id="ch05-val3"></a><a data-type="indexterm" data-primary="quality of data maintained" id="idm45831176997480"></a> allows us to ensure that the schema and distribution of our data are only evolving in expected ways and catch data quality issues before they become production issues. TensorFlow Data Validation (TFDV) gives us the ability to validate our data.</p>

<p>To make the development process more straightforward, you should install TFX and TFDV locally.<a data-type="indexterm" data-primary="libraries" data-secondary="data validation via TensorFlow Extended" id="idm45831176995912"></a><a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="installing" id="idm45831176994920"></a><a data-type="indexterm" data-primary="TensorFlow Data Validation (TFDV)" data-secondary="installing" id="idm45831176993960"></a><a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="installing components" id="idm45831176993000"></a> While the code can be evaluated inside of Kubeflow only, having the library locally will speed up your development work. Installing TFX and TFDV is a simple pip install, shown in <a data-type="xref" href="#install_tfx">EXAMPLE 5-5</a>.<sup><a data-type="noteref" id="idm45831176990920-marker" href="#idm45831176990920">[9]</a></sup></p>
<div id="install_tfx" data-type="example" title2="Installing TFX and TFDV" no2="5-5">
<h5>Example 5-5. Installing TFX and TFDV</h5>

<pre data-type="programlisting" data-code-language="bash">pip3 install tfx tensorflow-data-validation</pre></div>

<p>Now let’s look at how to use TFX and TFDV in Kubeflow’s pipelines. The first step is loading the relevant components that we want to use. As we discussed in the previous chapter, while Kubeflow does have a <a data-type="indexterm" data-primary="components" data-secondary="load_component function warning" id="idm45831176985880"></a><a data-type="indexterm" data-primary="load_component warning" id="idm45831176985032"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="load_component function warning" id="idm45831176984424"></a><a data-type="indexterm" data-primary="load_component_from_file" id="idm45831176983544"></a><a data-type="indexterm" data-primary="components" data-secondary="load_component_from_file function" id="idm45831176982904"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="load_component_from_file function" id="idm45831176981288"></a><a data-type="indexterm" data-primary="schema" data-secondary="data validation via TensorFlow Extended" id="idm45831176980328"></a><a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="installing components" id="idm45831176979368"></a><code>load_component</code> function, it automatically resolves on master making it unsuitable for production use cases. So we’ll use <code>load_component_from_file</code> along with a local copy of Kubeflow components from <a data-type="xref" href="#dl_pipeline_release">EXAMPLE 4-15</a> to load our TFDV components. The basic components we need to load are: an example generator (think data loader), schema, statistics generators, and the validator itself. Loading the components is illustrated in <a data-type="xref" href="#load_tfdv_components">EXAMPLE 5-6</a>.</p>
<div id="load_tfdv_components" data-type="example" title2="Loading the components" no2="5-6">
<h5>Example 5-6. Loading the components</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">tfx_csv_gen</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">components</code><code class="o">.</code><code class="n">load_component_from_file</code><code class="p">(</code>
    <code class="s2">"pipelines-0.2.5/components/tfx/ExampleGen/CsvExampleGen/component.yaml"</code><code class="p">)</code>
<code class="n">tfx_statistic_gen</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">components</code><code class="o">.</code><code class="n">load_component_from_file</code><code class="p">(</code>
    <code class="s2">"pipelines-0.2.5/components/tfx/StatisticsGen/component.yaml"</code><code class="p">)</code>
<code class="n">tfx_schema_gen</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">components</code><code class="o">.</code><code class="n">load_component_from_file</code><code class="p">(</code>
    <code class="s2">"pipelines-0.2.5/components/tfx/SchemaGen/component.yaml"</code><code class="p">)</code>
<code class="n">tfx_example_validator</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">components</code><code class="o">.</code><code class="n">load_component_from_file</code><code class="p">(</code>
    <code class="s2">"pipelines-0.2.5/components/tfx/ExampleValidator/component.yaml"</code><code class="p">)</code></pre></div>

<p>In addition to the components, we also need our data. The current TFX components pass data between pipeline stages using <a data-type="indexterm" data-primary="file_output mechanism" id="idm45831176945832"></a><a data-type="indexterm" data-primary="MinIO" data-secondary="data validation via TensorFlow Extended" id="idm45831176914824"></a><a data-type="indexterm" data-primary="MinIO" data-secondary="file_output" id="idm45831176913880"></a>Kubeflow’s file_output mechanism. This places the output into MinIO, automatically tracking the artifacts related to the pipeline. To use TFDV on the recommender example’s input, we first download it using a standard container operation, as in <a data-type="xref" href="#dl_recommender_data">EXAMPLE 5-7</a>.</p>
<div id="dl_recommender_data" data-type="example" title2="Download recommender data" no2="5-7">
<h5>Example 5-7. Download recommender data</h5>

<pre data-type="programlisting" data-code-language="python">    <code class="n">fetch</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">dsl</code><code class="o">.</code><code class="n">ContainerOp</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'download'</code><code class="p">,</code>
                                <code class="n">image</code><code class="o">=</code><code class="s1">'busybox'</code><code class="p">,</code>
                                <code class="n">command</code><code class="o">=</code><code class="p">[</code><code class="s1">'sh'</code><code class="p">,</code> <code class="s1">'-c'</code><code class="p">],</code>
                                <code class="n">arguments</code><code class="o">=</code><code class="p">[</code>
                                    <code class="s1">'sleep 1;'</code>
                                    <code class="s1">'mkdir -p /tmp/data;'</code>
                                    <code class="s1">'wget '</code> <code class="o">+</code> <code class="n">data_url</code> <code class="o">+</code>
                                    <code class="s1">' -O /tmp/data/results.csv'</code>
                                <code class="p">],</code>
                                <code class="n">file_outputs</code><code class="o">=</code><code class="p">{</code><code class="s1">'downloaded'</code><code class="p">:</code> <code class="s1">'/tmp/data'</code><code class="p">})</code>
    <code class="c1"># This expects a directory of inputs not just a single file</code></pre></div>
<div data-type="tip"><h6>Tip</h6>
<p>If we had the data on a persistent volume (say, data fetched in a previous stage), we could then use the <code>filesystem/get_file</code> 
component.<a data-type="indexterm" data-primary="persistent volume storage" data-secondary="filesystem/get_file component" id="idm45831176834920"></a><a data-type="indexterm" data-primary="filesystem/get_file component" id="idm45831176833864"></a><a data-type="indexterm" data-primary="storage" data-secondary="persistent volumes" data-tertiary="filesystem/get_file component" id="idm45831176833176"></a><a data-type="indexterm" data-primary="data" data-secondary="persistent volumes" data-tertiary="filesystem/get_file component" id="idm45831176831944"></a></p>
</div>

<p>Once you have the data loaded, TFX has a set of tools called <a data-type="indexterm" data-primary="example generators in TensorFlow Extended" id="idm45831176829976"></a><a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="example generators" id="idm45831176829176"></a>example generators that ingest data. These support a few different formats, including CSV and TFRecord. There are also example generators for other systems, including <a data-type="indexterm" data-primary="Google BigQuery example generators" id="idm45831176827960"></a><a data-type="indexterm" data-primary="Python" data-secondary="Pandas and" id="idm45831176827272"></a><a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="Pandas dataframes accepted by" id="idm45831176826328"></a><a data-type="indexterm" data-primary="CSV component in recommender system" id="idm45831176825352"></a>Google’s BigQuery. There is not the same wide variety of formats supported by Spark or Pandas, so you may find a need to preprocess the records with another tool.<sup><a data-type="noteref" id="idm45831176824360-marker" href="#idm45831176824360">[10]</a></sup> In our recommender example, we use the CSV component, as shown in <a data-type="xref" href="#use_csv_examples">EXAMPLE 5-8</a>.</p>
<div id="use_csv_examples" data-type="example" title2="Using CSV component" no2="5-8">
<h5>Example 5-8. Using CSV component</h5>

<pre data-type="programlisting" data-code-language="python">    <code class="n">records_example</code> <code class="o">=</code> <code class="n">tfx_csv_gen</code><code class="p">(</code><code class="n">input_base</code><code class="o">=</code><code class="n">fetch</code><code class="o">.</code><code class="n">output</code><code class="p">)</code></pre></div>

<p>Now that we have a channel of examples, we can use this as one of the inputs for TFDV. <a data-type="indexterm" data-primary="schema" data-secondary="data validation via TensorFlow Extended" id="ch05-schem"></a><a data-type="indexterm" data-primary="schema" data-secondary="inferred by TensorFlow Data Validation" id="idm45831176751944"></a><a data-type="indexterm" data-primary="TensorFlow Data Validation (TFDV)" data-secondary="schema inferred by" id="idm45831176751096"></a><a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="schema inferred by TFDV" id="idm45831176815480"></a><a data-type="indexterm" data-primary="validation of data" data-secondary="TensorFlow Extended" data-tertiary="schema inferred" id="idm45831176814568"></a>The recommended approach for creating a schema is to use TFDV to infer the schema. To be able to infer the schema, TFDV first needs to compute some summary statistics of our data. <a data-type="xref" href="#make_schema">EXAMPLE 5-9</a> illustrates the pipeline stages for both of these steps.</p>
<div id="make_schema" data-type="example" title2="Creating the schema" no2="5-9">
<h5>Example 5-9. Creating the schema</h5>

<pre data-type="programlisting" data-code-language="python">    <code class="n">stats</code> <code class="o">=</code> <code class="n">tfx_statistic_gen</code><code class="p">(</code><code class="n">input_data</code><code class="o">=</code><code class="n">records_example</code><code class="o">.</code><code class="n">output</code><code class="p">)</code>
    <code class="n">schema_op</code> <code class="o">=</code> <code class="n">tfx_schema_gen</code><code class="p">(</code><code class="n">stats</code><code class="o">.</code><code class="n">output</code><code class="p">)</code></pre></div>

<p>If we infer the schema each time, we are unlikely to catch schema changes.<a data-type="indexterm" data-primary="schema" data-secondary="saved to catch changes" id="idm45831176797144"></a> Instead, you should save the schema and reuse it in future runs for validation. The pipeline’s run web page has a link to the schema in MinIO, and you can either fetch it or copy it somewhere using another component or container operation.</p>

<p>Regardless of where you persist the schema, you should inspect it.<a data-type="indexterm" data-primary="schema" data-secondary="inspecting" id="idm45831176795304"></a><a data-type="indexterm" data-primary="TensorFlow Data Validation (TFDV)" data-secondary="schema inspection" id="idm45831176794392"></a><a data-type="indexterm" data-primary="display_schema" id="idm45831176793480"></a><a data-type="indexterm" data-primary="user interfaces (UI)" data-secondary="display_schema" id="idm45831176725016"></a><a data-type="indexterm" data-primary="libraries" data-secondary="data validation via TensorFlow Extended" id="idm45831176724072"></a> To inspect the schema, you need to import the TFDV library, as shown in <a data-type="xref" href="#import_tfdv">EXAMPLE 5-10</a>. Before you start using a schema to validate data, you should inspect the schema. To inspect the schema, download the schema locally (or onto your notebook) and use the <code>display_schema</code> function from TFDV, as shown in <a data-type="xref" href="#display_tfdv_schema">EXAMPLE 5-11</a>.</p>
<div id="import_tfdv" data-type="example" title2="Download the schema locally" no2="5-10">
<h5>Example 5-10. Download the schema locally</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">tensorflow_data_validation</code> <code class="kn">as</code> <code class="nn">tfdv</code></pre></div>
<div id="display_tfdv_schema" data-type="example" title2="Display the schema" no2="5-11">
<h5>Example 5-11. Display the schema</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">schema</code> <code class="o">=</code> <code class="n">tfdv</code><code class="o">.</code><code class="n">load_schema_text</code><code class="p">(</code><code class="s2">"schema_info_2"</code><code class="p">)</code>
<code class="n">tfdv</code><code class="o">.</code><code class="n">display_schema</code><code class="p">(</code><code class="n">schema</code><code class="p">)</code></pre></div>

<p>If needed, the <code>schema_util.py</code> script (downloadble from the<a data-type="indexterm" data-primary="schema" data-secondary="tool for modifying" id="idm45831176708696"></a> <a href="https://oreil.ly/qjHeI">TensorFlow GitHub repo</a>) provides the tools to modify your schema (be it for evolution or incorrect inference).</p>

<p>Now that we know we’re using the right schema, let’s validate our data. The validate component takes in both the schema and the statistics we’ve generated, as shown in <a data-type="xref" href="#tfdv_validate">EXAMPLE 5-12</a>. You should replace the schema and statistics generation components with downloads of their outputs at production time.</p>
<div id="tfdv_validate" data-type="example" class="less_space pagebreak-before" title2="Validating the data" no2="5-12">
<h5>Example 5-12. Validating the data</h5>

<pre data-type="programlisting" data-code-language="python">    <code class="n">tfx_example_validator</code><code class="p">(</code><code class="n">stats</code><code class="o">=</code><code class="n">stats</code><code class="o">.</code><code class="n">outputs</code><code class="p">[</code><code class="s1">'output'</code><code class="p">],</code>
                          <code class="n">schema</code><code class="o">=</code><code class="n">schema_op</code><code class="o">.</code><code class="n">outputs</code><code class="p">[</code><code class="s1">'output'</code><code class="p">])</code></pre></div>
<div data-type="tip"><h6>Tip</h6>
<p>Check the size of the rejected records before pushing to production.<a data-type="indexterm" data-primary="schema" data-secondary="rejected records check" id="idm45831176612648"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="distributed" data-tertiary="rejected records check" id="idm45831176611768"></a> You may find that the data format has changed, and you need to use the schema evolution guide and possibly update the rest of the pipeline.<a data-type="indexterm" data-startref="ch05-val" id="idm45831176610264"></a><a data-type="indexterm" data-startref="ch05-val2" id="idm45831176609592"></a><a data-type="indexterm" data-startref="ch05-val3" id="idm45831176608920"></a><a data-type="indexterm" data-startref="ch05-val4" id="idm45831176608248"></a><a data-type="indexterm" data-startref="ch05-val5" id="idm45831176607576"></a><a data-type="indexterm" data-startref="ch05-schem" id="idm45831176606904"></a></p>
</div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="TensorFlow Transform, with TensorFlow Extended on Beam"><div class="sect3" id="idm45831177005000" title2="TensorFlow Transform, with TensorFlow Extended on Beam" no2="5.3.1.2">
<h3>5.3.1.2. TensorFlow Transform, with TensorFlow Extended on Beam</h3>

<p>The TFX program for doing feature preparation is called <a data-type="indexterm" data-primary="feature preparation" data-secondary="TensorFlow Transform" id="ch05-tft"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="feature preparation" data-tertiary="TensorFlow Transform" id="ch05-tft2"></a><a data-type="indexterm" data-primary="TensorFlow Transform (TFT)" data-secondary="feature preparation" id="ch05-tft3"></a><a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="Transform feature preparation" id="ch05-tft4"></a><a data-type="indexterm" data-primary="TensorFlow Transform (TFT)" data-secondary="Model Analysis integration" id="idm45831176599704"></a><a data-type="indexterm" data-primary="TensorFlow Model Analysis" id="idm45831176598728"></a>TensorFlow Transform (TFT) and integrates into the TensorFlow and Kubeflow ecosystems. As with TFDV, Kubeflow’s TensorFlow Transform component currently does not scale beyond single node processing. The best benefit of TFT is its integration into the TensorFlow Model Analysis tool, simplifying feature preparation during inference.</p>

<p>We need to specify what transformations we want TFT to apply. Our TFT program should be in a file separate from the pipeline definition, although it is possible to inline it as a string. To start with, we need some standard TFT imports, as shown in <a data-type="xref" href="#tft_imports">EXAMPLE 5-13</a>.</p>
<div id="tft_imports" data-type="example" title2="TFT imports" no2="5-13">
<h5>Example 5-13. TFT imports</h5>

<pre data-type="programlisting">import tensorflow as tf
import tensorflow_transform as tft
from tensorflow_transform.tf_metadata import schema_utils</pre></div>

<p>Now that we’ve got the imports, it’s time to create the entry point into our code for the component, shown in <a data-type="xref" href="#tft_entry">EXAMPLE 5-14</a>.</p>
<div id="tft_entry" data-type="example" title2="Creating the entry point" no2="5-14">
<h5>Example 5-14. Creating the entry point</h5>

<pre data-type="programlisting">def preprocessing_fn(inputs):</pre></div>

<p>Inside this function is where we do our data transformations to produce our features. Not all features need to be transformed, which is why there is also a copy method to mirror the input to the output if you’re only adding features. With our mailing list example, we can compute the vocabulary, as shown in <a data-type="xref" href="#tft_logic">EXAMPLE 5-15</a>.</p>
<div id="tft_logic" data-type="example" class="less_space pagebreak-before" title2="Compute the vocabulary" no2="5-15">
<h5>Example 5-15. Compute the vocabulary</h5>

<pre data-type="programlisting">    outputs = {}
    # TFT business logic goes here
    outputs["body_stuff"] = tft.compute_and_apply_vocabulary(inputs["body"],
                                                             top_k=1000)
    return outputs</pre></div>

<p>This function does not support arbitrary python code.
All transformations must be expressed as TensorFlow or TensorFlow Transform operations.
TensorFlow operations operate on one tensor at a time, but in data preparation we often want to compute something over all of our input data, and TensorFlow Transform’s operations give us this ability.
See the <a href="https://oreil.ly/4j0mv">TFT Python docs</a> or call <code>help(tft)</code> to see some starting operations.</p>

<p>Once you’ve written the desired transformations, it is time to add them to the <a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="transformation code" id="idm45831176565560"></a><a data-type="indexterm" data-primary="tfx/Transform component" id="idm45831176564616"></a>pipeline. The simplest way to do this is with Kubeflow’s <code>tfx/Transform</code> component. Loading the component is the same as the other TFX components, illustrated in <a data-type="xref" href="#load_tfdv_components">EXAMPLE 5-6</a>.
Using this component is unique in requiring the transformation code to be passed in as a file uploaded to either S3 or GCS. It also needs the data, and you can use the output from TFDV (if you used it) or load the examples as we did for TFDV. Using the TFT component is illustrated in <a data-type="xref" href="#use_tft">EXAMPLE 5-16</a>.</p>
<div id="use_tft" data-type="example" title2="Using the TFT component" no2="5-16">
<h5>Example 5-16. Using the TFT component</h5>

<pre data-type="programlisting" data-code-language="python">    <code class="n">transformed_output</code> <code class="o">=</code> <code class="n">tfx_transform</code><code class="p">(</code>
        <code class="n">input_data</code><code class="o">=</code><code class="n">records_example</code><code class="o">.</code><code class="n">output</code><code class="p">,</code>
        <code class="n">schema</code><code class="o">=</code><code class="n">schema_op</code><code class="o">.</code><code class="n">outputs</code><code class="p">[</code><code class="s1">'output'</code><code class="p">],</code>
        <code class="n">module_file</code><code class="o">=</code><code class="n">module_file</code><code class="p">)</code>  <code class="c1"># Path to your TFT code on GCS/S3</code></pre></div>

<p>Now you have a machine learning pipeline that has feature preparation along with a critical artifact to transform requests at serving time. The close integration of TensorFlow Transform can make model serving much less complicated. TensorFlow Transform with Kubeflow components doesn’t have the power for all projects, so we’ll look at distributed feature preparation next.<a data-type="indexterm" data-startref="ch05-tft" id="idm45831176534280"></a><a data-type="indexterm" data-startref="ch05-tft2" id="idm45831176533784"></a><a data-type="indexterm" data-startref="ch05-tft3" id="idm45831176533176"></a><a data-type="indexterm" data-startref="ch05-tft4" id="idm45831176532536"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Distributed Data Using Apache Spark"><div class="sect2" id="idm45831176605848" title2="Distributed Data Using Apache Spark" no2="5.3.2">
<h2>5.3.2. Distributed Data Using Apache Spark</h2>

<p>Apache Spark is an open source distributed data processing tool that can run<a data-type="indexterm" data-primary="Apache Spark" data-secondary="about" id="idm45831176529976"></a><a data-type="indexterm" data-primary="Apache Spark" data-secondary="using with Kubeflow" id="idm45831176529000"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="distributed" data-tertiary="Apache Spark setup" id="ch05-spse"></a> on a variety of clusters. Kubeflow supports Apache Spark through a few different components so you can access cloud-specific features. Since you may not be familiar with Spark we’ll briefly introduce Spark’s Dataset/Dataframe APIs in the context of data and feature preparation. If you want to go beyond the basics, we <a data-type="indexterm" data-primary="Apache Spark" data-secondary="resources on" id="idm45831176526104"></a>recommend <a class="orm:hideurl" href="https://learning.oreilly.com/library/view/learning-spark/9781449359034"><em>Learning Spark</em></a>, <a class="orm:hideurl" href="https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201"><em>Spark: The Definitive Guide</em></a>, or <a class="orm:hideurl" href="https://learning.oreilly.com/library/view/high-performance-spark/9781491943199"><em>High Performance Spark</em></a> as resources to improve your Spark skills.</p>
<div data-type="note"><h6>Note</h6>
<p>Here our code is structured to go in as a single stage for all of the feature and data preparation, since once you’re at scale, writing and loading the data between steps is costly.</p>
</div>
<aside data-type="sidebar"><div class="sidebar" id="idm45831176520504">
<h5>Spark in Jupyter</h5>
<p>Spark is not preinstalled in the notebook images. You can use pip inside your <a data-type="indexterm" data-primary="Apache Spark" data-secondary="Jupyter notebooks" id="idm45831176518936"></a><a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="Apache Spark via Dockerfile" id="idm45831176517960"></a><a data-type="indexterm" data-primary="Docker" data-secondary="Apache Spark on Jupyter notebooks" id="idm45831176517048"></a>notebook to install Spark, but this does not support complex environments. Instead, take the notebook container you’re working with and add Spark with a new Dockerfile, as shown in <a data-type="xref" href="#add_spark_to_nb_image">EXAMPLE 5-17</a>.</p>
<div id="add_spark_to_nb_image" data-type="example" title2="Adding Spark" no2="5-17">
<h5>Example 5-17. Adding Spark</h5>

<pre data-type="programlisting" data-code-language="dockerfile"><code class="c"># See https://www.kubeflow.org/docs/notebooks/custom-notebook/</code>
ARG base
<code class="k">FROM</code><code class="s"> $base</code>
ARG sparkversion
ARG sparkrelease
ARG sparkserver https://www-us.apache.org/dist/spark
<code class="c"># We need to run as root for updates</code>
USER root

<code class="c"># Set an environment variable for where we are going to put Spark</code>
<code class="k">ENV</code><code class="s"> SPARK_HOME /opt/spark</code>

<code class="c"># Install Java because Spark needs it</code>
<code class="k">RUN</code> apt-get update <code class="o">&amp;&amp;</code> <code class="se">\</code>
    apt-get install -yq openjdk-8-jre openjdk-8-jre-headless <code class="o">&amp;&amp;</code> <code class="se">\</code>
    apt-get clean <code class="o">&amp;&amp;</code> <code class="se">\</code>
    rm -rf /var/lib/apt/lists/*

<code class="c"># Install Spark</code>
<code class="k">RUN</code> <code class="nb">set</code> -ex <code class="o">&amp;&amp;</code> <code class="se">\</code>
    rm /bin/sh <code class="o">&amp;&amp;</code> <code class="se">\</code>
    ln -sv /bin/bash /bin/sh

<code class="k">RUN</code>  <code class="nb">echo</code> <code class="s2">"Setting up </code><code class="nv">$sparkversion</code><code class="s2">"</code>
<code class="k">RUN</code>  <code class="nb">cd</code> /tmp <code class="o">&amp;&amp;</code> <code class="se">\</code>
     <code class="o">(</code>wget <code class="si">${</code><code class="nv">sparkserver</code><code class="si">}</code>/spark-<code class="si">${</code><code class="nv">sparkversion</code><code class="si">}</code>/<code class="si">${</code><code class="nv">sparkrelease</code><code class="si">}</code>.tgz<code class="o">)</code> <code class="o">&amp;&amp;</code> <code class="se">\</code>
     <code class="nb">cd</code> /opt <code class="o">&amp;&amp;</code> tar -xvf /tmp/<code class="si">${</code><code class="nv">sparkrelease</code><code class="si">}</code>.tgz <code class="o">&amp;&amp;</code> <code class="se">\</code>
     rm /tmp/<code class="si">${</code><code class="nv">sparkrelease</code><code class="si">}</code>.tgz <code class="o">&amp;&amp;</code> mv <code class="si">${</code><code class="nv">sparkrelease</code><code class="si">}</code> spark <code class="o">&amp;&amp;</code> <code class="se">\</code>
     <code class="nb">cd </code>spark/python <code class="o">&amp;&amp;</code> pip install -e .
<code class="c"># Fix permissions</code>
<code class="k">WORKDIR</code><code class="s"> /opt/spark/work-dir</code>
<code class="k">RUN</code> chmod -R <code class="m">777</code> /opt/spark/

<code class="c"># Switch the user back; using jovyan as a user is bad but the base image</code>
<code class="c"># depends on it.</code>
USER jovyan
<code class="c"># Install some common tools</code>
pip install pandas numpy scipy pyarrow</pre></div>

<p>The Spark workers don’t have a way to connect to our notebook server so we can’t send data and requests back and forth. To enable this, you can create a service using the name of the notebook to make it discoverable. The service definition exposes two ports, as shown in <a data-type="xref" href="#sample_spark_nb_service">EXAMPLE 5-18</a>, for a user in the “programmerboo” namespace with a notebook named “spark-test-2.” Once you’ve written the service definition, all that is needed is to run <code>kubectl apply -f my_spark_service.yaml</code>.</p>
<div id="sample_spark_nb_service" data-type="example" title2="Sample service definition" no2="5-18">
<h5>Example 5-18. Sample service definition</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Service</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">spark-driver</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">kubeflow-programmerboo</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">selector</code><code class="p">:</code>
    <code class="nt">notebook-name</code><code class="p">:</code> <code class="l-Scalar-Plain">spark-test-2</code>
  <code class="nt">ports</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">39235</code>
      <code class="nt">targetPort</code><code class="p">:</code> <code class="l-Scalar-Plain">39235</code>
      <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">spark-driver-port</code>
    <code class="p-Indicator">-</code> <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">39236</code>
      <code class="nt">targetPort</code><code class="p">:</code> <code class="l-Scalar-Plain">39236</code>
      <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">spark-block-port</code></pre></div>

<p>When we make the SparkContext, we’ll configure it to use this service as the hostname. Jupyter notebooks make important activities like testing and version management challenging. Notebooks are great for the exploration phase, but as you move on, you should consider using a Spark operator.</p>
</div></aside>










<section data-type="sect3" data-pdf-bookmark="Spark operators in Kubeflow"><div class="sect3" id="idm45831176271832" title2="Spark operators in Kubeflow" no2="5.3.2.1">
<h3>5.3.2.1. Spark operators in Kubeflow</h3>

<p>Using Kubeflow’s native Spark operator EMR, or Dataproc is best once you’ve <a data-type="indexterm" data-primary="Apache Spark" data-secondary="Kubeflow native operator" id="ch05-emr"></a><a data-type="indexterm" data-primary="EMR native Spark operator" id="ch05-emr2"></a><a data-type="indexterm" data-primary="Google Dataproc for Apache Spark" id="ch05-emr3"></a>moved beyond the experimental phase. The most portable operator is the native Spark operator, which does not depend on any specific cloud. To use any of the operators, you need to package the Spark program and store it on either a distributed filesystem (such as GCS, S3, and so on) or put it inside a container.</p>

<p>If you’re working in Python or R, we recommend building a Spark<a data-type="indexterm" data-primary="Python" data-secondary="Kubeflow native Spark operator" id="idm45831176379080"></a> container so you can install your dependencies. With Scala or Java code, this is less critical. If you put the application inside of a container, you can reference it with <code>local:///</code>. You can use the gcr.io/spark-operator/spark-py:v2.4.5 container as the base or build your own container—follow Spark on Kubernetes instructions, or see <a data-type="xref" href="#beyond_tf">CHAPTER 9</a>.
<a data-type="xref" href="#install_pyapp_and_deps">EXAMPLE 5-19</a> shows how to install any requirements and copy the application. If you decide to update the application, you can still use the container, just configure the main resource with a distributed filesystem.</p>

<p>We cover building custom containers additionally in <a data-type="xref" href="#beyond_tf">CHAPTER 9</a>.</p>
<div id="install_pyapp_and_deps" data-type="example" title2="Installing requirements and copying the application" no2="5-19">
<h5>Example 5-19. Installing requirements and copying the application</h5>

<pre data-type="programlisting" data-code-language="dockerfile"><code class="c"># Use the Spark operator image as base</code>
<code class="k">FROM</code><code class="s"> gcr.io/spark-operator/spark-py:v2.4.5</code>
<code class="c"># Install Python requirements</code>
COPY requirements.txt /
<code class="k">RUN</code> pip3 install -r /requirements.txt
<code class="c"># Now you can reference local:///job/my_file.py</code>
<code class="k">RUN</code> mkdir -p /job
COPY *.py /job

<code class="k">ENTRYPOINT</code><code class="s"> ["/opt/entrypoint.sh"]</code></pre></div>

<p>Two cloud-specific options for running Spark are the Amazon EMR and <a data-type="indexterm" data-primary="Apache Spark" data-secondary="cloud-specific options for running" id="idm45831176368280"></a><a data-type="indexterm" data-primary="Amazon EMR" id="idm45831176351976"></a>Google Dataproc components in Kubeflow. However, they each take different parameters, meaning that you will need to translate your pipeline.</p>

<p>The EMR components allow you to set up clusters, submit jobs, and clean up the clusters. The two cluster task components are <code>aws/emr/create_cluster</code> for the start and <code>aws/emr/delete_cluster</code>. The component for running a PySpark job is <code>aws/emr/submit_pyspark_job</code>. If you are not reusing an external cluster, it’s important to trigger the delete component regardless whether the submit_pyspark_job component succeeds.</p>

<p>While they have different parameters, the workflow for Dataproc clusters mirrors that of EMR. The components are similarly named, with <code>gcp/dataproc/create_cluster/</code> and <code>gcp/dataproc/delete_cluster/</code> for the life cycle and <code>gcp/dataproc/submit_pyspark_job/</code> for running our job.</p>

<p>Unlike the EMR and Dataproc components, the Spark operator does not have a component. For Kubernetes operators without components, you can use the dsl.ResourceOp to call them. <a data-type="xref" href="#launch_spark_with_operator">EXAMPLE 5-20</a> illustrates using the ResourceOp to launch a Spark job.</p>
<div id="launch_spark_with_operator" data-type="example" title2="Using the ResourceOp to launch a Spark job" no2="5-20">
<h5>Example 5-20. Using the ResourceOp to launch a Spark job</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">resource</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"apiVersion"</code><code class="p">:</code> <code class="s2">"sparkoperator.k8s.io/v1beta2"</code><code class="p">,</code>
    <code class="s2">"kind"</code><code class="p">:</code> <code class="s2">"SparkApplication"</code><code class="p">,</code>
    <code class="s2">"metadata"</code><code class="p">:</code> <code class="p">{</code>
        <code class="s2">"name"</code><code class="p">:</code> <code class="s2">"boop"</code><code class="p">,</code>
        <code class="s2">"namespace"</code><code class="p">:</code> <code class="s2">"kubeflow"</code>
    <code class="p">},</code>
    <code class="s2">"spec"</code><code class="p">:</code> <code class="p">{</code>
        <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"Python"</code><code class="p">,</code>
        <code class="s2">"mode"</code><code class="p">:</code> <code class="s2">"cluster"</code><code class="p">,</code>
        <code class="s2">"image"</code><code class="p">:</code> <code class="s2">"gcr.io/boos-demo-projects-are-rad/kf-steps/kubeflow/myspark"</code><code class="p">,</code>
        <code class="s2">"imagePullPolicy"</code><code class="p">:</code> <code class="s2">"Always"</code><code class="p">,</code>
        <code class="c1"># See the Dockerfile OR use GCS/S3/...</code>
        <code class="s2">"mainApplicationFile"</code><code class="p">:</code> <code class="s2">"local:///job/job.py"</code><code class="p">,</code>
        <code class="s2">"sparkVersion"</code><code class="p">:</code> <code class="s2">"2.4.5"</code><code class="p">,</code>
        <code class="s2">"restartPolicy"</code><code class="p">:</code> <code class="p">{</code>
            <code class="s2">"type"</code><code class="p">:</code> <code class="s2">"Never"</code>
        <code class="p">},</code>
        <code class="s2">"driver"</code><code class="p">:</code> <code class="p">{</code>
            <code class="s2">"cores"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code>
            <code class="s2">"coreLimit"</code><code class="p">:</code> <code class="s2">"1200m"</code><code class="p">,</code>
            <code class="s2">"memory"</code><code class="p">:</code> <code class="s2">"512m"</code><code class="p">,</code>
            <code class="s2">"labels"</code><code class="p">:</code> <code class="p">{</code>
                <code class="s2">"version"</code><code class="p">:</code> <code class="s2">"2.4.5"</code><code class="p">,</code>
            <code class="p">},</code>
            <code class="c1"># also try spark-operatoroperator-sa</code>
            <code class="s2">"serviceAccount"</code><code class="p">:</code> <code class="s2">"spark-operatoroperator-sa"</code><code class="p">,</code>
        <code class="p">},</code>
        <code class="s2">"executor"</code><code class="p">:</code> <code class="p">{</code>
            <code class="s2">"cores"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code>
            <code class="s2">"instances"</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code>
            <code class="s2">"memory"</code><code class="p">:</code> <code class="s2">"512m"</code>
        <code class="p">},</code>
        <code class="s2">"labels"</code><code class="p">:</code> <code class="p">{</code>
            <code class="s2">"version"</code><code class="p">:</code> <code class="s2">"2.4.5"</code>
        <code class="p">},</code>
    <code class="p">}</code>
<code class="p">}</code>


<code class="nd">@dsl.pipeline</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s2">"local Pipeline"</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s2">"No need to ask why."</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">local_pipeline</code><code class="p">():</code>

    <code class="n">rop</code> <code class="o">=</code> <code class="n">dsl</code><code class="o">.</code><code class="n">ResourceOp</code><code class="p">(</code>
        <code class="n">name</code><code class="o">=</code><code class="s2">"boop"</code><code class="p">,</code>
        <code class="n">k8s_resource</code><code class="o">=</code><code class="n">resource</code><code class="p">,</code>
        <code class="n">action</code><code class="o">=</code><code class="s2">"create"</code><code class="p">,</code>
        <code class="n">success_condition</code><code class="o">=</code><code class="s2">"status.applicationState.state == COMPLETED"</code><code class="p">)</code></pre></div>
<div data-type="warning"><h6>Warning</h6>
<p>Kubeflow doesn’t apply any validation to ResourceOp requests.<a data-type="indexterm" data-primary="ResourceOp request validation" id="idm45831176345992"></a><a data-type="indexterm" data-primary="Apache Spark" data-secondary="ResourceOp request validation" id="idm45831176345288"></a> For example, in Spark, the job name must be able to be used as the start of a valid DNS name, and while in container ops container names are rewritten, ResourceOps just directly passes through requests.</p>
</div>
<aside data-type="sidebar"><div class="sidebar" id="idm45831176092056">
<h5>Spark Basics</h5>
<p>Apache Spark has APIs available in Python, R, Scala, and Java, with <a data-type="indexterm" data-primary="Apache Spark" data-secondary="basics" id="idm45831176090648"></a><a data-type="indexterm" data-primary="Python" data-secondary="Apache Spark" data-tertiary="basics" id="idm45831176089672"></a>some third-party support for other languages. We’ll use the Python interface, as it is popular in the machine learning community. The first thing needed in any Spark program is a Spark session or context (as in <a data-type="xref" href="#launch_spark_with_operator_2">EXAMPLE 5-21</a>).</p>
<div id="launch_spark_with_operator_2" data-type="example" title2="Launching your Spark session" no2="5-21">
<h5>Example 5-21. Launching your Spark session</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">pyspark.sql</code> <code class="kn">import</code> <code class="n">SparkSession</code>
<code class="kn">from</code> <code class="nn">pyspark.sql.functions</code> <code class="kn">import</code> <code class="n">col</code><code class="p">,</code> <code class="n">to_date</code>
<code class="kn">from</code> <code class="nn">pyspark.sql.types</code> <code class="kn">import</code> <code class="o">*</code>
<code class="n">session</code> <code class="o">=</code> <code class="n">SparkSession</code><code class="o">.</code><code class="n">builder</code><code class="o">.</code><code class="n">getOrCreate</code><code class="p">()</code></pre></div>

<p>This example was so simple because it reads its configuration from the environment it is called in. This works with the Spark operator, which does much of the setup for us. When working in a notebook, though, we need to provide some extra information so the executors can connect back to the notebook. Once you’ve set up the service so the notebook and the driver can communicate, as described in <a data-type="xref" href="#sample_spark_nb_service">EXAMPLE 5-18</a>, you would then <a data-type="indexterm" data-primary="Apache Spark" data-secondary="configuring" id="idm45831176060520"></a>configure your Spark session to tell the executors to use this service, as shown in <a data-type="xref" href="#spark_nb_configs">EXAMPLE 5-22</a>.</p>
<div id="spark_nb_configs" data-type="example" title2="Configuring your Spark session" no2="5-22">
<h5>Example 5-22. Configuring your Spark session</h5>

<pre data-type="programlisting" data-code-language="python">    <code class="o">.</code><code class="n">config</code><code class="p">(</code><code class="s2">"spark.driver.bindAddress"</code><code class="p">,</code>
            <code class="s2">"0.0.0.0"</code><code class="p">)</code><code class="o">.</code><code class="n">config</code><code class="p">(</code><code class="s2">"spark.kubernetes.namespace"</code><code class="p">,</code>
                              <code class="s2">"kubeflow-programmerboo"</code><code class="p">)</code><code class="o">.</code>
    <code class="n">config</code><code class="p">(</code><code class="s2">"spark.master"</code><code class="p">,</code> <code class="s2">"k8s://https://kubernetes.default"</code><code class="p">)</code><code class="o">.</code><code class="n">config</code><code class="p">(</code>
        <code class="s2">"spark.driver.host"</code><code class="p">,</code>
        <code class="s2">"spark-driver.kubeflow-programmerboo.svc.cluster.local"</code><code class="p">)</code><code class="o">.</code><code class="n">config</code><code class="p">(</code>
            <code class="s2">"spark.kubernetes.executor.annotation.sidecar.istio.io/inject"</code><code class="p">,</code>
            <code class="s2">"false"</code><code class="p">)</code><code class="o">.</code><code class="n">config</code><code class="p">(</code><code class="s2">"spark.driver.port"</code><code class="p">,</code>
                            <code class="s2">"39235"</code><code class="p">)</code><code class="o">.</code><code class="n">config</code><code class="p">(</code><code class="s2">"spark.blockManager.port"</code><code class="p">,</code> <code class="s2">"39236"</code><code class="p">)</code></pre></div>

<p>Also, we need the versions of Python to match, since a version mismatch may cause serialization and function errors. To do this we add <code>os.environ["PYSPARK_PYTHON"] = "python3.6"</code> to our notebook and install Python 3.6 in Spark’s worker container, as in <a data-type="xref" href="#install_python_36_spark">EXAMPLE 5-23</a>.</p>
<div id="install_python_36_spark" data-type="example" title2="Installing Python 3.6 in Spark’s worker container" no2="5-23">
<h5>Example 5-23. Installing Python 3.6 in Spark’s worker container</h5>

<pre data-type="programlisting" data-code-language="dockerfile">ARG base
<code class="k">FROM</code><code class="s"> $base</code>

USER root

<code class="c"># Install libraries we need to build Python 3.6</code>
<code class="k">RUN</code> apt-get update <code class="o">&amp;&amp;</code> <code class="se">\</code>
    <code class="nv">DEBIAN_FRONTEND</code><code class="o">=</code>noninteractive apt-get install -y -q <code class="se">\</code>
    make build-essential libssl-dev zlib1g-dev libbz2-dev <code class="se">\</code>
    libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev <code class="se">\</code>
    libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev <code class="o">&amp;&amp;</code> <code class="se">\</code>
    rm -rf /var/cache/apt

<code class="c"># Install Python 3.6 to match the notebook</code>
<code class="k">RUN</code> <code class="nb">cd</code> /tmp <code class="o">&amp;&amp;</code> <code class="se">\</code>
    wget https://www.python.org/ftp/python/3.6.10/Python-3.6.10.tgz <code class="o">&amp;&amp;</code> <code class="se">\</code>
    tar -xvf Python-3.6.10.tgz <code class="o">&amp;&amp;</code> <code class="se">\</code>
    <code class="nb">cd </code>Python-3.6.10 <code class="o">&amp;&amp;</code> <code class="se">\</code>
    ./configure <code class="o">&amp;&amp;</code> <code class="se">\</code>
    make -j <code class="m">8</code> <code class="o">&amp;&amp;</code> <code class="se">\</code>
    make altinstall

<code class="k">RUN</code> python3.6 -m pip install pandas <code class="nv">pyarrow</code><code class="o">==</code>0.11.0 spacy
<code class="c"># We depend on Spark being on the PYTHONPATH so no pip install</code>
USER <code class="m">185</code></pre></div>

<p>Using MinIO, Kubeflow’s built-in S3-like service, requires some additional configuration.<a data-type="indexterm" data-primary="Apache Spark" data-secondary="MinIO configuration" id="idm45831175853784"></a><a data-type="indexterm" data-primary="MinIO" data-secondary="Apache Spark configuration" id="idm45831175825320"></a> <a data-type="xref" href="#configure_spark_minio">EXAMPLE 5-24</a> illustrates how to configure Spark to use MinIO in Kubeflow.</p>
<div id="configure_spark_minio" data-type="example" title2="Configuring Spark to use MinIO" no2="5-24">
<h5>Example 5-24. Configuring Spark to use MinIO</h5>

<pre data-type="programlisting">    .config("spark.hadoop.fs.s3a.endpoint",
            "minio-service.kubeflow.svc.cluster.local:9000").config(
                "fs.s3a.connection.ssl.enabled",
                "false").config("fs.s3a.path.style.access", "true")
    # You can also add an account using the minio command as described in
    # Chapter 1.
    .config("spark.hadoop.fs.s3a.access.key",
            "minio").config("spark.hadoop.fs.s3a.secret.key", "minio123")</pre></div>
<div data-type="note"><h6>Note</h6>
<p>MinIO only works out of the box with Spark 3 or higher.</p>
</div>

<p>Now that we’ve got Spark up and running, it’s time to look at the basic tasks you will want to do for data preparation and cleaning in Spark.<a data-type="indexterm" data-primary="validation of data" data-secondary="Apache Spark" id="ch05-sprk2"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="distributed" data-tertiary="Apache Spark for" id="ch05-sprk"></a><a data-type="indexterm" data-startref="ch05-emr" id="idm45831175816568"></a><a data-type="indexterm" data-startref="ch05-emr2" id="idm45831175815896"></a><a data-type="indexterm" data-startref="ch05-emr3" id="idm45831175815224"></a><a data-type="indexterm" data-startref="ch05-spse" id="idm45831175814552"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="distributed" data-tertiary="data validation" id="ch05-dvsp"></a></p>
</div></aside>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Reading the input data"><div class="sect3" id="idm45831176271368" title2="Reading the input data" no2="5.3.2.2">
<h3>5.3.2.2. Reading the input data</h3>

<p>Spark supports a wide variety of data sources, including (but not limited to):<a data-type="indexterm" data-primary="mailing list data preparation" data-secondary="Apache Spark" data-tertiary="reading input data" id="idm45831175811032"></a><a data-type="indexterm" data-primary="Python" data-secondary="Apache Spark" data-tertiary="reading input data" id="idm45831175809720"></a> Parquet, JSON, JDBC, ORC, JSON, Hive, CSV, ElasticSearch, MongoDB, Neo4j, Cassandra, Snowflake, Redis, Riak Time Series, etc.<sup><a data-type="noteref" id="idm45831175808232-marker" href="#idm45831175808232">[11]</a></sup>
Loading data is very straightforward, and often all that is needed is specifying the format. For instance, in our mailing list example, reading the Parquet-formatted output of our data preparation stage is done as in <a data-type="xref" href="#ex_load_parquet">EXAMPLE 5-25</a>.</p>
<div id="ex_load_parquet" data-type="example" title2="Reading our data’s Parquet-formatted output" no2="5-25">
<h5>Example 5-25. Reading our data’s Parquet-formatted output</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">initial_posts</code> <code class="o">=</code> <code class="n">session</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="s2">"parquet"</code><code class="p">)</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">fs_prefix</code> <code class="o">+</code>
                                                    <code class="s2">"/initial_posts"</code><code class="p">)</code>
<code class="n">ids_in_reply</code> <code class="o">=</code> <code class="n">session</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="s2">"parquet"</code><code class="p">)</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">fs_prefix</code> <code class="o">+</code> <code class="s2">"/ids_in_reply"</code><code class="p">)</code></pre></div>

<p>If this had instead been formatted as JSON, we would only have to change “parquet” to “JSON.”<sup><a data-type="noteref" id="idm45831175711608-marker" href="#idm45831175711608">[12]</a></sup></p>
<aside data-type="sidebar" class="less_space pagebreak-before"><div class="sidebar" id="idm45831175753384">
<h5>Fetching Input Data</h5>
<p>We can also speed up our data fetching by using a block of our Python code to fetch our data in parallel. If we look back at the mailing list example, we could download each year on a different computer. Or if we wanted to look at multiple projects, we could also fetch by project. <a data-type="indexterm" data-primary="mailing list data preparation" data-secondary="parallelize for data fetching" id="idm45831175751448"></a><a data-type="indexterm" data-primary="parallelize for data fetching" id="idm45831175750536"></a>We can do this by using <code>parallelize</code>, which gives us a distributed list, and <code>flatMap</code>, which runs a Python function on the different executors. For example, <code>sc.parallelize([1, 2, 3]).map(fetchRecord)</code> effectively runs the <code>fetchRecords</code> function in parallel 3 times with the inputs 1, 2, and 3, respectively, and concatenates the results.</p>
</div></aside>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Validating the schema"><div class="sect3" id="idm45831175747768" title2="Validating the schema" no2="5.3.2.3">
<h3>5.3.2.3. Validating the schema</h3>

<p>We often believe we know the fields and types of our data. <a data-type="indexterm" data-primary="Apache Spark" data-secondary="schema validation" id="idm45831175746440"></a><a data-type="indexterm" data-primary="schema" data-secondary="validation by Apache Spark" id="idm45831175745464"></a>Spark can quickly discover the schema when our data is in a self-describing format like Parquet. In other formats, like JSON, the schema isn’t known until Spark reads the records. Regardless of the data format, it is good practice to specify the schema and ensure the data matches it, as shown in <a data-type="xref" href="#ex_load_parquet_schema">EXAMPLE 5-26</a>. Errors during data load are easier to debug than errors during model deployment.</p>
<div id="ex_load_parquet_schema" data-type="example" title2="Specifying the schema" no2="5-26">
<h5>Example 5-26. Specifying the schema</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">ids_schema</code> <code class="o">=</code> <code class="n">StructType</code><code class="p">([</code>
    <code class="n">StructField</code><code class="p">(</code><code class="s2">"In-Reply-To"</code><code class="p">,</code> <code class="n">StringType</code><code class="p">(),</code> <code class="n">nullable</code><code class="o">=</code><code class="bp">True</code><code class="p">),</code>
    <code class="n">StructField</code><code class="p">(</code><code class="s2">"message-id"</code><code class="p">,</code> <code class="n">StringType</code><code class="p">(),</code> <code class="n">nullable</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="p">])</code>
<code class="n">ids_in_reply</code> <code class="o">=</code> <code class="n">session</code><code class="o">.</code><code class="n">read</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="s2">"parquet"</code><code class="p">)</code><code class="o">.</code><code class="n">schema</code><code class="p">(</code><code class="n">ids_schema</code><code class="p">)</code><code class="o">.</code><code class="n">load</code><code class="p">(</code>
    <code class="n">fs_prefix</code> <code class="o">+</code> <code class="s2">"/ids_in_reply"</code><code class="p">)</code></pre></div>

<p>You can configure Spark to handle corrupted and nonconforming records by dropping them, keeping them, or stopping the process (i.e., failing the job). The default is permissive, which keeps the invalid records while setting the fields to null, allowing us to handle schema mismatch with the same techniques for missing fields.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Handling missing fields"><div class="sect3" id="idm45831175665256" title2="Handling missing fields" no2="5.3.2.4">
<h3>5.3.2.4. Handling missing fields</h3>

<p>In many situations, some of our data is missing. You can choose to drop <a data-type="indexterm" data-primary="data preparation" data-secondary="distributed" data-tertiary="missing data" id="idm45831175664120"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="missing data" data-tertiary="distributed platform" id="idm45831175662872"></a><a data-type="indexterm" data-primary="mailing list data preparation" data-secondary="Apache Spark" data-tertiary="handling missing data" id="idm45831175661656"></a>records with missing fields, fall back to secondary fields, impute averages, or leave as is. Spark’s built-in tools for these tasks are inside <code>DataFrameNaFunctions</code>. The correct solution depends on both your data and the algorithm you end up using. The most common is to drop records and make sure that we have not filtered out too many records, illustrated using the mailing list data in <a data-type="xref" href="#drop_na_spark">EXAMPLE 5-27</a>.</p>
<div id="drop_na_spark" data-type="example" title2="Dropping records" no2="5-27">
<h5>Example 5-27. Dropping records</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">initial_posts_count</code> <code class="o">=</code> <code class="n">initial_posts</code><code class="o">.</code><code class="n">count</code><code class="p">()</code>
<code class="n">initial_posts_cleaned</code> <code class="o">=</code> <code class="n">initial_posts</code><code class="o">.</code><code class="n">na</code><code class="o">.</code><code class="n">drop</code><code class="p">(</code><code class="n">how</code><code class="o">=</code><code class="s1">'any'</code><code class="p">,</code>
                                              <code class="n">subset</code><code class="o">=</code><code class="p">[</code><code class="s1">'body'</code><code class="p">,</code> <code class="s1">'from'</code><code class="p">])</code>
<code class="n">initial_posts_cleaned_count</code> <code class="o">=</code> <code class="n">initial_posts_cleaned</code><code class="o">.</code><code class="n">count</code><code class="p">()</code></pre></div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Filtering out bad data"><div class="sect3" id="idm45831175656136" title2="Filtering out bad data" no2="5.3.2.5">
<h3>5.3.2.5. Filtering out bad data</h3>

<p>Detecting incorrect data can be challenging. However, without performing<a data-type="indexterm" data-primary="mailing list data preparation" data-secondary="Apache Spark" data-tertiary="filtering out bad data" id="idm45831175588104"></a><a data-type="indexterm" data-primary="denoising data" data-secondary="Apache Spark" id="idm45831175586920"></a><a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="Apache Spark" id="idm45831175585976"></a> at least some data cleaning, the model may train on noise. Often, determining bad data depends on the practitioner’s domain knowledge of the problem.</p>

<p>A common technique supported in Spark is outlier removal. However, naively applying this can remove valid records. Using your domain experience, you can write a custom validation function and remove any records that do not match it using Spark’s <code>filter</code> function, as illustrated with our mailing list example in <a data-type="xref" href="#filter_junk_spark">EXAMPLE 5-28</a>.</p>
<div id="filter_junk_spark" data-type="example" title2="Filtering out bad data" no2="5-28">
<h5>Example 5-28. Filtering out bad data</h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">is_ok</code><code class="p">(</code><code class="n">post</code><code class="p">):</code>
    <code class="c1"># Your special business logic goes here</code>
    <code class="k">return</code> <code class="bp">True</code>


<code class="n">spark_mailing_list_data_cleaned</code> <code class="o">=</code> <code class="n">spark_mailing_list_data_with_date</code><code class="o">.</code><code class="n">filter</code><code class="p">(</code>
    <code class="n">is_ok</code><code class="p">)</code></pre></div>
<aside data-type="sidebar"><div class="sidebar" id="idm45831175514840">
<h5>Using Spark SQL</h5>
<p>If you’re a pro at SQL and less so with Scala or Python, you can also <a data-type="indexterm" data-primary="mailing list data preparation" data-secondary="Apache Spark" data-tertiary="SQL" id="idm45831175563624"></a><a data-type="indexterm" data-primary="SQL in Apache Spark" id="idm45831175562440"></a><a data-type="indexterm" data-primary="Apache Spark" data-secondary="SQL" id="idm45831175561768"></a>directly write SQL queries. After you’ve loaded data, you can give the data names with registerTempTable and then use the SQL function on the Spark session (see <a data-type="xref" href="#use_spark_sql">EXAMPLE 5-29</a>).</p>
<div id="use_spark_sql" data-type="example" title2="Using Spark SQL" no2="5-29">
<h5>Example 5-29. Using Spark SQL</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">ids_in_reply</code><code class="o">.</code><code class="n">registerTempTable</code><code class="p">(</code><code class="s2">"cheese"</code><code class="p">)</code>
<code class="n">no_text</code> <code class="o">=</code> <code class="n">session</code><code class="o">.</code><code class="n">sql</code><code class="p">(</code><code class="s2">"select * from cheese where body = '' AND subject = ''"</code><code class="p">)</code></pre></div>
</div></aside>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Saving the output"><div class="sect3" id="idm45831175523048" title2="Saving the output" no2="5.3.2.6">
<h3>5.3.2.6. Saving the output</h3>

<p>Once you have the data ready, it’s time to save the output. If you’re going<a data-type="indexterm" data-primary="mailing list data preparation" data-secondary="Apache Spark" data-tertiary="saving the output" id="idm45831175521800"></a><a data-type="indexterm" data-primary="persistent volume storage" data-secondary="Apache Spark output" id="idm45831175494920"></a><a data-type="indexterm" data-primary="storage" data-secondary="persistent volumes" data-tertiary="Apache Spark output" id="idm45831175494008"></a><a data-type="indexterm" data-primary="data" data-secondary="persistent volumes" data-tertiary="Apache Spark output" id="idm45831175492792"></a> to use Apache Spark to do feature preparation, you can skip this step for now.</p>

<p>If you want to go back to single-machine tools, it’s often simplest to save to a persistent volume. To do this, bring the data back to the main program by calling <code>toPandas()</code>, as shown in <a data-type="xref" href="#spark_to_pandas">EXAMPLE 5-30</a>. Now you can save the data in whatever format the next tool expects.</p>
<div id="spark_to_pandas" data-type="example" title2="Saving to a persistent volume" no2="5-30">
<h5>Example 5-30. Saving to a persistent volume</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">initial_posts</code><code class="o">.</code><code class="n">toPandas</code><code class="p">()</code></pre></div>

<p>If the data is large, or you otherwise want to use an object store, <a data-type="indexterm" data-primary="object stores" data-secondary="using with Apache Spark" id="idm45831175483928"></a>Spark can write to many different formats (just as it can load from many different formats). The correct format depends on the tool you intend to use for feature preparation. Writing to Parquet is shown in <a data-type="xref" href="#write_big_data">EXAMPLE 5-31</a>.</p>
<div id="write_big_data" data-type="example" title2="Writing to Parquet" no2="5-31">
<h5>Example 5-31. Writing to Parquet</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">initial_posts</code><code class="o">.</code><code class="n">write</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="s2">"parquet"</code><code class="p">)</code><code class="o">.</code><code class="n">mode</code><code class="p">(</code><code class="s1">'overwrite'</code><code class="p">)</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="n">fs_prefix</code> <code class="o">+</code>
                                                             <code class="s2">"/initial_posts"</code><code class="p">)</code>
<code class="n">ids_in_reply</code><code class="o">.</code><code class="n">write</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="s2">"parquet"</code><code class="p">)</code><code class="o">.</code><code class="n">mode</code><code class="p">(</code><code class="s1">'overwrite'</code><code class="p">)</code><code class="o">.</code><code class="n">save</code><code class="p">(</code><code class="n">fs_prefix</code> <code class="o">+</code>
                                                            <code class="s2">"/ids_in_reply"</code><code class="p">)</code></pre></div>

<p>Now you’ve seen a variety of different tools you can use to source and clean the data. We’ve looked at the flexibility of local tools, the scalability of distributed tools, and the integration from TensorFlow Extended. With the data in shape, let’s now make sure the right features are available and get them in a usable format for the machine learning model.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Distributed Feature Preparation Using Apache Spark"><div class="sect2" id="idm45831176531272" title2="Distributed Feature Preparation Using Apache Spark" no2="5.3.3">
<h2>5.3.3. Distributed Feature Preparation Using Apache Spark</h2>

<p>Apache Spark has a large number of built-in feature preparation tools, <a data-type="indexterm" data-primary="data preparation" data-secondary="distributed" data-tertiary="Apache Spark feature preparation" id="idm45831175395864"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="feature preparation" data-tertiary="Apache Spark" id="idm45831175394680"></a><a data-type="indexterm" data-primary="feature preparation" data-secondary="Apache Spark" id="idm45831175393464"></a><a data-type="indexterm" data-primary="Apache Spark" data-secondary="feature preparation example" id="idm45831175392520"></a>in <code>pyspark.ml.feature</code>, that you can use to generate features. You can use Spark in the same way as you did during data preparation. You may find using Spark’s own ML pipeline an easy way to put together multiple feature preparation stages.</p>

<p>For the Spark mailing list example, we have textual input data. To allow us to train a variety of models, converting this into word vectors is our preferred form of feature prep. Doing so involves first tokenizing the data with Spark’s Tokenizer. Once we have the tokens, we can train a Word2Vec model and produce our word vectors. <a data-type="xref" href="#ex_spark_feature_prep">EXAMPLE 5-32</a> illustrates how to prepare features for the mailing list example using Spark.<a data-type="indexterm" data-primary="Apache Spark" data-secondary="mailing list example" id="idm45831175389224"></a></p>
<div id="ex_spark_feature_prep" data-type="example" title2="Preparing features for the mailing list" no2="5-32">
<h5>Example 5-32. Preparing features for the mailing list</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">tokenizer</code> <code class="o">=</code> <code class="n">Tokenizer</code><code class="p">(</code><code class="n">inputCol</code><code class="o">=</code><code class="s2">"body"</code><code class="p">,</code> <code class="n">outputCol</code><code class="o">=</code><code class="s2">"body_tokens"</code><code class="p">)</code>
<code class="n">body_hashing</code> <code class="o">=</code> <code class="n">HashingTF</code><code class="p">(</code><code class="n">inputCol</code><code class="o">=</code><code class="s2">"body_tokens"</code><code class="p">,</code>
                         <code class="n">outputCol</code><code class="o">=</code><code class="s2">"raw_body_features"</code><code class="p">,</code>
                         <code class="n">numFeatures</code><code class="o">=</code><code class="mi">10000</code><code class="p">)</code>
<code class="n">body_idf</code> <code class="o">=</code> <code class="n">IDF</code><code class="p">(</code><code class="n">inputCol</code><code class="o">=</code><code class="s2">"raw_body_features"</code><code class="p">,</code> <code class="n">outputCol</code><code class="o">=</code><code class="s2">"body_features"</code><code class="p">)</code>
<code class="n">body_word2Vec</code> <code class="o">=</code> <code class="n">Word2Vec</code><code class="p">(</code><code class="n">vectorSize</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code>
                         <code class="n">minCount</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>
                         <code class="n">numPartitions</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code>
                         <code class="n">inputCol</code><code class="o">=</code><code class="s2">"body_tokens"</code><code class="p">,</code>
                         <code class="n">outputCol</code><code class="o">=</code><code class="s2">"body_vecs"</code><code class="p">)</code>
<code class="n">assembler</code> <code class="o">=</code> <code class="n">VectorAssembler</code><code class="p">(</code><code class="n">inputCols</code><code class="o">=</code><code class="p">[</code>
    <code class="s2">"body_features"</code><code class="p">,</code> <code class="s2">"body_vecs"</code><code class="p">,</code> <code class="s2">"contains_python_stack_trace"</code><code class="p">,</code>
    <code class="s2">"contains_java_stack_trace"</code><code class="p">,</code> <code class="s2">"contains_exception_in_task"</code>
<code class="p">],</code>
                            <code class="n">outputCol</code><code class="o">=</code><code class="s2">"features"</code><code class="p">)</code></pre></div>

<p>With this final distributed feature preparation example, you’re ready to <a data-type="indexterm" data-primary="scalability of Kubeflow" data-secondary="Apache Spark feature preparation" id="idm45831175384744"></a>scale up to handle larger data sizes if they ever come your way. If you’re working with smaller data, you’ve seen how you can use the same simple techniques of containerization to continue to work with your favorite tools. Either way, you’re almost ready for the next stage in the machine learning pipeline.<a data-type="indexterm" data-startref="ch05-sprk" id="idm45831175255144"></a><a data-type="indexterm" data-startref="ch05-dvsp" id="idm45831175254536"></a><a data-type="indexterm" data-startref="ch05-sprk2" id="idm45831175253896"></a></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Putting It Together in a Pipeline"><div class="sect1" id="putting_it_in_a_pipeline" title2="Putting It Together in a Pipeline" no2="5.4">
<h1>5.4. Putting It Together in a Pipeline</h1>

<p>We have shown how to solve individual problems in data and feature preparation, but now we need to bring it all together.<a data-type="indexterm" data-primary="data preparation" data-secondary="putting together into a pipeline" id="ch05-tog"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="data and feature preparation" id="ch05-tog2"></a><a data-type="indexterm" data-primary="mailing list data preparation" data-secondary="putting together into a pipeline" id="ch05-tog3"></a> In our local example, we wrote our functions with the types and returned parameters to make it easy to put into a pipeline. Since we return the path of where our output is in each stage, we can use the function outputs to create the dependency graph for us. Putting these functions together into a pipeline is illustrated in <a data-type="xref" href="#single_machine_pipeline">EXAMPLE 5-33</a>.</p>
<div id="single_machine_pipeline" data-type="example" title2="Putting the functions together" no2="5-33">
<h5>Example 5-33. Putting the functions together</h5>

<pre data-type="programlisting" data-code-language="python"><code class="nd">@kfp.dsl.pipeline</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'Simple1'</code><code class="p">,</code> <code class="n">description</code><code class="o">=</code><code class="s1">'Simple1'</code><code class="p">)</code>
<code class="k">def</code> <code class="nf">my_pipeline_mini</code><code class="p">(</code><code class="n">year</code><code class="p">:</code> <code class="nb">int</code><code class="p">):</code>
    <code class="n">dvop</code> <code class="o">=</code> <code class="n">dsl</code><code class="o">.</code><code class="n">VolumeOp</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s2">"create_pvc"</code><code class="p">,</code>
                        <code class="n">resource_name</code><code class="o">=</code><code class="s2">"my-pvc-2"</code><code class="p">,</code>
                        <code class="n">size</code><code class="o">=</code><code class="s2">"5Gi"</code><code class="p">,</code>
                        <code class="n">modes</code><code class="o">=</code><code class="n">dsl</code><code class="o">.</code><code class="n">VOLUME_MODE_RWO</code><code class="p">)</code>
    <code class="n">tldvop</code> <code class="o">=</code> <code class="n">dsl</code><code class="o">.</code><code class="n">VolumeOp</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s2">"create_pvc"</code><code class="p">,</code>
                          <code class="n">resource_name</code><code class="o">=</code><code class="s2">"tld-volume-2"</code><code class="p">,</code>
                          <code class="n">size</code><code class="o">=</code><code class="s2">"100Mi"</code><code class="p">,</code>
                          <code class="n">modes</code><code class="o">=</code><code class="n">dsl</code><code class="o">.</code><code class="n">VOLUME_MODE_RWO</code><code class="p">)</code>
    <code class="n">download_data_op</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">components</code><code class="o">.</code><code class="n">func_to_container_op</code><code class="p">(</code>
        <code class="n">download_data</code><code class="p">,</code> <code class="n">packages_to_install</code><code class="o">=</code><code class="p">[</code><code class="s1">'lxml'</code><code class="p">,</code> <code class="s1">'requests'</code><code class="p">])</code>
    <code class="n">download_tld_info_op</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">components</code><code class="o">.</code><code class="n">func_to_container_op</code><code class="p">(</code>
        <code class="n">download_tld_data</code><code class="p">,</code>
        <code class="n">packages_to_install</code><code class="o">=</code><code class="p">[</code><code class="s1">'requests'</code><code class="p">,</code> <code class="s1">'pandas&gt;=0.24'</code><code class="p">,</code> <code class="s1">'tables'</code><code class="p">])</code>
    <code class="n">clean_data_op</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">components</code><code class="o">.</code><code class="n">func_to_container_op</code><code class="p">(</code>
        <code class="n">clean_data</code><code class="p">,</code> <code class="n">packages_to_install</code><code class="o">=</code><code class="p">[</code><code class="s1">'pandas&gt;=0.24'</code><code class="p">,</code> <code class="s1">'tables'</code><code class="p">])</code>

    <code class="n">step1</code> <code class="o">=</code> <code class="n">download_data_op</code><code class="p">(</code><code class="n">year</code><code class="p">)</code><code class="o">.</code><code class="n">add_pvolumes</code><code class="p">(</code>
        <code class="p">{</code><code class="s2">"/data_processing"</code><code class="p">:</code> <code class="n">dvop</code><code class="o">.</code><code class="n">volume</code><code class="p">})</code>
    <code class="n">step2</code> <code class="o">=</code> <code class="n">clean_data_op</code><code class="p">(</code><code class="n">input_path</code><code class="o">=</code><code class="n">step1</code><code class="o">.</code><code class="n">output</code><code class="p">)</code><code class="o">.</code><code class="n">add_pvolumes</code><code class="p">(</code>
        <code class="p">{</code><code class="s2">"/data_processing"</code><code class="p">:</code> <code class="n">dvop</code><code class="o">.</code><code class="n">volume</code><code class="p">})</code>
    <code class="n">step3</code> <code class="o">=</code> <code class="n">download_tld_info_op</code><code class="p">()</code><code class="o">.</code><code class="n">add_pvolumes</code><code class="p">({</code><code class="s2">"/tld_info"</code><code class="p">:</code> <code class="n">tldvop</code><code class="o">.</code><code class="n">volume</code><code class="p">})</code>


<code class="n">kfp</code><code class="o">.</code><code class="n">compiler</code><code class="o">.</code><code class="n">Compiler</code><code class="p">()</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">my_pipeline_mini</code><code class="p">,</code> <code class="s1">'local-data-prep-2.zip'</code><code class="p">)</code></pre></div>

<p>You can see that the feature preparation step here follows the same general pattern of all of the local components. However, the libraries that we need for our feature 
preparation are a bit different, so we’ve changed the <code>packages_to_install</code> value to install <a data-type="indexterm" data-primary="libraries" data-secondary="Scikit-learn" id="idm45831175241160"></a><a data-type="indexterm" data-primary="Scikit-learn" data-secondary="library" id="idm45831175240248"></a>Scikit-learn, as shown in <a data-type="xref" href="#Instal_Scikit_learn">EXAMPLE 5-34</a>.</p>
<div id="Instal_Scikit_learn" data-type="example" title2="Installing Scikit-learn" no2="5-34">
<h5>Example 5-34. Installing Scikit-learn</h5>

<pre data-type="programlisting" data-code-language="python">    <code class="n">prepare_features_op</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">components</code><code class="o">.</code><code class="n">func_to_container_op</code><code class="p">(</code>
        <code class="n">prepare_features</code><code class="p">,</code>
        <code class="n">packages_to_install</code><code class="o">=</code><code class="p">[</code><code class="s1">'pandas&gt;=0.24'</code><code class="p">,</code> <code class="s1">'tables'</code><code class="p">,</code> <code class="s1">'scikit-learn'</code><code class="p">])</code></pre></div>
<div data-type="tip"><h6>Tip</h6>
<p>When you start exploring a new dataset, you may find it easier to use a<a data-type="indexterm" data-primary="datasets" data-see="data" id="idm45831175032040"></a><a data-type="indexterm" data-primary="data" data-secondary="exploring new" id="idm45831175031128"></a> notebook as usual, without the pipeline components. When possible following the same general structure you would with pipelines will make it easier to productionize your exploratory work.</p>
</div>

<p>These steps don’t specify the container to use. For the container with<a data-type="indexterm" data-primary="mailing list data preparation" data-secondary="Apache SpamAssassin package" id="idm45831175029336"></a><a data-type="indexterm" data-primary="Apache SpamAssassin" id="idm45831175028424"></a><a data-type="indexterm" data-primary="containers" data-secondary="SpamAssassin package" id="idm45831175027752"></a><a data-type="indexterm" data-primary="SpamAssassin package" id="idm45831175024088"></a> SpamAssassin you’ve just built, you write it as in <a data-type="xref" href="#use_spamassassin">EXAMPLE 5-35</a>.</p>
<div id="use_spamassassin" data-type="example" title2="Specifying a container" no2="5-35">
<h5>Example 5-35. Specifying a container</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">clean_data_op</code> <code class="o">=</code> <code class="n">kfp</code><code class="o">.</code><code class="n">components</code><code class="o">.</code><code class="n">func_to_container_op</code><code class="p">(</code>
    <code class="n">clean_data</code><code class="p">,</code>
    <code class="n">base_image</code><code class="o">=</code><code class="s2">"{0}/kubeflow/spammassisan"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">container_registry</code><code class="p">),</code>
    <code class="n">packages_to_install</code><code class="o">=</code><code class="p">[</code><code class="s1">'pandas&gt;=0.24'</code><code class="p">,</code> <code class="s1">'tables'</code><code class="p">])</code></pre></div>

<p>Sometimes the cost of writing our data out in between stages is too expensive.<a data-type="indexterm" data-primary="mailing list data preparation" data-secondary="Apache Spark" data-tertiary="saving the output" id="idm45831175010456"></a> In our recommender example, unlike in the mailing list example, we’ve chosen to put data and feature prep together into a single pipeline stage. In our distributed mailing list example, we build one single Spark job as well. In these cases, our entire work so far is just one stage. Using a single stage allows us to avoid having to write the file out in between, but can complicate debugging.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Using an Entire Notebook as a Data Preparation 
Pipeline Stage"><div class="sect1" id="notebook_as_pipeline_stage" title2="Using an Entire Notebook as a Data Preparation 
Pipeline Stage" no2="5.5">
<h1>5.5. Using an Entire Notebook as a Data Preparation <br>Pipeline Stage</h1>

<p>If you don’t want to turn the individual parts of the data preparation notebook into a pipeline,<a data-type="indexterm" data-primary="data preparation" data-secondary="putting together into a pipeline" data-tertiary="entire notebook as pipeline stage" id="idm45831174909928"></a><a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="data and feature preparation" data-tertiary="entire notebook as pipeline stage" id="idm45831174908648"></a> you can use the entire notebook as one stage. You can use the same containers used by <a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="JupyterHub" id="idm45831174907272"></a>JupyterHub to run the notebook programmatically.</p>

<p>To do this, you’ll need to make a new <code>Dockerfile</code>, specify that it is based<a data-type="indexterm" data-primary="Docker" data-secondary="entire notebook as pipeline stage" id="idm45831174905176"></a> on top of another container using <code>FROM</code>, and then add a <code>COPY</code> directive to package the notebook inside the new container. Since the census data example has a preexisting notebook, that’s the approach we’ve taken in <a data-type="xref" href="#dockerfile_run_nb">EXAMPLE 5-36</a>.</p>
<div id="dockerfile_run_nb" data-type="example" title2="Using an entire notebook as data preparation" no2="5-36">
<h5>Example 5-36. Using an entire notebook as data preparation</h5>

<pre data-type="programlisting" data-code-language="dockerfile"><code class="k">FROM</code><code class="s"> gcr.io/kubeflow-images-public/tensorflow-1.6.0-notebook-cpu</code>

COPY ./ /workdir /</pre></div>

<p>If you require additional Python dependencies, you can use the <code>RUN</code> directive to install them. Putting the dependencies in the container can help speed up the pipeline, especially for complicated packages. For our mailing list example, the Dockerfile would look like <a data-type="xref" href="#add_py_deps_nb">EXAMPLE 5-37</a>.</p>
<div id="add_py_deps_nb" data-type="example" title2="Using RUN to add Python dependencies to the container" no2="5-37">
<h5>Example 5-37. Using RUN to add Python dependencies to the container</h5>

<pre data-type="programlisting" data-code-language="dockerfile"><code class="k">RUN</code> pip3 install --upgrade lxml pandas</pre></div>

<p>We can use this container like any other in the pipeline with <code>dsl.ContainerOp</code>, as we did with the recommender example in <a data-type="xref" href="#pipelines_ch">CHAPTER 4</a>. Now you have two ways to use notebooks in Kubeflow, and we’ll cover options beyond notebooks next.<a data-type="indexterm" data-startref="ch05-tog" id="idm45831174890072"></a><a data-type="indexterm" data-startref="ch05-tog2" id="idm45831174889432"></a><a data-type="indexterm" data-startref="ch05-tog3" id="idm45831174894408"></a></p>
<div data-type="tip"><h6>Tip</h6>
<p>Does the notebook need GPU resources? When specifying the<a data-type="indexterm" data-primary="GPUs" data-secondary="resource marking in code" id="idm45831174892536"></a><a data-type="indexterm" data-primary="Python" data-secondary="GPU resource marking" id="idm45831174891592"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="GPU resource marking" id="idm45831174938952"></a><a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="GPU resources" id="idm45831174938008"></a> <code>dsl.ContainerOp</code>, add a call to <code>set_gpu_limit</code> and specify either <code>nvidia</code> or <code>amd</code> depending on the desired GPU type.</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45831174935048" title2="Conclusion" no2="5.6">
<h1>5.6. Conclusion</h1>

<p>Now you have your data ready to train a model. We’ve seen how there is no one-size-fits-all approach to feature and data preparation; our different examples needed different tooling. We’ve also seen how the methods can require changing within the same problem, like when we expanded the scope of the mailing list example to include more data. The amount and quality of the features, and the data to produce them, are critical to the success of the machine learning projects. You can test this by running the examples with smaller data sizes and comparing the models.</p>

<p>It’s also important to remember that data and feature preparation is not a one-and-done activity, and you may want to revisit this step as you develop this model. You may find that there is a feature you wish you had, or that a feature you thought would perform well isn’t suggesting data quality issues. In the coming chapters, as we train our models and serve them, feel free to revisit the data and feature preparation.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45831177513800"><sup><a href="#idm45831177513800-marker">[1]</a></sup> See the <a href="">TFX documentation</a> for a good summary if you are new to data  preparation.</p><p data-type="footnote" id="idm45831177510760"><sup><a href="#idm45831177510760-marker">[2]</a></sup> The positive impact of using more data in training is made clear in A. Halevy et al., “The Unreasonable Effectiveness of Data,” IEEE Intelligent Systems 24, no. 2 (March/April 2009): 8-12, <a href=""><em class="hyperlink">https://oreil.ly/YI820</em></a>, and T. Schnoebelen, “More Data Beats Better Algorithms,” Data Science Central, September 23, 2016, <a href=""><em class="hyperlink">https://oreil.ly/oLe1R</em></a>.</p><p data-type="footnote" id="idm45831177504856"><sup><a href="#idm45831177504856-marker">[3]</a></sup> For the formal definition, see <a href="">“Six Steps to Master Machine Learning with Data Preparation”</a>.</p><p data-type="footnote" id="idm45831177496600"><sup><a href="#idm45831177496600-marker">[4]</a></sup> There are too many tools to cover here, but this <a href="">blog post</a> includes many.</p><p data-type="footnote" id="idm45831177492744"><sup><a href="#idm45831177492744-marker">[5]</a></sup> Datasets tend to grow over time rather than shrinking, so starting with distributed tooling can help you scale your work.</p><p data-type="footnote" id="idm45831177195672"><sup><a href="#idm45831177195672-marker">[6]</a></sup> See this <a href="">blog post</a> on some common techniques for imputing missing data.</p><p data-type="footnote" id="idm45831177067768"><sup><a href="#idm45831177067768-marker">[7]</a></sup> Have some VB6 code you really need to run? Check out <a data-type="xref" href="#beyond_tf">CHAPTER 9</a>, on going beyond TensorFlow, and make a small sacrifice of wine.</p><p data-type="footnote" id="idm45831177011608"><sup><a href="#idm45831177011608-marker">[8]</a></sup> There is a compatibility matrix available on <a href="">this Apache page</a>, although currently Beam’s Python support requires launching an additional Docker container, making support on Kubernetes more complicated.</p><p data-type="footnote" id="idm45831176990920"><sup><a href="#idm45831176990920-marker">[9]</a></sup> While TFX automatically installs TFDV, if you have an old installation and you don’t specify <code>tensorflow-data-validation</code>, you may get an error of <code>Could not find a version that satisfies the requirement</code> so we illustrate explicitly installing both here.</p><p data-type="footnote" id="idm45831176824360"><sup><a href="#idm45831176824360-marker">[10]</a></sup> While technically not a file format, since TFX can accept Pandas dataframes, a common pattern is to load with Pandas first.</p><p data-type="footnote" id="idm45831175808232"><sup><a href="#idm45831175808232-marker">[11]</a></sup> There is no definitive list, although many vendors list their formats on  <a href="">this Spark page</a>.</p><p data-type="footnote" id="idm45831175711608"><sup><a href="#idm45831175711608-marker">[12]</a></sup> Of course, since most formats have slight variations, they have configuration options if the defaults don’t work.</p></div></div></section>
<section data-type="chapter" data-pdf-bookmark="Chapter 6. Artifact and Metadata Store"><div class="chapter" id="artifact_store" data-type="chapter" title2="Artifact and Metadata Store" no2="6">
<h1>Chapter 6. Artifact and Metadata Store</h1>


<p>Machine learning typically involves dealing with a large amount of raw and intermediate<a data-type="indexterm" data-primary="metadata" data-secondary="defined" id="idm45831174954840"></a><a data-type="indexterm" data-primary="data" data-secondary="metadata definition" data-seealso="metadata" id="idm45831174953864"></a> (transformed) data where the ultimate goal is creating and deploying the model.
In order to understand our model it is necessary to be able to explore datasets used for its <a data-type="indexterm" data-primary="data" data-secondary="data lineage" id="idm45831174952328"></a><a data-type="indexterm" data-primary="metadata" data-secondary="resource on" id="idm45831174951384"></a>creation and transformations (data lineage). The collection of these datasets and the transformation applied to them is called the <em>metadata of our model</em>.<sup><a data-type="noteref" id="idm45831174949784-marker" href="#idm45831174949784">[1]</a></sup></p>

<p>Model metadata is critical for <em>reproducibility</em> in machine<a data-type="indexterm" data-primary="metadata" data-secondary="reproducibility importance" id="idm45831174947352"></a><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="reproducibility importance" id="idm45831174946328"></a><a data-type="indexterm" data-primary="reproducibility in machine learning" id="idm45831174945368"></a><a data-type="indexterm" data-primary="model inference" data-secondary="deployment reproducibility importance" id="idm45831174944680"></a> learning;<sup><a data-type="noteref" id="idm45831174790872-marker" href="#idm45831174790872">[2]</a></sup> reproducibility is critical for reliable production deployments. Capturing the metadata allows us to understand variations when rerunning jobs or experiments. Understanding variations is necessary to iteratively develop and improve our models. It also provides a solid foundation for model comparisons. As Pete Warden defined it in this <a href="https://oreil.ly/dQZjL">post</a>:</p>
<blockquote>
<p>To reproduce results, code, training data, and the overall platform need to be recorded accurately.</p></blockquote>

<p>The same information is also required for other common ML operations—model comparison, reproducible model creation, etc.</p>

<p>There are many different options for tracking the metadata of models. Kubeflow has a built-in tool for this called<a data-type="indexterm" data-primary="metadata" data-secondary="tracking tool in Kubeflow" data-seealso="Kubeflow ML Metadata" id="idm45831174786760"></a><a data-type="indexterm" data-primary="Kubeflow ML Metadata" data-secondary="about" id="idm45831174785560"></a><a data-type="indexterm" data-primary="ML Metadata TensorFlow Extended (TFX)" id="idm45831174784616"></a><a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="ML Metadata" id="idm45831174783928"></a> <a href="https://oreil.ly/0rVN1">Kubeflow ML Metadata</a>.<sup><a data-type="noteref" id="idm45831174782152-marker" href="#idm45831174782152">[3]</a></sup>
The goal of this tool is to help Kubeflow users understand and manage their ML workflows by tracking and managing the metadata that the workflows produce.
Another tool for tracking metadata that we can integrate into our Kubeflow pipelines is MLflow Tracking.<a data-type="indexterm" data-primary="MLflow (Databricks)" data-secondary="metadata tools" data-tertiary="MLflow Tracking" id="idm45831174779688"></a>
It provides API and UI for logging parameters, code versions, metrics, and output files when running your machine learning code and for later visualizing the results.</p>

<p>In this chapter we will discuss the capabilities of Kubeflow’s ML Metadata project and show how it can be used. We will also consider some shortcomings of this implementation and explore usage of additional third-party <a data-type="indexterm" data-primary="Databricks MLflow" data-secondary="about" id="idm45831174777640"></a><a data-type="indexterm" data-primary="MLflow (Databricks)" data-secondary="about" id="idm45831174776664"></a>software: <a href="https://mlflow.org">MLflow</a>.<sup><a data-type="noteref" id="idm45831174774904-marker" href="#idm45831174774904">[4]</a></sup></p>






<section data-type="sect1" data-pdf-bookmark="Kubeflow ML Metadata"><div class="sect1" id="idm45831174773496" title2="Kubeflow ML Metadata" no2="6.1">
<h1>6.1. Kubeflow ML Metadata</h1>

<p><a href="https://oreil.ly/7WOp1">Kubeflow ML Metadata</a> is a library for recording and retrieving <a data-type="indexterm" data-primary="Kubeflow ML Metadata" data-secondary="about" id="idm45831174771208"></a><a data-type="indexterm" data-primary="Python" data-secondary="Kubeflow ML Metadata" id="idm45831174770232"></a><a data-type="indexterm" data-primary="Kubeflow ML Metadata" data-secondary="Python only APIs" id="idm45831174769288"></a>metadata associated with model creation. In the current implementation, Kubeflow Metadata provides only Python APIs. To use other languages, you need to implement the language-specific Python plug-in to be able to use the library. To understand how it works, we will start with a simple artificial example showing the basic capabilities of Kubeflow Metadata using a very simple notebook (based on this <a href="https://oreil.ly/Sm3ML">demo</a>).<sup><a data-type="noteref" id="idm45831174767144-marker" href="#idm45831174767144">[5]</a></sup></p>

<p>Implementation of Kubeflow Metadata starts with required imports,<a data-type="indexterm" data-primary="Kubeflow ML Metadata" data-secondary="required imports" id="idm45831174765448"></a> as shown in <a data-type="xref" href="#required_imports">EXAMPLE 6-1</a>.</p>
<div id="required_imports" data-type="example" title2="Required imports" no2="6-1">
<h5>Example 6-1. Required imports</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">kfmd</code> <code class="kn">import</code> <code class="n">metadata</code>
<code class="kn">import</code> <code class="nn">pandas</code>
<code class="kn">from</code> <code class="nn">datetime</code> <code class="kn">import</code> <code class="n">datetime</code></pre></div>

<p>In Kubeflow Metadata, all the information is organized in terms of a workspace, run, and execution.<a data-type="indexterm" data-primary="Kubeflow ML Metadata" data-secondary="information organization" id="idm45831174744424"></a><a data-type="indexterm" data-primary="Kubeflow ML Metadata" data-secondary="defining a workspace" id="idm45831174755256"></a> You need to define a workspace so Kubeflow can track and organize the records. The code in <a data-type="xref" href="#define_workspace">EXAMPLE 6-2</a> shows how this can be done.</p>
<div id="define_workspace" data-type="example" title2="Define a workspace" no2="6-2">
<h5>Example 6-2. Define a workspace</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">ws1</code> <code class="o">=</code> <code class="n">metadata</code><code class="o">.</code><code class="n">Workspace</code><code class="p">(</code>
    <code class="c1"># Connect to metadata-service in namespace kubeflow.</code>
    <code class="n">backend_url_prefix</code><code class="o">=</code><code class="s2">"metadata-service.kubeflow.svc.cluster.local:8080"</code><code class="p">,</code>
    <code class="n">name</code><code class="o">=</code><code class="s2">"ws1"</code><code class="p">,</code>
    <code class="n">description</code><code class="o">=</code><code class="s2">"a workspace for testing"</code><code class="p">,</code>
    <code class="n">labels</code><code class="o">=</code><code class="p">{</code><code class="s2">"n1"</code><code class="p">:</code> <code class="s2">"v1"</code><code class="p">})</code>
<code class="n">r</code> <code class="o">=</code> <code class="n">metadata</code><code class="o">.</code><code class="n">Run</code><code class="p">(</code>
    <code class="n">workspace</code><code class="o">=</code><code class="n">ws1</code><code class="p">,</code>
    <code class="n">name</code><code class="o">=</code><code class="s2">"run-"</code> <code class="o">+</code> <code class="n">datetime</code><code class="o">.</code><code class="n">utcnow</code><code class="p">()</code><code class="o">.</code><code class="n">isoformat</code><code class="p">(</code><code class="s2">"T"</code><code class="p">)</code> <code class="p">,</code>
    <code class="n">description</code><code class="o">=</code><code class="s2">"a run in ws_1"</code><code class="p">,</code>
<code class="p">)</code>
<code class="k">exec</code> <code class="o">=</code> <code class="n">metadata</code><code class="o">.</code><code class="n">Execution</code><code class="p">(</code>
    <code class="n">name</code> <code class="o">=</code> <code class="s2">"execution"</code> <code class="o">+</code> <code class="n">datetime</code><code class="o">.</code><code class="n">utcnow</code><code class="p">()</code><code class="o">.</code><code class="n">isoformat</code><code class="p">(</code><code class="s2">"T"</code><code class="p">)</code> <code class="p">,</code>
    <code class="n">workspace</code><code class="o">=</code><code class="n">ws1</code><code class="p">,</code>
    <code class="n">run</code><code class="o">=</code><code class="n">r</code><code class="p">,</code>
    <code class="n">description</code><code class="o">=</code><code class="s2">"execution example"</code><code class="p">,</code>
<code class="p">)</code></pre></div>
<div data-type="tip"><h6>Tip</h6>
<p>Workspace, run, and execution can be defined multiple times in the same or different applications. If they do not exist, they will be created; if they already exist, they will be used.</p>
</div>

<p>Kubeflow does not automatically track the datasets used by the application.<a data-type="indexterm" data-primary="Kubeflow ML Metadata" data-secondary="dataset tracking" id="idm45831174634920"></a><a data-type="indexterm" data-primary="MNIST (Modified National Institute of Standards and Technology)" data-secondary="data registration example" id="idm45831174634008"></a> They have to be explicitly registered in code. Following a classic MNIST example data sets registration in Metadata should be implemented as shown in <a data-type="xref" href="#metadata_example">EXAMPLE 6-3</a>.</p>
<div id="metadata_example" data-type="example" title2="Metadata example" no2="6-3">
<h5>Example 6-3. Metadata example</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">data_set</code> <code class="o">=</code> <code class="k">exec</code><code class="o">.</code><code class="n">log_input</code><code class="p">(</code>
        <code class="n">metadata</code><code class="o">.</code><code class="n">DataSet</code><code class="p">(</code>
            <code class="n">description</code><code class="o">=</code><code class="s2">"an example data"</code><code class="p">,</code>
            <code class="n">name</code><code class="o">=</code><code class="s2">"mytable-dump"</code><code class="p">,</code>
            <code class="n">owner</code><code class="o">=</code><code class="s2">"owner@my-company.org"</code><code class="p">,</code>
            <code class="n">uri</code><code class="o">=</code><code class="s2">"file://path/to/dataset"</code><code class="p">,</code>
            <code class="n">version</code><code class="o">=</code><code class="s2">"v1.0.0"</code><code class="p">,</code>
            <code class="n">query</code><code class="o">=</code><code class="s2">"SELECT * FROM mytable"</code><code class="p">))</code></pre></div>

<p>In addition to data, Kubeflow Metadata allows you to store information about your model and its metrics.<a data-type="indexterm" data-primary="Kubeflow ML Metadata" data-secondary="information about model and metrics" id="idm45831174625192"></a> The code implementing it is presented in <a data-type="xref" href="#metadata_example2">EXAMPLE 6-4</a>.</p>
<div id="metadata_example2" data-type="example" title2="Another metadata example" no2="6-4">
<h5>Example 6-4. Another metadata example</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="k">exec</code><code class="o">.</code><code class="n">log_output</code><code class="p">(</code>
    <code class="n">metadata</code><code class="o">.</code><code class="n">Model</code><code class="p">(</code>
            <code class="n">name</code><code class="o">=</code><code class="s2">"MNIST"</code><code class="p">,</code>
            <code class="n">description</code><code class="o">=</code><code class="s2">"model to recognize handwritten digits"</code><code class="p">,</code>
            <code class="n">owner</code><code class="o">=</code><code class="s2">"someone@kubeflow.org"</code><code class="p">,</code>
            <code class="n">uri</code><code class="o">=</code><code class="s2">"gcs://my-bucket/mnist"</code><code class="p">,</code>
            <code class="n">model_type</code><code class="o">=</code><code class="s2">"neural network"</code><code class="p">,</code>
            <code class="n">training_framework</code><code class="o">=</code><code class="p">{</code>
                <code class="s2">"name"</code><code class="p">:</code> <code class="s2">"tensorflow"</code><code class="p">,</code>
                <code class="s2">"version"</code><code class="p">:</code> <code class="s2">"v1.0"</code>
            <code class="p">},</code>
            <code class="n">hyperparameters</code><code class="o">=</code><code class="p">{</code>
                <code class="s2">"learning_rate"</code><code class="p">:</code> <code class="mf">0.5</code><code class="p">,</code>
                <code class="s2">"layers"</code><code class="p">:</code> <code class="p">[</code><code class="mi">10</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code>
                <code class="s2">"early_stop"</code><code class="p">:</code> <code class="bp">True</code>
            <code class="p">},</code>
            <code class="n">version</code><code class="o">=</code><code class="s2">"v0.0.1"</code><code class="p">,</code>
            <code class="n">labels</code><code class="o">=</code><code class="p">{</code><code class="s2">"mylabel"</code><code class="p">:</code> <code class="s2">"l1"</code><code class="p">}))</code>
<code class="n">metrics</code> <code class="o">=</code> <code class="k">exec</code><code class="o">.</code><code class="n">log_output</code><code class="p">(</code>
    <code class="n">metadata</code><code class="o">.</code><code class="n">Metrics</code><code class="p">(</code>
            <code class="n">name</code><code class="o">=</code><code class="s2">"MNIST-evaluation"</code><code class="p">,</code>
            <code class="n">description</code><code class="o">=</code><code class="s2">"validating the MNIST model to recognize handwritten digits"</code><code class="p">,</code>
            <code class="n">owner</code><code class="o">=</code><code class="s2">"someone@kubeflow.org"</code><code class="p">,</code>
            <code class="n">uri</code><code class="o">=</code><code class="s2">"gcs://my-bucket/mnist-eval.csv"</code><code class="p">,</code>
            <code class="n">data_set_id</code><code class="o">=</code><code class="n">data_set</code><code class="o">.</code><code class="n">id</code><code class="p">,</code>
            <code class="n">model_id</code><code class="o">=</code><code class="n">model</code><code class="o">.</code><code class="n">id</code><code class="p">,</code>
            <code class="n">metrics_type</code><code class="o">=</code><code class="n">metadata</code><code class="o">.</code><code class="n">Metrics</code><code class="o">.</code><code class="n">VALIDATION</code><code class="p">,</code>
            <code class="n">values</code><code class="o">=</code><code class="p">{</code><code class="s2">"accuracy"</code><code class="p">:</code> <code class="mf">0.95</code><code class="p">},</code>
            <code class="n">labels</code><code class="o">=</code><code class="p">{</code><code class="s2">"mylabel"</code><code class="p">:</code> <code class="s2">"l1"</code><code class="p">}))</code></pre></div>

<p>These code snippets will implement all of the main steps for storing model creation metadata:<a data-type="indexterm" data-primary="metadata" data-secondary="about storing model creation metadata" id="idm45831174604408"></a></p>
<ol>
<li>
<p>Define workspace, run, and execution.</p>
</li>
<li>
<p>Store information about data assets used for model creation.</p>
</li>
<li>
<p>Store information about the created model, including its version, type, training framework, and hyperparameters used for its creation.</p>
</li>
<li>
<p>Store information about model evaluation metrics.</p>
</li>

</ol>

<p>In real-world implementations these snippets should be used in the actual code to capture metadata used for data preparation, machine learning, etc. See <a data-type="xref" href="#tf_ch">CHAPTER 7</a> for examples of where and how this information is captured.</p>

<p>Collecting metadata is useful only if there are ways to view it. Kubeflow Metadata provides two ways of viewing it—programmatically, and using Metadata UI.</p>








<section data-type="sect2" data-pdf-bookmark="Programmatic Query"><div class="sect2" id="idm45831174347464" title2="Programmatic Query" no2="6.1.1">
<h2>6.1.1. Programmatic Query</h2>

<p>The following functionality is available for programmatic query.<a data-type="indexterm" data-primary="metadata" data-secondary="viewing" data-tertiary="programmatic query" id="ch06-pm"></a></p>

<p>First, we list all the models in the workspace, as shown in <a data-type="xref" href="#list_models">EXAMPLE 6-5</a>.</p>
<div id="list_models" data-type="example" title2="List all models" no2="6-5">
<h5>Example 6-5. List all models</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">pandas</code><code class="o">.</code><code class="n">DataFrame</code><code class="o">.</code><code class="n">from_dict</code><code class="p">(</code><code class="n">ws1</code><code class="o">.</code><code class="n">list</code><code class="p">(</code><code class="n">metadata</code><code class="o">.</code><code class="n">Model</code><code class="o">.</code><code class="n">ARTIFACT_TYPE_NAME</code><code class="p">))</code></pre></div>

<p class="less_space pagebreak-before">In our code we created only a single model, which is returned as a result of this query (see <a data-type="xref" href="#query_result">TABLE 6-1</a>).</p>
<table id="query_result" data-type="table" title2="List of models" no2="6-1">
<caption>Table 6-1. List of models</caption>
<thead>
<tr>
<th>&nbsp;</th>
<th>id</th>
<th>workspace</th>
<th>run</th>
<th>create_time</th>
<th>description</th>
<th>model_type</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>0</p></td>
<td><p>2</p></td>
<td><p>ws1</p></td>
<td><p>run-2020-01-10T22:13:20.959882</p></td>
<td><p>2020-01-10T22:13:26.324443Z</p></td>
<td><p>model to recognize handwritten digits</p></td>
<td><p>neural network</p></td>
</tr>
</tbody>
</table>
<table data-type="table" id="untitled_table_1" title2="(no caption)" no2="">

<thead>
<tr>
<th>name</th>
<th>owner</th>
<th>version</th>
<th>uri</th>
<th>training_framework</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>MNIST</p></td>
<td><p>someone@kubeflow.org</p></td>
<td><p>v0.0.1</p></td>
<td><p>gcs://my-bucket/mnist</p></td>
<td><p>{<em>name</em>: <em>tensorflow</em>, <em>version</em>: <em>v1.0</em>}</p></td>
</tr>
</tbody>
</table>

<p>Next, we get basic lineage (see <a data-type="xref" href="#basic_lineage">EXAMPLE 6-6</a>). In our case we created a single model, so the returned lineage will contain only the ID of this model.</p>
<div id="basic_lineage" data-type="example" title2="Basic lineage" no2="6-6">
<h5>Example 6-6. Basic lineage</h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">print</code><code class="p">(</code><code class="s2">"</code><code class="s2">model id is </code><code class="s2">"</code><code> </code><code class="o">+</code><code> </code><code class="n">model</code><code class="o">.</code><code class="n">id</code><code class="p">)</code><code> </code><a class="co" id="co_artifact_and_metadata_store_CO1-1" href="#callout_artifact_and_metadata_store_CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_artifact_and_metadata_store_CO1-1" href="#co_artifact_and_metadata_store_CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>Returns <code>model id is 2</code>.</p></dd>
</dl></div>

<p>Then we find the execution that produces this model. In our toy application we created a single execution. An ID of this execution is returned as a result of this query, as shown in <a data-type="xref" href="#find_execution">EXAMPLE 6-7</a>.</p>
<div id="find_execution" data-type="example" title2="Find the execution" no2="6-7">
<h5>Example 6-7. Find the execution</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">output_events</code><code> </code><code class="o">=</code><code> </code><code class="n">ws1</code><code class="o">.</code><code class="n">client</code><code class="o">.</code><code class="n">list_events2</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">id</code><code class="p">)</code><code class="o">.</code><code class="n">events</code><code>
</code><code class="n">execution_id</code><code> </code><code class="o">=</code><code> </code><code class="n">output_events</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">execution_id</code><code>
</code><code class="k">print</code><code class="p">(</code><code class="n">execution_id</code><code class="p">)</code><code> </code><a class="co" id="co_artifact_and_metadata_store_CO2-1" href="#callout_artifact_and_metadata_store_CO2-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_artifact_and_metadata_store_CO2-1" href="#co_artifact_and_metadata_store_CO2-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>Returns <code>1</code>.</p></dd>
</dl></div>

<p>Finally, we find all events related to that execution, as illustrated in <a data-type="xref" href="#getting_events">EXAMPLE 6-8</a>.</p>
<div id="getting_events" data-type="example" title2="Getting all related events" no2="6-8">
<h5>Example 6-8. Getting all related events</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">all_events</code> <code class="o">=</code> <code class="n">ws1</code><code class="o">.</code><code class="n">client</code><code class="o">.</code><code class="n">list_events</code><code class="p">(</code><code class="n">execution_id</code><code class="p">)</code><code class="o">.</code><code class="n">events</code>
<code class="k">assert</code> <code class="nb">len</code><code class="p">(</code><code class="n">all_events</code><code class="p">)</code> <code class="o">==</code> <code class="mi">3</code>
<code class="k">print</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">All events related to this model:"</code><code class="p">)</code>
<code class="n">pandas</code><code class="o">.</code><code class="n">DataFrame</code><code class="o">.</code><code class="n">from_dict</code><code class="p">([</code><code class="n">e</code><code class="o">.</code><code class="n">to_dict</code><code class="p">()</code> <code class="k">for</code> <code class="n">e</code> <code class="ow">in</code> <code class="n">all_events</code><code class="p">])</code></pre></div>

<p class="less_space pagebreak-before">In our case we used a single input that was used to create a model and metrics. So the result of this query looks as shown in <a data-type="xref" href="#query_table">TABLE 6-2</a>.<a data-type="indexterm" data-startref="ch06-pm" id="idm45831174124232"></a></p>
<table id="query_table" data-type="table" title2="Query result as a table" no2="6-2">
<caption>Table 6-2. Query result as a table</caption>
<thead>
<tr>
<th></th>
<th>artifact_id</th>
<th>execution_id</th>
<th>path</th>
<th>type</th>
<th>milliseconds_since_epoch</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>1</p></td>
<td><p>None</p></td>
<td><p>INPUT</p></td>
<td><p>1578694406318</p></td>
</tr>
<tr>
<td><p>1</p></td>
<td><p>2</p></td>
<td><p>1</p></td>
<td><p>None</p></td>
<td><p>OUTPUT</p></td>
<td><p>1578694406338</p></td>
</tr>
<tr>
<td><p>2</p></td>
<td><p>3</p></td>
<td><p>1</p></td>
<td><p>None</p></td>
<td><p>OUTPUT</p></td>
<td><p>1578694406358</p></td>
</tr>
</tbody>
</table>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Kubeflow Metadata UI"><div class="sect2" id="idm45831174346840" title2="Kubeflow Metadata UI" no2="6.1.2">
<h2>6.1.2. Kubeflow Metadata UI</h2>

<p>In addition to providing APIs for writing code to analyze metadata, the <a data-type="indexterm" data-primary="metadata" data-secondary="viewing" data-tertiary="Metadata UI" id="idm45831174105016"></a><a data-type="indexterm" data-primary="user interfaces (UI)" data-secondary="Metadata UI" id="idm45831174103768"></a>Kubeflow Metadata tool provides a UI, which allows you to view metadata without writing code. Access to the Metadata UI is done through the main Kubeflow UI, as seen in <a data-type="xref" href="#metadata_ui">FIGURE 6-1</a>.</p>

<figure><div id="metadata_ui" class="figure" data-type="figure" title2="Accessing Metadata UI" no2="6-1">
<img src="assets/kfml_0603.png" alt="Metadata UI Screenshot" width="1440" height="1056">
<h6>Figure 6-1. Accessing Metadata UI</h6>
</div></figure>

<p class="less_space pagebreak-before">Once you click the <a data-type="indexterm" data-primary="metadata" data-secondary="artifact store and" id="idm45831174099048"></a>Artifact Store, you should see the list of available <a data-type="indexterm" data-primary="artifact store, for logged metadata events" id="idm45831174097912"></a>artifacts (logged metadata events), as in <a data-type="xref" href="#artifact_store_ui">FIGURE 6-2</a>.</p>

<figure><div id="artifact_store_ui" class="figure" data-type="figure" title2="List of artifacts in the Artifact Store UI" no2="6-2">
<img src="assets/kfml_0604.png" alt="Artifact Store UI Screenshot" width="1440" height="680">
<h6>Figure 6-2. List of artifacts in the Artifact Store UI</h6>
</div></figure>

<p>From this view we can click the individual artifact and see its details, as shown in <a data-type="xref" href="#artifact_view">FIGURE 6-3</a>.</p>

<figure><div id="artifact_view" class="figure" data-type="figure" title2="Artifact view" no2="6-3">
<img src="assets/kfml_0605.png" alt="Artifact View Screenshot" width="1440" height="555">
<h6>Figure 6-3. Artifact view</h6>
</div></figure>

<p>Kubeflow Metadata provides some basic capabilities for storing and<a data-type="indexterm" data-primary="metadata" data-secondary="Kubeflow Metadata" data-see="Kubeflow ML Metadata" id="idm45831174090728"></a><a data-type="indexterm" data-primary="Kubeflow ML Metadata" data-secondary="limitations of" id="idm45831174089480"></a> viewing of machine learning metadata; however, its capabilities are extremely limited, especially in terms of viewing and manipulating stored metadata. A more powerful implementation of machine learning metadata management is done by MLflow. Though MLflow isn’t part of Kubeflow distribution, it’s very easy to deploy it alongside Kubeflow and use it from Kubeflow-based applications, as described in the next section.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Using MLflow’s Metadata Tools with Kubeflow"><div class="sect1" id="idm45831174772872" title2="Using MLflow’s Metadata Tools with Kubeflow" no2="6.2">
<h1>6.2. Using MLflow’s Metadata Tools with Kubeflow</h1>

<p>MLflow is an open source platform for managing the end-to-end machine learning life cycle. It includes three primary functions:<a data-type="indexterm" data-primary="MLflow (Databricks)" data-secondary="metadata tools" data-tertiary="about" id="idm45831174086168"></a><a data-type="indexterm" data-primary="Databricks MLflow" data-secondary="metadata tools" data-tertiary="about" id="idm45831174084920"></a></p>
<dl>
<dt><a href="https://oreil.ly/appTI">MLflow Tracking</a></dt>
<dd>
<p>Tracking experiments to record and compare parameters and results</p>
</dd>
<dt><a href="https://oreil.ly/YmBmS">MLflow Projects</a></dt>
<dd>
<p>Packaging ML code in a reusable, reproducible form in order to share with other data scientists or transfer to production</p>
</dd>
<dt><a href="https://oreil.ly/WN0nm">MLflow Models</a></dt>
<dd>
<p>Managing and deploying models from a variety of ML libraries to a variety of model serving and inference platforms</p>
</dd>
</dl>

<p>For the purposes of our Kubeflow metadata discussion we will only discuss deployment and usage of MLflow tracking components—an API and UI for logging parameters, code versions, metrics, and output files when running your machine learning code and for visualizing the results. MLflow Tracking lets you log and query experiments using <a href="https://oreil.ly/BfpRU">Python</a>, <a href="https://oreil.ly/pGC0j">REST</a>, <a href="https://oreil.ly/5xKAK">R</a>, and <a href="https://oreil.ly/AUUXL">Java</a> APIs, which significantly extends the reach of APIs, allowing you to store and access metadata from different ML 
components.</p>

<p>MLflow Tracking is organized around the concept of runs, which are executions<a data-type="indexterm" data-primary="MLflow (Databricks)" data-secondary="metadata tools" data-tertiary="MLflow Tracking" id="idm45831174042392"></a><a data-type="indexterm" data-primary="Databricks MLflow" data-secondary="metadata tools" data-tertiary="MLflow Tracking" id="idm45831174041144"></a> of some piece of data science code. Each run records the following information:</p>
<dl>
<dt>Code version</dt>
<dd>
<p>Git commit hash used for the run, if it was run from an MLflow Project</p>
</dd>
<dt>Start and end time</dt>
<dd>
<p>Start and end time of the run</p>
</dd>
<dt>Source</dt>
<dd>
<p>Name of the file to launch the run, or the project name and entry point for the run if run from an MLflow Project</p>
</dd>
<dt>Parameters</dt>
<dd>
<p>Key-value input parameters of your choice. Both keys and values are strings.</p>
</dd>
<dt>Metrics</dt>
<dd>
<p>Key-value metrics, where the value is numeric. Each metric can be updated throughout the course of the run (for example, to track how your model’s loss function is converging), and MLflow records and lets you visualize the metric’s full history.</p>
</dd>
<dt>Artifacts</dt>
<dd>
<p>Output files in any format. Here you can record images (such as PNG files), models (for example, a pickled Scikit-learn model), and data files (for example, a Parquet file) as artifacts.</p>
</dd>
</dl>

<p>Most of the <a href="https://oreil.ly/oik7f">MLflow examples</a> use local MLflow installations, which is not appropriate for our purposes. For our implementation we need a cluster-based installation, allowing us to write metadata from different Docker instances and view them centrally. Following the approach outlined in the project<a data-type="indexterm" data-primary="MLflow (Databricks)" data-secondary="metadata tools" data-tertiary="MLflow Tracking Server" id="ch06-trk"></a><a data-type="indexterm" data-primary="Databricks MLflow" data-secondary="metadata tools" data-tertiary="MLflow Tracking Server" id="ch06-trk2"></a> <a href="https://oreil.ly/5DdRC">MLflow Tracking Server based on Docker and AWS S3</a>, the overall architecture of such MLflow Tracking component deployment is presented in <a data-type="xref" href="#mlflow_arch">FIGURE 6-4</a>.</p>

<figure><div id="mlflow_arch" class="figure" data-type="figure" title2="Overall architecture of MLflow components deployment" no2="6-4">
<img src="assets/kfml_0606.png" alt="Overall architecture of MLflow components deployment" width="1396" height="664">
<h6>Figure 6-4. Overall architecture of MLflow components deployment</h6>
</div></figure>

<p>The main components of this architecture are:</p>

<ul>
<li>
<p>MinIO server, already part of the Kubeflow installation</p>
</li>
<li>
<p>MLflow tracking server—the MLflow UI component—an additional component that needs to be added to Kubeflow installation to support MLflow usage</p>
</li>
<li>
<p>User code such as notebook, Python, R, or Java application</p>
</li>
</ul>








<section data-type="sect2" data-pdf-bookmark="Creating and Deploying an MLflow Tracking Server"><div class="sect2" id="idm45831174018360" title2="Creating and Deploying an MLflow Tracking Server" no2="6.2.1">
<h2>6.2.1. Creating and Deploying an MLflow Tracking Server</h2>

<p>MLflow Tracking Server allows you to record MLflow runs to local files, to a SQLAlchemy-compatible database, or remotely to a tracking server. In our implementation we are using a remote server.</p>

<p>An MLflow Tracking Server has two components for storage: a backend store and an artifact store. The backend store is where MLflow Tracking Server stores experiment and run metadata as well as parameters, metrics, and tags for runs. MLflow supports two types of backend stores: file store and database-backed store. For simplicity we will be using a file store. In our deployment, this file store is part of the Docker image, which means that this data is lost in the case of server restart. If you need longer-term storage, you can either use an external filesystem, like NFS server, or a database.
The artifact store is a location suitable for large data (such as an S3 bucket or shared NFS filesystem) and is where clients log their artifact output (for example, models). To make our deployment cloud independent, we decided to use MinIO (part of Kubeflow) as an artifact store.
Based on these decisions, a Docker file for building the MLflow Tracking Server looks like <a data-type="xref" href="#mlflow_docker">EXAMPLE 6-9</a> (similar to the implementation in <a href="https://oreil.ly/VOe9f">this GitHub repo</a>).</p>
<div id="mlflow_docker" data-type="example" title2="MLflow Tracking Server" no2="6-9">
<h5>Example 6-9. MLflow Tracking Server</h5>

<pre data-type="programlisting" data-code-language="docker"><code class="k">FROM</code><code class="s"> python:3.7</code>

<code class="k">RUN</code> pip3 install --upgrade pip <code class="o">&amp;&amp;</code> <code class="se">\</code>
   pip3 install mlflow --upgrade <code class="o">&amp;&amp;</code> <code class="se">\</code>
   pip3 install awscli --upgrade  <code class="o">&amp;&amp;</code> <code class="se">\</code>
   pip3 install boto3 --upgrade

<code class="k">ENV</code><code class="s"> PORT 5000</code>
<code class="k">ENV</code><code class="s"> AWS_BUCKET bucket</code>
<code class="k">ENV</code><code class="s"> AWS_ACCESS_KEY_ID aws_id</code>
<code class="k">ENV</code><code class="s"> AWS_SECRET_ACCESS_KEY aws_key</code>
<code class="k">ENV</code><code class="s"> FILE_DIR /tmp/mlflow</code>

<code class="k">RUN</code> mkdir -p /opt/mlflow
COPY run.sh /opt/mlflow
<code class="k">RUN</code> chmod -R <code class="m">777</code> /opt/mlflow/

<code class="k">ENTRYPOINT</code><code class="s"> ["/opt/mlflow/run.sh"]</code></pre></div>

<p>Here we first load MLflow code (using pip), set environment variables, and then copy and run the startup script. The start-up script used here looks like <a data-type="xref" href="#mlflow_startup">EXAMPLE 6-10</a>.<sup><a data-type="noteref" id="idm45831173984648-marker" href="#idm45831173984648">[6]</a></sup></p>
<div id="mlflow_startup" data-type="example" title2="MLflow startup script" no2="6-10">
<h5>Example 6-10. MLflow startup script</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="c">#!/bin/sh</code>
mkdir -p <code class="nv">$FILE_DIR</code>

mlflow server <code class="se">\</code>
   --backend-store-uri file://<code class="nv">$FILE_DIR</code> <code class="se">\</code>
   --default-artifact-root s3://<code class="nv">$AWS_BUCKET</code>/mlflow/artifacts <code class="se">\</code>
   --host 0.0.0.0 <code class="se">\</code>
   --port <code class="nv">$PORT</code></pre></div>

<p>This script sets an environment and then verifies that all required environment variables are set. Once validation succeeds, an MLflow server is started.
Once the Docker is created, the Helm command in <a data-type="xref" href="#install_required1">EXAMPLE 6-11</a> (the Helm chart is located on <a href="https://oreil.ly/Kubeflow_for_ML_ch06_mlflowchart">this book’s GitHub repo</a>) can be used to install the server.</p>
<div id="install_required1" data-type="example" title2="Installing MLflow server with Helm" no2="6-11">
<h5>Example 6-11. Installing MLflow server with Helm</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">helm</code> <code class="n">install</code> <code class="o">&lt;</code><code class="n">location</code> <code class="n">of</code> <code class="n">the</code> <code class="n">Helm</code> <code class="n">chart</code><code class="o">&gt;</code></pre></div>

<p>This Helm chart installs three main components implementing the MLflow Tracking Server:</p>
<dl>
<dt><a href="https://oreil.ly/GYQwv">Deployment</a></dt>
<dd>
<p>Deploying MLflow server itself (single replica). The important parameters here are the environment, including MinIO endpoint, credentials, and bucket used for artifact storage.</p>
</dd>
<dt><a href="https://oreil.ly/RZq4U">Service</a></dt>
<dd>
<p>Creating a Kubernetes service exposing MLflow deployment</p>
</dd>
<dt><a href="https://oreil.ly/-myuN">Virtual service</a></dt>
<dd>
<p>Exposing MLflow service to users through the Istio ingress gateway used by Kubeflow</p>
</dd>
</dl>

<p>Once the server is deployed, we can get access to the UI, but at this point it will say that there are no available experiments. Let’s now look at how this server can be used to capture<a data-type="indexterm" data-startref="ch06-trk" id="idm45831173894968"></a><a data-type="indexterm" data-startref="ch06-trk2" id="idm45831173894264"></a> metadata.<sup><a data-type="noteref" id="idm45831173893464-marker" href="#idm45831173893464">[7]</a></sup></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Logging Data on Runs"><div class="sect2" id="idm45831174017736" title2="Logging Data on Runs" no2="6.2.2">
<h2>6.2.2. Logging Data on Runs</h2>

<p>As an example of logging data, let’s look at some simple<a data-type="indexterm" data-primary="MLflow (Databricks)" data-secondary="metadata tools" data-tertiary="logging data on runs" id="idm45831173890408"></a><a data-type="indexterm" data-primary="Databricks MLflow" data-secondary="metadata tools" data-tertiary="logging data on runs" id="idm45831173889160"></a> code.<sup><a data-type="noteref" id="idm45831173887816-marker" href="#idm45831173887816">[8]</a></sup> We will start by installing required packages, shown in Examples <a data-type="xref" data-xrefstyle="select:labelnumber" href="#install_required1">EXAMPLE 6-11</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="#install_required2">EXAMPLE 6-12</a>.</p>
<div id="install_required2" data-type="example" title2="Install required" no2="6-12">
<h5>Example 6-12. Install required</h5>

<pre data-type="programlisting" data-code-language="python"><code class="err">!</code><code class="n">pip</code><code> </code><code class="n">install</code><code> </code><code class="n">pandas</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">upgrade</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">user</code><code>
</code><code class="err">!</code><code class="n">pip</code><code> </code><code class="n">install</code><code> </code><code class="n">mlflow</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">upgrade</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">user</code><code> </code><a class="co" id="co_artifact_and_metadata_store_CO3-1" href="#callout_artifact_and_metadata_store_CO3-1"><img src="assets/1.png" alt="1" width="12" height="12"></a><code>
</code><code class="err">!</code><code class="n">pip</code><code> </code><code class="n">install</code><code> </code><code class="n">joblib</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">upgrade</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">user</code><code>
</code><code class="err">!</code><code class="n">pip</code><code> </code><code class="n">install</code><code> </code><code class="n">numpy</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">upgrade</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">user</code><code>
</code><code class="err">!</code><code class="n">pip</code><code> </code><code class="n">install</code><code> </code><code class="n">scipy</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">upgrade</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">user</code><code>
</code><code class="err">!</code><code class="n">pip</code><code> </code><code class="n">install</code><code> </code><code class="n">scikit</code><code class="o">-</code><code class="n">learn</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">upgrade</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">user</code><code>
</code><code class="err">!</code><code class="n">pip</code><code> </code><code class="n">install</code><code> </code><code class="n">boto3</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">upgrade</code><code> </code><code class="o">-</code><code class="o">-</code><code class="n">user</code><code> </code><a class="co" id="co_artifact_and_metadata_store_CO3-2" href="#callout_artifact_and_metadata_store_CO3-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_artifact_and_metadata_store_CO3-1" href="#co_artifact_and_metadata_store_CO3-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>Here <code>mlflow</code> and <code>boto3</code> are the packages required for metadata logging, while the rest are used for machine learning itself.</p></dd>
</dl></div>

<p>Once these packages are installed, we can define required imports, as shown in <a data-type="xref" href="#import_required">EXAMPLE 6-13</a>.</p>
<div id="import_required" data-type="example" title2="Import required libraries" no2="6-13">
<h5>Example 6-13. Import required libraries</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">time</code>
<code class="kn">import</code> <code class="nn">json</code>
<code class="kn">import</code> <code class="nn">os</code>
<code class="kn">from</code> <code class="nn">joblib</code> <code class="kn">import</code> <code class="n">Parallel</code><code class="p">,</code> <code class="n">delayed</code>

<code class="kn">import</code> <code class="nn">pandas</code> <code class="kn">as</code> <code class="nn">pd</code>
<code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>
<code class="kn">import</code> <code class="nn">scipy</code>

<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code><code class="p">,</code> <code class="n">KFold</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">mean_squared_error</code><code class="p">,</code> <code class="n">mean_absolute_error</code>
<code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">r2_score</code><code class="p">,</code> <code class="n">explained_variance_score</code>
<code class="kn">from</code> <code class="nn">sklearn.exceptions</code> <code class="kn">import</code> <code class="n">ConvergenceWarning</code>

<code class="kn">import</code> <code class="nn">mlflow</code>
<code class="kn">import</code> <code class="nn">mlflow.sklearn</code>
<code class="kn">from</code> <code class="nn">mlflow.tracking</code> <code class="kn">import</code> <code class="n">MlflowClient</code>

<code class="kn">from</code> <code class="nn">warnings</code> <code class="kn">import</code> <code class="n">simplefilter</code>
<code class="n">simplefilter</code><code class="p">(</code><code class="n">action</code><code class="o">=</code><code class="s1">'ignore'</code><code class="p">,</code> <code class="n">category</code> <code class="o">=</code> <code class="ne">FutureWarning</code><code class="p">)</code>
<code class="n">simplefilter</code><code class="p">(</code><code class="n">action</code><code class="o">=</code><code class="s1">'ignore'</code><code class="p">,</code> <code class="n">category</code> <code class="o">=</code> <code class="n">ConvergenceWarning</code><code class="p">)</code></pre></div>

<p>Here again, <code>os</code> and the last three imports are required for MLflow logging, while the rest are used for machine learning. Now we need to define the environment variables (see <a data-type="xref" href="#set_env">EXAMPLE 6-14</a>) required for proper access to the MinIO server for storing 
artifacts.</p>
<div id="set_env" data-type="example" title2="Set environment variables" no2="6-14">
<h5>Example 6-14. Set environment variables</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="p">[</code><code class="s1">'MLFLOW_S3_ENDPOINT_URL'</code><code class="p">]</code> <code class="o">=</code> \
     <code class="s1">'http://minio-service.kubeflow.svc.cluster.local:9000'</code>
<code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="p">[</code><code class="s1">'AWS_ACCESS_KEY_ID'</code><code class="p">]</code> <code class="o">=</code> <code class="s1">'minio'</code>
<code class="n">os</code><code class="o">.</code><code class="n">environ</code><code class="p">[</code><code class="s1">'AWS_SECRET_ACCESS_KEY'</code><code class="p">]</code> <code class="o">=</code> <code class="s1">'minio123'</code></pre></div>

<p>Note here that in addition to the tracking server itself, <code>MLFLOW_S3_ENDPOINT_URL</code> is defined not only in the tracking server definition, but also in the code that actually captures the metadata. This is because, as we mentioned previously, user code writes to the artifact store directly, bypassing the server.</p>

<p>Here we skip the majority of the code (the full code can be found on <a href="https://oreil.ly/Kubeflow_for_ML_ch06_MLflow">this book’s GitHub repo</a>) and concentrate only on the parts related to the MLflow logging. The next step (see <a data-type="xref" href="#create_experiment">EXAMPLE 6-15</a>) is connecting to the tracking server and creating an 
experiment.</p>
<div id="create_experiment" data-type="example" title2="Create experiment" no2="6-15">
<h5>Example 6-15. Create experiment</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">remote_server_uri</code> <code class="o">=</code> <code class="s2">"http://mlflowserver.kubeflow.svc.cluster.local:5000"</code>
<code class="n">mlflow</code><code class="o">.</code><code class="n">set_tracking_uri</code><code class="p">(</code><code class="n">remote_server_uri</code><code class="p">)</code>
<code class="n">experiment_name</code> <code class="o">=</code> <code class="s2">"electricityconsumption-forecast"</code>
<code class="n">mlflow</code><code class="o">.</code><code class="n">set_experiment</code><code class="p">(</code><code class="n">experiment_name</code><code class="p">)</code></pre></div>

<p>Once connected to the server and creating (choosing) an experiment, we can start logging data. As an example, let’s look at the code for storing
<a href="https://oreil.ly/g5yH3">KNN regressor</a> information, in <a data-type="xref" href="#knn_model">EXAMPLE 6-16</a>.</p>
<div id="knn_model" data-type="example" title2="Sample KNN model" no2="6-16">
<h5>Example 6-16. Sample KNN model</h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">train_knnmodel</code><code class="p">(</code><code class="n">parameters</code><code class="p">,</code> <code class="n">inputs</code><code class="p">,</code> <code class="n">tags</code><code class="p">,</code> <code class="n">log</code> <code class="o">=</code> <code class="bp">False</code><code class="p">):</code>
    <code class="k">with</code> <code class="n">mlflow</code><code class="o">.</code><code class="n">start_run</code><code class="p">(</code><code class="n">nested</code> <code class="o">=</code> <code class="bp">True</code><code class="p">):</code>

<code class="err">………………………………………………</code><code class="o">.</code>
        <code class="c1"># Build the model</code>
        <code class="n">tic</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>
        <code class="n">model</code> <code class="o">=</code> <code class="n">KNeighborsRegressor</code><code class="p">(</code><code class="n">parameters</code><code class="p">[</code><code class="s2">"nbr_neighbors"</code><code class="p">],</code>
                                <code class="n">weights</code> <code class="o">=</code> <code class="n">parameters</code><code class="p">[</code><code class="s2">"weight_method"</code><code class="p">])</code>
        <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">array_inputs_train</code><code class="p">,</code> <code class="n">array_output_train</code><code class="p">)</code>
        <code class="n">duration_training</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code> <code class="o">-</code> <code class="n">tic</code>

        <code class="c1"># Make the prediction</code>
        <code class="n">tic1</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code>
        <code class="n">prediction</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">array_inputs_test</code><code class="p">)</code>
        <code class="n">duration_prediction</code> <code class="o">=</code> <code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">()</code> <code class="o">-</code> <code class="n">tic1</code>

        <code class="c1"># Evaluate the model prediction</code>
        <code class="n">metrics</code> <code class="o">=</code> <code class="n">evaluation_model</code><code class="p">(</code><code class="n">array_output_test</code><code class="p">,</code> <code class="n">prediction</code><code class="p">)</code>

        <code class="c1"># Log in mlflow (parameter)</code>
        <code class="n">mlflow</code><code class="o">.</code><code class="n">log_params</code><code class="p">(</code><code class="n">parameters</code><code class="p">)</code>

        <code class="c1"># Log in mlflow (metrics)</code>
        <code class="n">metrics</code><code class="p">[</code><code class="s2">"duration_training"</code><code class="p">]</code> <code class="o">=</code> <code class="n">duration_training</code>
        <code class="n">metrics</code><code class="p">[</code><code class="s2">"duration_prediction"</code><code class="p">]</code> <code class="o">=</code> <code class="n">duration_prediction</code>
        <code class="n">mlflow</code><code class="o">.</code><code class="n">log_metrics</code><code class="p">(</code><code class="n">metrics</code><code class="p">)</code>

        <code class="c1"># Log in mlflow (model)</code>
        <code class="n">mlflow</code><code class="o">.</code><code class="n">sklearn</code><code class="o">.</code><code class="n">log_model</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">f</code><code class="s2">"model"</code><code class="p">)</code>

        <code class="c1"># Save model</code>
        <code class="c1">#mlflow.sklearn.save_model(model,</code>
                         <code class="n">f</code><code class="s2">"mlruns/1/{uri}/artifacts/model/sklearnmodel"</code><code class="p">)</code>

        <code class="c1"># Tag the model</code>
        <code class="n">mlflow</code><code class="o">.</code><code class="n">set_tags</code><code class="p">(</code><code class="n">tags</code><code class="p">)</code></pre></div>

<p>In this code snippet, we can see how different kinds of data about model creation and prediction test statistics are logged. The information here is very similar to the information captured by Kubeflow Metadata and includes inputs, models, and metrics.</p>

<p>Finally, similar to Kubeflow Metadata, MLflow allows you to access this metadata programmatically. The main APIs provided by MLflow include what you see in <a data-type="xref" href="#getting_runs">EXAMPLE 6-17</a>.</p>
<div id="getting_runs" data-type="example" title2="Getting the runs for a given experiment" no2="6-17">
<h5>Example 6-17. Getting the runs for a given experiment</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">df_runs</code><code> </code><code class="o">=</code><code> </code><code class="n">mlflow</code><code class="o">.</code><code class="n">search_runs</code><code class="p">(</code><code class="n">experiment_ids</code><code class="o">=</code><code class="s2">"</code><code class="s2">0</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" id="co_artifact_and_metadata_store_CO4-1" href="#callout_artifact_and_metadata_store_CO4-1"><img src="assets/1.png" alt="1" width="12" height="12"></a><code>
</code><code class="k">print</code><code class="p">(</code><code class="s2">"</code><code class="s2">Number of runs done : </code><code class="s2">"</code><code class="p">,</code><code> </code><code class="nb">len</code><code class="p">(</code><code class="n">df_runs</code><code class="p">)</code><code class="p">)</code><code>
</code><code>
</code><code class="n">df_runs</code><code class="o">.</code><code class="n">sort_values</code><code class="p">(</code><code class="p">[</code><code class="s2">"</code><code class="s2">metrics.rmse</code><code class="s2">"</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">ascending</code><code> </code><code class="o">=</code><code> </code><code class="bp">True</code><code class="p">,</code><code> </code><code class="n">inplace</code><code> </code><code class="o">=</code><code> </code><code class="bp">True</code><code class="p">)</code><code> </code><a class="co" id="co_artifact_and_metadata_store_CO4-2" href="#callout_artifact_and_metadata_store_CO4-2"><img src="assets/2.png" alt="2" width="12" height="12"></a><code>
</code><code class="n">df_runs</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_artifact_and_metadata_store_CO4-1" href="#co_artifact_and_metadata_store_CO4-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>Getting the the runs for a given experiment</p></dd>
<dt><a class="co" id="callout_artifact_and_metadata_store_CO4-2" href="#co_artifact_and_metadata_store_CO4-2"><img src="assets/2.png" alt="2" width="12" height="12"></a></dt>
<dd><p>Sorting runs based on the specific parameters</p></dd>
</dl></div>

<p>MLflow will sort runs by root mean square error (rmse) and show the best ones.</p>

<p>For additional capabilities of the programmatic runs querying, consult the <a href="https://oreil.ly/Jistd">MLflow documentation</a>.</p>

<p>With all the capabilities of running programmatic queries, the most powerful way to evaluate runs’ metadata is through the MLflow UI, which we will cover next.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using the MLflow UI"><div class="sect2" id="idm45831173891512" title2="Using the MLflow UI" no2="6.2.3">
<h2>6.2.3. Using the MLflow UI</h2>

<p>The Tracking UI in MLflow lets you visualize, search, and compare runs, as well<a data-type="indexterm" data-primary="MLflow (Databricks)" data-secondary="metadata tools" data-tertiary="UI" id="idm45831173300632"></a><a data-type="indexterm" data-primary="Databricks MLflow" data-secondary="metadata tools" data-tertiary="UI" id="idm45831173270456"></a><a data-type="indexterm" data-primary="user interfaces (UI)" data-secondary="MLflow (Databricks)" id="idm45831173269240"></a> as download run artifacts or metadata for analysis in other tools. Because MLflow is not part of Kubeflow, its access is not provided by Kubeflow UI. Based on the provided virtual service, the MLflow UI is available at <em>&lt;Kubeflow Istio ingress gateway URL&gt;/mlflow</em>.</p>

<p><a data-type="xref" href="#mlflow_mainpage">FIGURE 6-5</a> shows the results produced by the run described. It is possible to filter results using the search box. For example, if we want to see only results for the KNN model, then the search criteria <code>tags.model="knn"</code> can be used. You can also use more complex filters, such as <code>tags.model="knn"</code> and <code>metrics.duration_prediction &lt; 0.002</code>, which will return results for the KNN model for which prediction duration is less than 0.002 sec.</p>

<figure><div id="mlflow_mainpage" class="figure" data-type="figure" title2="MLflow main page" no2="6-5">
<img src="assets/kfml_0607.png" alt="MLFlow Mainpage Screenshot" width="1440" height="736">
<h6>Figure 6-5. MLflow main page</h6>
</div></figure>

<p>By clicking the individual run we can see its details, as shown in <a data-type="xref" href="#indi_run">FIGURE 6-6</a>.</p>

<figure><div id="indi_run" class="figure" data-type="figure" title2="View of the individual run" no2="6-6">
<img src="assets/kfml_0608.png" alt="Screenshot of Individual Run" width="1440" height="1016">
<h6>Figure 6-6. View of the individual run</h6>
</div></figure>

<p>Alternatively, we can compare several runs by picking them and clicking compare, as seen in <a data-type="xref" href="#multirun">FIGURE 6-7</a>.</p>

<figure><div id="multirun" class="figure" data-type="figure" title2="Run comparison view" no2="6-7">
<img src="assets/kfml_0609.png" alt="Run Comparison Screenshot" width="1680" height="757">
<h6>Figure 6-7. Run comparison view</h6>
</div></figure>

<p>We can also view metrics comparison for multiple runs, as in <a data-type="xref" href="#metrics">FIGURE 6-8</a>.<sup><a data-type="noteref" id="idm45831173255160-marker" href="#idm45831173255160">[9]</a></sup></p>

<figure><div id="metrics" class="figure" data-type="figure" title2="Run metrics comparison view" no2="6-8">
<img src="assets/kfml_0610.png" alt="Run Metrics Comparison View Screenshot" width="1680" height="521">
<h6>Figure 6-8. Run metrics comparison view</h6>
</div></figure>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45831174087512" title2="Conclusion" no2="6.3">
<h1>6.3. Conclusion</h1>

<p>In this chapter we have shown how the Kubeflow Metadata component of the Kubeflow deployment supports storing and viewing ML metadata. We have also discussed shortcomings of this implementation, including its Python-only support and weak UI. Last, we covered how to supplement Kubeflow with components with similar functionality—MLflow and additional capabilities that can be achieved in this case.</p>

<p>In <a data-type="xref" href="#tf_ch">CHAPTER 7</a>, we explore using Kubeflow with TensorFlow to train and serve models.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45831174949784"><sup><a href="#idm45831174949784-marker">[1]</a></sup> For a good background on metadata for machine learning, and an overview of what to capture refer to this <a href="">blog post</a> written by Luigi Patruno.</p><p data-type="footnote" id="idm45831174790872"><sup><a href="#idm45831174790872-marker">[2]</a></sup> For more on this topic, see this <a href="">blog post</a> by Jennifer Villa and Yoav Zimmerman.</p><p data-type="footnote" id="idm45831174782152"><sup><a href="#idm45831174782152-marker">[3]</a></sup> Note that Kubeflow ML Metadata is different from <a href="">ML Metadata</a>, which is part of <a href="">TFX</a>.</p><p data-type="footnote" id="idm45831174774904"><sup><a href="#idm45831174774904-marker">[4]</a></sup> MLflow was initially developed by Databricks and currently is part of the <a href="">Linux Foundation</a>.</p><p data-type="footnote" id="idm45831174767144"><sup><a href="#idm45831174767144-marker">[5]</a></sup> The complete code for this notebook is located in <a href="">this book’s GitHub repo</a>.</p><p data-type="footnote" id="idm45831173984648"><sup><a href="#idm45831173984648-marker">[6]</a></sup> This is a simplified implementation. For complete implementation, see <a href="">this book’s GitHub repo</a>.</p><p data-type="footnote" id="idm45831173893464"><sup><a href="#idm45831173893464-marker">[7]</a></sup> Here we are showing usage of Python APIs. For additional APIs (R, Java, REST) refer to the <a href="">MLflow documentation</a>.</p><p data-type="footnote" id="idm45831173887816"><sup><a href="#idm45831173887816-marker">[8]</a></sup> The code here is adapted from <a href="">this article</a> by Jean-Michel Daignan.</p><p data-type="footnote" id="idm45831173255160"><sup><a href="#idm45831173255160-marker">[9]</a></sup> Also see the <a href="">MLflow documentation</a> for additional UI capabilities.</p></div></div></section>
<section data-type="chapter" data-pdf-bookmark="Chapter 7. Training a Machine Learning Model"><div class="chapter" id="tf_ch" data-type="chapter" title2="Training a Machine Learning Model" no2="7">
<h1>Chapter 7. Training a Machine Learning Model</h1>


<p>In <a data-type="xref" href="#data_and_feature_prep">CHAPTER 5</a>, we learned how to prepare and clean up our data, which is the first step in the machine learning pipeline. Now let’s take a deep dive into how to use our data to train a machine learning model.</p>

<p>Training is often considered the “bulk” of the work in machine learning. Our goal<a data-type="indexterm" data-primary="training" data-secondary="about" id="idm45831173245528"></a> is to create a function (the “model”) that can accurately predict results that it hasn’t seen before. Intuitively, model training is very much like how humans learn a new skill—we observe, practice, correct our mistakes, and gradually improve. In machine learning, we start with an initial model that might not be very good at its job. We then put the model through a series of training steps, where training data is fed to the model. At each training step, we compare the prediction results produced by our model with the true results, and see how well our model performed. We then tinker with the parameters to this model (for example, by changing how much weight is given to each feature) to attempt to improve the model’s accuracy. A good model is one that makes accurate predictions without overfitting to a specific set of inputs.</p>

<p>In this chapter, we are going to learn how to train machine learning models using two different libraries—TensorFlow and Scikit-learn. TensorFlow has native, first-class support in Kubeflow, while Scikit-learn does not. But as we will see in this chapter, both libraries can be easily integrated with Kubeflow. We’ll demonstrate how you can experiment with models in Kubeflow’s notebooks, and how you can deploy these models to production environments.</p>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Building a Recommender with TensorFlow"><div class="sect1" id="recommender_example" title2="Building a Recommender with TensorFlow" no2="7.1">
<h1>7.1. Building a Recommender with TensorFlow</h1>

<p>Let us first take a look at TensorFlow—an open source framework for machine learning<a data-type="indexterm" data-primary="training" data-secondary="TensorFlow recommender" data-tertiary="about TensorFlow" id="idm45831173240264"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="about" id="idm45831173239016"></a> developed by Google. It is currently one of the most popular libraries for machine learning–powered applications, in particular for implementing deep learning. TensorFlow has great support for computational tasks on a variety of hardware, including CPUs, GPUs, and TPUs. We chose TensorFlow for this tutorial because its high-level APIs are user-friendly and abstract away many of the gory details.</p>
<aside data-type="sidebar"><div class="sidebar" id="idm45831173237400">
<h5>What Is Deep Learning?</h5>
<p>In recent years, <em>deep learning</em>—a category of algorithms that leverage artificial<a data-type="indexterm" data-primary="deep learning" id="idm45831173235656"></a><a data-type="indexterm" data-primary="training" data-secondary="deep learning" id="idm45831173234952"></a> neural networks to progressively extract higher-level features from input data—has become increasingly popular. Deep learning has the ability to leverage hidden layers in the neural network to learn highly abstract models of the input.</p>

<p>Deep learning algorithms can be found in many everyday applications, like image recognition and natural language processing. The multiple hidden layers of neural networks allow these algorithms to discover increasingly abstract details from data. For example, while the initial layer in an image classification neural network might discover only object edges, the deeper layer may learn more complex features and classify the objects in the images.</p>
</div></aside>

<p>Let’s get acquainted with TensorFlow with a simple tutorial. In <a data-type="xref" href="#who_is_kubeflow_for_ch">CHAPTER 1</a> we <a data-type="indexterm" data-primary="recommendation systems" data-secondary="about" id="idm45831173231400"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="recommender" data-tertiary="about" id="idm45831173230424"></a>introduced our case studies, one of which is a product recommendation system for customers. In this chapter, we will be implementing this system with TensorFlow. Specifically, we will do two things:</p>
<ol>
<li>
<p>Use TensorFlow to train a model for product recommendation.</p>
</li>
<li>
<p>Use Kubeflow to wrap the training code and deploy it to a production cluster.</p>
</li>

</ol>

<p>TensorFlow’s high-level Keras API makes it relatively easy to implement our model.<a data-type="indexterm" data-primary="TensorFlow" data-secondary="recommender" data-tertiary="Keras API" id="idm45831173226312"></a><a data-type="indexterm" data-primary="training" data-secondary="TensorFlow recommender" data-tertiary="Keras API" id="idm45831173225064"></a> In fact, the bulk of the model can be implemented with less than 50 lines of Python code.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Keras is the high-level TensorFlow API for deep learning models. It has a user-friendly <a data-type="indexterm" data-primary="Keras API" id="idm45831173222520"></a>interface and high extensibility. As an added bonus, Keras has many common neural network implementations straight out of the box, so you can get a model up and running right away.</p>
</div>

<p class="less_space pagebreak-before">Let’s begin by selecting a model for our recommender. We begin with a simple<a data-type="indexterm" data-primary="TensorFlow" data-secondary="recommender" data-tertiary="model selection" id="idm45831173220648"></a><a data-type="indexterm" data-primary="training" data-secondary="TensorFlow recommender" data-tertiary="model selection" id="idm45831173219400"></a><a data-type="indexterm" data-primary="training" data-secondary="model selection" id="idm45831173156232"></a> assumption—that if two people (Alice and Bob) have similar opinions on a set of products, then they are also more likely to think similarly about other products. In other words, Alice is more likely to have the same preferences as Bob than would a randomly chosen third person. Thus, we can build a recommendation model using just the users’ purchase history. This is the idea of <a data-type="indexterm" data-primary="collaborative filtering" id="idm45831173154856"></a><a data-type="indexterm" data-primary="recommendation systems" data-secondary="collaborative filtering" id="idm45831173154248"></a>collaborative filtering—we collect preferential information from many users (hence “collaborative”) and use this data to make selective predictions (hence “filtering”).</p>

<p>To build this recommender model, we will need a few things:</p>
<dl>
<dt>Users’ purchasing history</dt>
<dd>
<p>We will use the example input data <a href="https://oreil.ly/F-8rS">from this GitHub repo</a>.</p>
</dd>
<dt>Data storage</dt>
<dd>
<p>To make sure that our model works across different platforms, we’ll use MinIO as the storage system.</p>
</dd>
<dt>Training model</dt>
<dd>
<p>The implementation that we are using is based on a  <a href="https://oreil.ly/hTGQf">Keras model on GitHub</a>.</p>
</dd>
</dl>

<p>We will first experiment with this model using Kubeflow’s notebook servers,<a data-type="indexterm" data-primary="TensorFlow" data-secondary="recommender" data-tertiary="notebook setup" id="ch07-nb2"></a><a data-type="indexterm" data-primary="training" data-secondary="TensorFlow recommender" data-tertiary="notebook setup" id="ch07-nb3"></a><a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="TensorFlow recommender notebook setup" id="ch07-nb4"></a> and then deploy the training job to our cluster using Kubeflow’s TFJob APIs.</p>








<section data-type="sect2" data-pdf-bookmark="Getting Started"><div class="sect2" id="idm45831173141736" title2="Getting Started" no2="7.1.1">
<h2>7.1.1. Getting Started</h2>

<p>Let’s get started by downloading the prerequisites. You can download the notebook from <a href="https://oreil.ly/Kubeflow_for_ML_ch07">this book’s GitHub repo</a>. To run the notebook, you will need a running Kubeflow cluster that includes a MinIO service. Review <a data-type="xref" href="#Supp_components">SECTION 3.2</a> to configure MinIO. Make sure that MinIO Client (“mc”) is also installed.</p>

<p>We also need to prepare the data to facilitate training: you can download the user purchase history data from <a href="https://oreil.ly/BK6XS">this GitHub site</a>. Then you can use MinIO Client to create the storage objects, as shown in <a data-type="xref" href="#set_up_prerequisites">EXAMPLE 7-1</a>.</p>
<div id="set_up_prerequisites" data-type="example" title2="Setting up prerequisites" no2="7-1">
<h5>Example 7-1. Setting up prerequisites</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="c"># Port-forward the MinIO service to http://localhost:9000</code>
kubectl port-forward -n kubeflow svc/minio-service 9000:9000 <code class="p">&amp;</code>

<code class="c"># Configure MinIO host</code>
mc config host add minio http://localhost:9000 minio minio123

<code class="c"># Create storage bucket</code>
mc mb minio/data

<code class="c"># Copy storage objects</code>
mc cp go/src/github.com/medium/items-recommender/data/recommend_1.csv <code class="se">\\</code>
        minio/data/recommender/users.csv
mc cp go/src/github.com/medium/items-recommender/data/trx_data.csv <code class="se">\\</code>
        minio/data/recommender/transactions.csv</pre></div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Starting a New Notebook Session"><div class="sect2" id="idm45831173119752" title2="Starting a New Notebook Session" no2="7.1.2">
<h2>7.1.2. Starting a New Notebook Session</h2>

<p>Now let’s start by creating a new notebook. You can do this by navigating to the “Notebook Servers” panel in your Kubeflow dashboard, then clicking “New Server” and following the instructions. For this example,
we use the <code>tensorFlow-1.15.2-notebook-cpu:1.0</code> image.<sup><a data-type="noteref" id="idm45831173117976-marker" href="#idm45831173117976">[1]</a></sup></p>

<p>When the notebook server starts up, click the “Upload” button in the top right corner and upload the <code>Recommender_Kubeflow.ipynb</code> file. Click the file to start a new 
session.</p>

<p>The first few sections of the code involve importing libraries and reading the training data from MinIO. <a data-type="indexterm" data-primary="feature preparation" data-secondary="recommendation system" id="idm45831173125928"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="feature preparation" data-tertiary="recommendation system" id="idm45831173124952"></a>Then we normalize the input data so that we are ready to start training. This process is called feature preparation, which we discussed in <a data-type="xref" href="#data_and_feature_prep">CHAPTER 5</a>. In this chapter we’ll focus on the model training part of the exercise.<a data-type="indexterm" data-startref="ch07-nb2" id="idm45831173122648"></a><a data-type="indexterm" data-startref="ch07-nb3" id="idm45831173121976"></a><a data-type="indexterm" data-startref="ch07-nb4" id="idm45831173113352"></a></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="TensorFlow Training"><div class="sect2" id="idm45831173112424" title2="TensorFlow Training" no2="7.1.3">
<h2>7.1.3. TensorFlow Training</h2>

<p>Now that our notebook is set up and the data is prepared, we can create a <a data-type="indexterm" data-primary="TensorFlow" data-secondary="recommender" data-tertiary="creating TensorFlow session" id="idm45831173110408"></a><a data-type="indexterm" data-primary="training" data-secondary="TensorFlow recommender" data-tertiary="creating TensorFlow session" id="idm45831173109144"></a>TensorFlow session, as shown in <a data-type="xref" href="#creating_tensorflow_session">EXAMPLE 7-2</a>.<sup><a data-type="noteref" id="idm45831173106920-marker" href="#idm45831173106920">[2]</a></sup></p>
<div id="creating_tensorflow_session" data-type="example" title2="Creating a TensorFlow session" no2="7-2">
<h5>Example 7-2. Creating a TensorFlow session</h5>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Create TF session and set it in Keras</code>
<code class="n">sess</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">Session</code><code class="p">()</code>
<code class="n">K</code><code class="o">.</code><code class="n">set_session</code><code class="p">(</code><code class="n">sess</code><code class="p">)</code>
<code class="n">K</code><code class="o">.</code><code class="n">set_learning_phase</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code></pre></div>

<p>For the model class, we use the code in <a data-type="xref" href="#DeepCollaborativeFiltering_learning">EXAMPLE 7-3</a> for collaborative filtering.</p>
<div id="DeepCollaborativeFiltering_learning" data-type="example" class="less_space pagebreak-before" title2="DeepCollaborativeFiltering learning" no2="7-3">
<h5>Example 7-3. DeepCollaborativeFiltering learning</h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">DeepCollaborativeFiltering</code><code class="p">(</code><code class="n">Model</code><code class="p">):</code>
   <code class="n">def__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">n_customers</code><code class="p">,</code> <code class="n">n_products</code><code class="p">,</code> <code class="n">n_factors</code><code class="p">,</code> <code class="n">p_dropout</code> <code class="o">=</code> <code class="mf">0.2</code><code class="p">):</code>
      <code class="n">x1</code> <code class="o">=</code> <code class="n">Input</code><code class="p">(</code><code class="n">shape</code> <code class="o">=</code> <code class="p">(</code><code class="mi">1</code><code class="p">,),</code> <code class="n">name</code><code class="o">=</code><code class="s2">"user"</code><code class="p">)</code>

      <code class="n">P</code> <code class="o">=</code> <code class="n">Embedding</code><code class="p">(</code><code class="n">n_customers</code><code class="p">,</code> <code class="n">n_factors</code><code class="p">,</code> <code class="n">input_length</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)(</code><code class="n">x1</code><code class="p">)</code>
      <code class="n">P</code> <code class="o">=</code> <code class="n">Reshape</code><code class="p">((</code><code class="n">n_factors</code><code class="p">,))(</code><code class="n">P</code><code class="p">)</code>

      <code class="n">x2</code> <code class="o">=</code> <code class="n">Input</code><code class="p">(</code><code class="n">shape</code> <code class="o">=</code> <code class="p">(</code><code class="mi">1</code><code class="p">,),</code> <code class="n">name</code><code class="o">=</code><code class="s2">"product"</code><code class="p">)</code>

      <code class="n">Q</code> <code class="o">=</code> <code class="n">Embedding</code><code class="p">(</code><code class="n">n_products</code><code class="p">,</code> <code class="n">n_factors</code><code class="p">,</code> <code class="n">input_length</code> <code class="o">=</code> <code class="mi">1</code><code class="p">)(</code><code class="n">x2</code><code class="p">)</code>
      <code class="n">Q</code> <code class="o">=</code> <code class="n">Reshape</code><code class="p">((</code><code class="n">n_factors</code><code class="p">,))(</code><code class="n">Q</code><code class="p">)</code>

      <code class="n">x</code> <code class="o">=</code> <code class="n">concatenate</code><code class="p">([</code><code class="n">P</code><code class="p">,</code> <code class="n">Q</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
      <code class="n">x</code> <code class="o">=</code> <code class="n">Dropout</code><code class="p">(</code><code class="n">p_dropout</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>

      <code class="n">x</code> <code class="o">=</code> <code class="n">Dense</code><code class="p">(</code><code class="n">n_factors</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
      <code class="n">x</code> <code class="o">=</code> <code class="n">Activation</code><code class="p">(</code><code class="s1">'relu'</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
      <code class="n">x</code> <code class="o">=</code> <code class="n">Dropout</code><code class="p">(</code><code class="n">p_dropout</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>

      <code class="n">output</code> <code class="o">=</code> <code class="n">Dense</code><code class="p">(</code><code class="mi">1</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>

      <code class="nb">super</code><code class="p">(</code><code class="n">DeepCollaborativeFiltering</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="nf-Magic">__init__</code><code class="p">([</code><code class="n">x1</code><code class="p">,</code> <code class="n">x2</code><code class="p">],</code> <code class="n">output</code><code class="p">)</code>

   <code class="k">def</code> <code class="nf">rate</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">customer_idxs</code><code class="p">,</code> <code class="n">product_idxs</code><code class="p">):</code>
      <code class="k">if</code> <code class="p">(</code><code class="nb">type</code><code class="p">(</code><code class="n">customer_idxs</code><code class="p">)</code> <code class="o">==</code> <code class="nb">int</code> <code class="ow">and</code> <code class="nb">type</code><code class="p">(</code><code class="n">product_idxs</code><code class="p">)</code> <code class="o">==</code> <code class="nb">int</code><code class="p">):</code>
          <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">predict</code><code class="p">([</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">customer_idxs</code><code class="p">)</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="mi">1</code><code class="p">,)),</code>\
                  <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">product_idxs</code><code class="p">)</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="mi">1</code><code class="p">,))])</code>

      <code class="k">if</code> <code class="p">(</code><code class="nb">type</code><code class="p">(</code><code class="n">customer_idxs</code><code class="p">)</code> <code class="o">==</code> <code class="nb">str</code> <code class="ow">and</code> <code class="nb">type</code><code class="p">(</code><code class="n">product_idxs</code><code class="p">)</code> <code class="o">==</code> <code class="nb">str</code><code class="p">):</code>
          <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code> \
                 <code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">customerMapping</code><code class="p">[</code><code class="n">customer_idxs</code><code class="p">])</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="mi">1</code><code class="p">,)),</code>\
                 <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">productMapping</code><code class="p">[</code><code class="n">product_idxs</code><code class="p">])</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="mi">1</code><code class="p">,))])</code>

      <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">predict</code><code class="p">([</code>
         <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">customerMapping</code><code class="p">[</code><code class="n">customer_idx</code><code class="p">]</code> \
                <code class="k">for</code> <code class="n">customer_idx</code> <code class="ow">in</code> <code class="n">customer_idxs</code><code class="p">]),</code>
            <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="n">productMapping</code><code class="p">[</code><code class="n">product_idx</code><code class="p">]</code> \
                <code class="k">for</code> <code class="n">product_idx</code> <code class="ow">in</code> <code class="n">product_idxs</code><code class="p">])</code>
      <code class="p">])</code></pre></div>

<p>This is the basis of our model class. It includes a constructor with some code to instantiate the collaborative filtering model using Keras APIs, and a “rate” function that we can use to make a prediction using our model—namely, what rating a customer would give to a particular product.</p>

<p>We can create an instance of the model, as in <a data-type="xref" href="#Model_creation_ex">EXAMPLE 7-4</a>.</p>
<div id="Model_creation_ex" data-type="example" title2="Model creation" no2="7-4">
<h5>Example 7-4. Model creation</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code> <code class="o">=</code> <code class="n">DeepCollaborativeFiltering</code><code class="p">(</code><code class="n">n_customers</code><code class="p">,</code> <code class="n">n_products</code><code class="p">,</code> <code class="n">n_factors</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">summary</code><code class="p">()</code></pre></div>

<p>Now we are ready to start training our model. We can do this by setting a few hyperparameters, as shown in <a data-type="xref" href="#Settg_Training_config">EXAMPLE 7-5</a>.<a data-type="indexterm" data-primary="TensorFlow" data-secondary="recommender" data-tertiary="hyperparameters" id="idm45831172757816"></a><a data-type="indexterm" data-primary="training" data-secondary="TensorFlow recommender" data-tertiary="hyperparameters" id="idm45831172750088"></a><a data-type="indexterm" data-primary="hyperparameters" data-secondary="TensorFlow recommender" id="idm45831172748872"></a></p>
<div id="Settg_Training_config" data-type="example" title2="Setting Training configuration" no2="7-5">
<h5>Example 7-5. Setting Training configuration</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">bs</code> <code class="o">=</code> <code class="mi">64</code>
<code class="n">val_per</code> <code class="o">=</code> <code class="mf">0.25</code>
<code class="n">epochs</code> <code class="o">=</code> <code class="mi">3</code></pre></div>

<p>These are hyperparameters that control the training process. They are typically set before training begins, unlike model parameters, which are learned from the training process. Setting the right values for hyperparameters can significantly impact the effectiveness of your model. For now, let’s just set some default values for them. In <a data-type="xref" href="#hyperparameter_tuning">CHAPTER 10</a> we’ll look at how to use Kubeflow to tune hyperparameters.</p>

<p>We are now ready to run the training code. See <a data-type="xref" href="#Fitting_model_ex">EXAMPLE 7-6</a>.<a data-type="indexterm" data-primary="TensorFlow" data-secondary="recommender" data-tertiary="running training code" id="idm45831172738664"></a><a data-type="indexterm" data-primary="training" data-secondary="TensorFlow recommender" data-tertiary="running training code" id="idm45831172737416"></a></p>
<div id="Fitting_model_ex" data-type="example" title2="Fitting model" no2="7-6">
<h5>Example 7-6. Fitting model</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">model</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">optimizer</code> <code class="o">=</code> <code class="s1">'adam'</code><code class="p">,</code> <code class="n">loss</code> <code class="o">=</code> <code class="n">mean_squared_logarithmic_error</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">x</code> <code class="o">=</code> <code class="p">[</code><code class="n">customer_idxs</code><code class="p">,</code> <code class="n">product_idxs</code><code class="p">],</code> <code class="n">y</code> <code class="o">=</code> <code class="n">ratings</code><code class="p">,</code>
        <code class="n">batch_size</code> <code class="o">=</code> <code class="n">bs</code><code class="p">,</code> <code class="n">epochs</code> <code class="o">=</code> <code class="n">epochs</code><code class="p">,</code> <code class="n">validation_split</code> <code class="o">=</code> <code class="n">val_per</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Done training!'</code><code class="p">)</code></pre></div>

<p>Once the training is complete, you should see results like in <a data-type="xref" href="#Model_training_results_ex">EXAMPLE 7-7</a>.</p>
<div id="Model_training_results_ex" data-type="example" title2="Model training results" no2="7-7">
<h5>Example 7-7. Model training results</h5>

<pre data-type="programlisting" data-code-language="bash">Train on <code class="m">100188</code> samples, validate on <code class="m">33397</code> samples
Epoch 1/3
100188/100188 <code class="o">[==============================]</code>
- 21s 212us/step - loss: 0.0105 - val_loss: 0.0186
Epoch 2/3
100188/100188 <code class="o">[==============================]</code>
- 20s 203us/step - loss: 0.0092 - val_loss: 0.0188
Epoch 3/3
100188/100188 <code class="o">[==============================]</code>
- 21s 212us/step - loss: 0.0078 - val_loss: 0.0192
Done training!</pre></div>

<p>Congratulations: you’ve successfully trained a TensorFlow model in a Jupyter notebook. But we’re not quite done yet—to make use of our model later, we should first <a data-type="indexterm" data-primary="TensorFlow" data-secondary="recommender" data-tertiary="exporting model" id="idm45831172639272"></a><a data-type="indexterm" data-primary="training" data-secondary="TensorFlow recommender" data-tertiary="exporting model" id="idm45831172638296"></a><a data-type="indexterm" data-primary="models" data-secondary="exporting" data-tertiary="TensorFlow" id="idm45831172636312"></a><a data-type="indexterm" data-primary="MinIO" data-secondary="Client exporting a model" id="idm45831172635096"></a>export it. You can do this by setting up the export destination using MinIO Client, as shown in <a data-type="xref" href="#Setting_export_dest">EXAMPLE 7-8</a>.</p>
<div id="Setting_export_dest" data-type="example" class="less_space pagebreak-before" title2="Setting export destination" no2="7-8">
<h5>Example 7-8. Setting export destination</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">directorystream</code> <code class="o">=</code> <code class="n">minioClient</code><code class="o">.</code><code class="n">get_object</code><code class="p">(</code><code class="s1">'data'</code><code class="p">,</code> <code class="s1">'recommender/directory.txt'</code><code class="p">)</code>
<code class="n">directory</code> <code class="o">=</code> <code class="s2">""</code>
<code class="k">for</code> <code class="n">d</code> <code class="ow">in</code> <code class="n">directorystream</code><code class="o">.</code><code class="n">stream</code><code class="p">(</code><code class="mi">32</code><code class="o">*</code><code class="mi">1024</code><code class="p">):</code>
    <code class="n">directory</code> <code class="o">+=</code> <code class="n">d</code><code class="o">.</code><code class="n">decode</code><code class="p">(</code><code class="s1">'utf-8'</code><code class="p">)</code>
<code class="n">arg_version</code> <code class="o">=</code> <code class="s2">"1"</code>
<code class="n">export_path</code> <code class="o">=</code> <code class="s1">'s3://models/'</code> <code class="o">+</code> <code class="n">directory</code> <code class="o">+</code> <code class="s1">'/'</code> <code class="o">+</code> <code class="n">arg_version</code> <code class="o">+</code> <code class="s1">'/'</code>
<code class="k">print</code> <code class="p">(</code><code class="s1">'Exporting trained model to'</code><code class="p">,</code> <code class="n">export_path</code><code class="p">)</code></pre></div>

<p>Once you have set up your export destination, you can then export the model, as in <a data-type="xref" href="#Exporting_model_example">EXAMPLE 7-9</a>.</p>
<div id="Exporting_model_example" data-type="example" title2="Exporting the model" no2="7-9">
<h5>Example 7-9. Exporting the model</h5>

<pre data-type="programlisting" data-code-language="python"><code class="c1"># Inputs/outputs</code>
<code class="n">tensor_info_users</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">saved_model</code><code class="o">.</code><code class="n">utils</code><code class="o">.</code><code class="n">build_tensor_info</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">input</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
<code class="n">tensor_info_products</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">saved_model</code><code class="o">.</code><code class="n">utils</code><code class="o">.</code><code class="n">build_tensor_info</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">input</code><code class="p">[</code><code class="mi">1</code><code class="p">])</code>
<code class="n">tensor_info_pred</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">saved_model</code><code class="o">.</code><code class="n">utils</code><code class="o">.</code><code class="n">build_tensor_info</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">output</code><code class="p">)</code>

<code class="k">print</code> <code class="p">(</code><code class="s2">"tensor_info_users"</code><code class="p">,</code> <code class="n">tensor_info_users</code><code class="o">.</code><code class="n">name</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="s2">"tensor_info_products"</code><code class="p">,</code> <code class="n">tensor_info_products</code><code class="o">.</code><code class="n">name</code><code class="p">)</code>
<code class="k">print</code> <code class="p">(</code><code class="s2">"tensor_info_pred"</code><code class="p">,</code> <code class="n">tensor_info_pred</code><code class="o">.</code><code class="n">name</code><code class="p">)</code>

<code class="c1"># Signature</code>
<code class="n">prediction_signature</code> <code class="o">=</code> <code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">saved_model</code><code class="o">.</code><code class="n">signature_def_utils</code><code class="o">.</code><code class="n">build_signature_def</code><code class="p">(</code>
        <code class="n">inputs</code><code class="o">=</code><code class="p">{</code><code class="s2">"users"</code><code class="p">:</code> <code class="n">tensor_info_users</code><code class="p">,</code> <code class="s2">"products"</code><code class="p">:</code> <code class="n">tensor_info_products</code><code class="p">},</code>
        <code class="n">outputs</code><code class="o">=</code><code class="p">{</code><code class="s2">"predictions"</code><code class="p">:</code> <code class="n">tensor_info_pred</code><code class="p">},</code>
        <code class="n">method_name</code><code class="o">=</code><code class="n">tf</code><code class="o">.</code><code class="n">saved_model</code><code class="o">.</code><code class="n">signature_constants</code><code class="o">.</code><code class="n">PREDICT_METHOD_NAME</code><code class="p">))</code>
<code class="c1"># Export</code>
<code class="n">legacy_init_op</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">group</code><code class="p">(</code><code class="n">tf</code><code class="o">.</code><code class="n">tables_initializer</code><code class="p">(),</code> <code class="n">name</code><code class="o">=</code><code class="s1">'legacy_init_op'</code><code class="p">)</code>
<code class="n">builder</code> <code class="o">=</code> <code class="n">tf</code><code class="o">.</code><code class="n">saved_model</code><code class="o">.</code><code class="n">builder</code><code class="o">.</code><code class="n">SavedModelBuilder</code><code class="p">(</code><code class="n">export_path</code><code class="p">)</code>
<code class="n">builder</code><code class="o">.</code><code class="n">add_meta_graph_and_variables</code><code class="p">(</code>
      <code class="n">sess</code><code class="p">,</code> <code class="p">[</code><code class="n">tf</code><code class="o">.</code><code class="n">saved_model</code><code class="o">.</code><code class="n">tag_constants</code><code class="o">.</code><code class="n">SERVING</code><code class="p">],</code>
      <code class="n">signature_def_map</code><code class="o">=</code><code class="p">{</code>
        <code class="n">tf</code><code class="o">.</code><code class="n">saved_model</code><code class="o">.</code><code class="n">signature_constants</code><code class="o">.</code><code class="n">DEFAULT_SERVING_SIGNATURE_DEF_KEY</code><code class="p">:</code>
          <code class="n">prediction_signature</code><code class="p">,</code>
      <code class="p">},</code>
      <code class="n">legacy_init_op</code><code class="o">=</code><code class="n">legacy_init_op</code><code class="p">)</code>
<code class="n">builder</code><code class="o">.</code><code class="n">save</code><code class="p">()</code></pre></div>

<p>Now we’re ready to use this model to serve predictions, as we’ll learn in <a data-type="xref" href="#inference_ch">CHAPTER 8</a>. But before that, let’s look at how to deploy this training job using Kubeflow.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Deploying a TensorFlow Training Job"><div class="sect1" id="idm45831173241944" title2="Deploying a TensorFlow Training Job" no2="7.2">
<h1>7.2. Deploying a TensorFlow Training Job</h1>

<p>So far we have done some TensorFlow training using Jupyter notebooks, which<a data-type="indexterm" data-primary="TensorFlow" data-secondary="recommender" data-tertiary="deployment" id="ch07-dep2"></a><a data-type="indexterm" data-primary="training" data-secondary="TensorFlow recommender" data-tertiary="deployment" id="ch07-dep3"></a> is a great way to prototype and experiment. But soon we may discover that our prototype is insufficient—perhaps we need to refine the model using more data, or perhaps we need to train the model using specialized hardware. Sometimes we may even need to continuously run the training job because our model is constantly evolving. Perhaps most importantly, our model has to be deployable to production, where it can serve actual customer requests.</p>

<p>In order to handle these requirements, our training code must be easily  packageable and deployable to various different environments.
One of the ways to achieve this is to use <a data-type="indexterm" data-primary="TFJob for deployment" data-secondary="TensorFlow recommender" id="ch07-dep5"></a>TFJob—a Kubernetes custom resource (implemented using Kubernetes operator <code>tf-operator</code>) that you can use to run TensorFlow training jobs on Kubernetes.</p>
<aside data-type="sidebar"><div class="sidebar" id="idm45831172152008">
<h5>Why Should You Use TFJobs?</h5>
<p>There are many challenges to deploying our training code to a production environment. To name a few:<a data-type="indexterm" data-primary="model inference" data-secondary="deployment strategies" id="idm45831172150312"></a></p>

<ul>
<li>
<p>What kind of infrastructure do we have to work with? Are we running in the cloud or on-premises?</p>
</li>
<li>
<p>Who has access to the training job and its data? Can we share a training job with our teammates?</p>
</li>
<li>
<p>How do we scale up the training job? How do we clean up the resources when we are done?</p>
</li>
<li>
<p>What do we do with the model after we’ve trained it? How do we export the model so we can make use of it?</p>
</li>
</ul>

<p>Typically, these problems require the user to implement a large amount of “glue code” to work with the underlying infrastructure. Such code is likely to differ greatly depending on the environment and the technical constraints, which means that developing this technical stack could be much more time-consuming than the model itself.</p>

<p>These are the problems that Kubeflow aims to solve. Since <a data-type="indexterm" data-primary="scalability of Kubeflow" data-secondary="Kubernetes foundation" id="idm45831172143736"></a><a data-type="indexterm" data-primary="portability of Kubeflow" data-secondary="Kubernetes foundation" id="idm45831172142760"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="cloud native microservices" id="idm45831172141816"></a><a data-type="indexterm" data-primary="cloud native microservices" id="idm45831172140808"></a>Kubeflow’s architecture is entirely based on Kubernetes, all of Kubernetes’ scalable and portable features are available to Kubeflow. Applications in Kubernetes are developed as “cloud native” microservices. In the case of
machine learning training, if you want to scale up a training job, simply increase the number of desired replicas and the underlying system will take care of that for you. The same Kubeflow job that runs on Amazon Cloud can easily be exported to a different cluster running Google Cloud or even to 
on-premises.</p>

<p>Kubeflow makes it easy to configure TensorFlow jobs on a <a data-type="indexterm" data-primary="TensorFlow" data-secondary="jobs as Kubernetes custom resources" id="idm45831172138504"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="custom resources APIs" id="idm45831172137512"></a><a data-type="indexterm" data-primary="custom resources on Kubernetes" id="idm45831172136568"></a>Kubernetes cluster by orchestrating them as custom resources. Custom resources are extensions to the core Kubernetes API that store collections of API objects. By using custom resources, developers only need to provide a “desired state” of their applications, and the underlying controllers will take care of the rest.</p>
</div></aside>

<p>We’ll start by deploying our recommender as a single-container TFJob. Since we already have a Python notebook, exporting it as a Python file is fairly simple—just select “File,” then “Download as” and select “Python.” This should save your notebook as a ready-to-execute Python file.</p>

<p>The next step is to package the training code in a container. This can be done with the Dockerfile, as seen in <a data-type="xref" href="#TFJob_Dockerfi">EXAMPLE 7-10</a>.<a data-type="indexterm" data-primary="Docker" data-secondary="deploying recommender training code" id="idm45831172133448"></a></p>
<div id="TFJob_Dockerfi" data-type="example" title2="TFJob Dockerfile" no2="7-10">
<h5>Example 7-10. TFJob Dockerfile</h5>

<pre data-type="programlisting" data-code-language="docker"><code class="k">FROM</code><code class="s">  tensorflow/tensorflow:1.15.2-py3</code>
<code class="k">RUN</code> pip3 install --upgrade pip
<code class="k">RUN</code> pip3 install pandas --upgrade
<code class="k">RUN</code> pip3 install keras --upgrade
<code class="k">RUN</code> pip3 install minio --upgrade
<code class="k">RUN</code> mkdir -p /opt/kubeflow
COPY Recommender_Kubeflow.py /opt/kubeflow/
<code class="k">ENTRYPOINT</code><code class="s"> ["python3", "/opt/kubeflow/Recommender_Kubeflow.py"]</code></pre></div>

<p>Next, we need to build this container along with its required libraries, and push the container image to a repository:</p>

<pre data-type="programlisting" id="untitled_programlisting_11" title2="(no caption)" no2="">docker build -t kubeflow/recommenderjob:1.0 .
docker push kubeflow/recommenderjob:1.0</pre>

<p>Once that’s done, we are ready to create the specification for a TFJob, as in <a data-type="xref" href="#TFJob_exam">EXAMPLE 7-11</a>.<a data-type="indexterm" data-primary="TFJob for deployment" data-secondary="specifications" id="idm45831172121272"></a></p>
<div id="TFJob_exam" data-type="example" title2="Single-container TFJob example" no2="7-11">
<h5>Example 7-11. Single-container TFJob example</h5>

<pre data-type="programlisting">apiVersion: "kubeflow.org/v1"   <a class="co" id="co_training_a_machine_learning_model_CO1-1" href="#callout_training_a_machine_learning_model_CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"></a>
kind: "TFJob"                   <a class="co" id="co_training_a_machine_learning_model_CO1-2" href="#callout_training_a_machine_learning_model_CO1-2"><img src="assets/2.png" alt="2" width="12" height="12"></a>
metadata:
  name: "recommenderjob"        <a class="co" id="co_training_a_machine_learning_model_CO1-3" href="#callout_training_a_machine_learning_model_CO1-3"><img src="assets/3.png" alt="3" width="12" height="12"></a>
spec:
  tfReplicaSpecs:               <a class="co" id="co_training_a_machine_learning_model_CO1-4" href="#callout_training_a_machine_learning_model_CO1-4"><img src="assets/4.png" alt="4" width="12" height="12"></a>
    Worker:
      replicas: 1
    restartPolicy: Never
    template:
      spec:
        containers:
        - name: tensorflow image: kubeflow/recommenderjob:1.0</pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_training_a_machine_learning_model_CO1-1" href="#co_training_a_machine_learning_model_CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>The <code>apiVersion</code> field specifies which version of the TFJob custom resource you are using. The corresponding version (in this case v1) needs to be installed in your Kubeflow cluster.</p></dd>
<dt><a class="co" id="callout_training_a_machine_learning_model_CO1-2" href="#co_training_a_machine_learning_model_CO1-2"><img src="assets/2.png" alt="2" width="12" height="12"></a></dt>
<dd><p>The <code>kind</code> field identifies the type of the custom resource—in this case a TFJob.</p></dd>
<dt><a class="co" id="callout_training_a_machine_learning_model_CO1-3" href="#co_training_a_machine_learning_model_CO1-3"><img src="assets/3.png" alt="3" width="12" height="12"></a></dt>
<dd><p>The <code>metadata</code> field is common to all Kubernetes objects and is used to uniquely identify the object in the cluster—you can add fields like name, namespace, and labels here.</p></dd>
<dt><a class="co" id="callout_training_a_machine_learning_model_CO1-4" href="#co_training_a_machine_learning_model_CO1-4"><img src="assets/4.png" alt="4" width="12" height="12"></a></dt>
<dd><p>The most important part of the schema is <code>tfReplicaSpecs</code>. This is the actual description of your TensorFlow training cluster and its desired state. For this example, we just have a single worker replica. In the following section, we’ll examine this field further.</p></dd>
</dl></div>

<p>There are a few other optional configurations for your TFJob, including:</p>
<dl>
<dt><code>activeDeadlineSeconds</code></dt>
<dd>
<p>How long to keep this job active before the system can terminate it. If this is set, the system will kill the job after the deadline expires.</p>
</dd>
<dt><code>backoffLimit</code> </dt>
<dd>
<p>How many times to keep retrying this job before marking it as failed. For example, setting this to 3 means that if a job fails 3 times, the system will stop retrying.</p>
</dd>
<dt><code>cleanPodPolicy</code></dt>
<dd>
<p>Configures whether or not to clean up the Kubernetes pods after the job completes. Setting this policy can be useful to keep pods for debugging purposes. This can be set to All (all pods are cleaned up), Running (only running pods are cleaned up), or None (no pods are cleaned up).</p>
</dd>
</dl>

<p>Now deploy the TFJob to your cluster, as in <a data-type="xref" href="#Depl_TFJob">EXAMPLE 7-12</a>.</p>
<div id="Depl_TFJob" data-type="example" title2="Deploying TFJob" no2="7-12">
<h5>Example 7-12. Deploying TFJob</h5>

<pre data-type="programlisting">kubectl apply -f recommenderjob.yaml</pre></div>

<p>You can monitor the status of the TFJob with the command in <a data-type="xref" href="#View_state_TFJob">EXAMPLE 7-13</a>.</p>
<div id="View_state_TFJob" data-type="example" title2="Viewing the state of TFJob" no2="7-13">
<h5>Example 7-13. Viewing the state of TFJob</h5>

<pre data-type="programlisting">kubectl describe tfjob recommenderjob</pre></div>

<p>This should display something like <a data-type="xref" href="#TF_Recommender_job_descr_ex">EXAMPLE 7-14</a>.</p>
<div id="TF_Recommender_job_descr_ex" data-type="example" class="less_space pagebreak-before" title2="TF Recommender job description" no2="7-14">
<h5>Example 7-14. TF Recommender job description</h5>

<pre data-type="programlisting">Status:
  Completion Time:  2019-05-18T00:58:27Z
  Conditions:
    Last Transition Time:  2019-05-18T02:34:24Z
    Last Update Time:      2019-05-18T02:34:24Z
    Message:               TFJob recommenderjob is created.
    Reason:                TFJobCreated
    Status:                True
    Type:                  Created
    Last Transition Time:  2019-05-18T02:38:28Z
    Last Update Time:      2019-05-18T02:38:28Z
    Message:               TFJob recommenderjob is running.
    Reason:                TFJobRunning
    Status:                False
    Type:                  Running
    Last Transition Time:  2019-05-18T02:38:29Z
    Last Update Time:      2019-05-18T02:38:29Z
    Message:               TFJob recommenderjob successfully completed.
    Reason:                TFJobSucceeded
    Status:                True
    Type:                  Succeeded
  Replica Statuses:
    Worker:
      Succeeded:  1</pre></div>

<p>Note that the status field contains a list of job conditions, which represent when the job transitioned into each state. This is useful for debugging—if the job failed, the reason for the job’s failure would appear here.<a data-type="indexterm" data-primary="model inference" data-secondary="debugging TFJob deployment" id="idm45831172055848"></a><a data-type="indexterm" data-primary="debugging" data-secondary="TFJob deployment" id="idm45831172054808"></a><a data-type="indexterm" data-startref="ch07-dep2" id="idm45831172053864"></a><a data-type="indexterm" data-startref="ch07-dep3" id="idm45831172053192"></a><a data-type="indexterm" data-startref="ch07-dep5" id="idm45831172052520"></a></p>

<p>So far we have trained a fairly simple and straightforward model with a modest number of training samples. In real life, learning more complex models may require significantly more training samples or model parameters. Such models can be too large and computationally complex to be handled by one machine. This is where distributed training comes in.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Distributed Training"><div class="sect1" id="idm45831172389080" title2="Distributed Training" no2="7.3">
<h1>7.3. Distributed Training</h1>

<p>By now we’ve deployed a single-worker TensorFlow job with Kubeflow. It is called “single-worker”<a data-type="indexterm" data-primary="TensorFlow" data-secondary="single-worker jobs" id="idm45831172049224"></a><a data-type="indexterm" data-primary="single-worker TensorFlow jobs" id="idm45831172048248"></a><a data-type="indexterm" data-primary="training" data-secondary="TensorFlow recommender" data-tertiary="single-worker jobs" id="idm45831172047560"></a><a data-type="indexterm" data-primary="training" data-secondary="distributed training" data-tertiary="about" id="idm45831172046344"></a><a data-type="indexterm" data-primary="distributed training" data-secondary="about" id="idm45831172045128"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="distributed training" data-tertiary="about" id="idm45831172044184"></a> because everything from hosting the data to executing the actual training steps is done on a single machine. However, as models become more complex, a single machine is often insufficient—we may need to distribute the model or the training samples over several networked machines. TensorFlow supports a distributed training mode, in which training is performed in parallel over several worker nodes.</p>

<p>Distributed training typically comes in two flavors: data parallelism and model parallelism.<a data-type="indexterm" data-primary="data parallelism distributed training" id="idm45831172041928"></a><a data-type="indexterm" data-primary="model training" data-see="training" id="idm45831172041208"></a><a data-type="indexterm" data-primary="training" data-secondary="distributed training" data-tertiary="data versus model parallelism" id="idm45831172040264"></a> In data parallelism, the training data is partitioned into chunks, and the same training code runs on each chunk. At the end of each training step, each worker communicates its updates to all other nodes. Model parallelism is the opposite—the same training data is used in all workers, but the model itself is partitioned. At the end of each step, each worker is responsible for synchronizing the shared parts of the model.</p>
<aside data-type="sidebar"><div class="sidebar" id="idm45831172038328">
<h5>Distribution Strategies in TensorFlow</h5>
<p>TensorFlow supports a number of different strategies for distributed training. These include:<a data-type="indexterm" data-primary="TensorFlow" data-secondary="distributed training" data-tertiary="distribution strategies" id="idm45831172036952"></a></p>
<dl>
<dt>Mirrored strategy</dt>
<dd>
<p>This is a synchronous strategy, which means the training steps and gradients are synced across replicas.<a data-type="indexterm" data-primary="mirrored distributed training strategy" id="idm45831172033720"></a> Copies of all variables in the model are replicated on each device across all workers.</p>
</dd>
<dt>TPU strategy</dt>
<dd>
<p>Similar to mirrored strategy, but allows you to train on Google’s TPUs.<a data-type="indexterm" data-primary="TPU distributed training strategy" id="idm45831172031592"></a></p>
</dd>
<dt>Multiworker mirrored strategy</dt>
<dd>
<p>Also similar to mirrored strategy, but uses CollectiveOps multiworker all-reduce to keep variables in sync.<a data-type="indexterm" data-primary="multiworker mirrored distributed training strategy" id="idm45831172029496"></a></p>
</dd>
<dt>Central storage strategy</dt>
<dd>
<p>Instead of replicating variables across all workers, this strategy stores variables on a <a data-type="indexterm" data-primary="central storage distributed training" id="idm45831172027288"></a>central CPU while replicating computational work across workers.</p>
</dd>
<dt>Parameter server strategy</dt>
<dd>
<p>Nodes are classified as either workers or parameter servers. Each model parameter is <a data-type="indexterm" data-primary="parameter server distributed training" id="idm45831172025144"></a>stored on one parameter server, while computational work is replicated among workers.</p>
</dd>
</dl>
</div></aside>

<p>The TFJob interface supports multiworker distributed training.<a data-type="indexterm" data-primary="TFJob for deployment" data-secondary="multiworker distributed training" id="idm45831172023640"></a> Conceptually, a TFJob is a logical grouping of all resources related to a training job, including <em>pods</em> and <em>services</em>. In Kubeflow, each replicated worker or parameter server is scheduled on its own single-container pod. In order for the replicas to synchronize with each other, each replica needs to expose itself through an endpoint, which is a Kubernetes internal service. Grouping these resources logically under a parent resource (which is the TFJob) allows these resources to be co-scheduled and garbage collected together.</p>

<p>In this section we’ll deploy a simple MNIST example with distributed training.<a data-type="indexterm" data-primary="training" data-secondary="distributed training" data-tertiary="MNIST example" id="ch07-dist"></a><a data-type="indexterm" data-primary="model inference" data-secondary="deployment of in distributed training MNIST example" id="ch07-dr"></a><a data-type="indexterm" data-primary="MNIST (Modified National Institute of Standards and Technology)" data-secondary="distributed training" id="ch07-MN"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="distributed training" data-tertiary="MNIST example" id="ch07-MN2"></a> The TensorFlow training code is provided for you at <a href="https://oreil.ly/ySztV">this GitHub repo</a>.</p>

<p>Let’s take a look at the YAML file for the distributed TensorFlow job in <a data-type="xref" href="#distrib_TFJob">EXAMPLE 7-15</a>.<a data-type="indexterm" data-primary="YAML" data-secondary="TensorFlow distributed training job" id="idm45831172013128"></a></p>
<div id="distrib_TFJob" data-type="example" class="less_space pagebreak-before" title2="Distributed TFJob example" no2="7-15">
<h5>Example 7-15. Distributed TFJob example</h5>

<pre data-type="programlisting">apiVersion: "kubeflow.org/v1"
kind: "TFJob"
metadata:
  name: "mnist"
  namespace: kubeflow
spec:
  cleanPodPolicy: None
  tfReplicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: Never
      template:
        spec:
          containers:
            - name: tensorflow
              image: gcr.io/kubeflow-ci/tf-mnist-with-summaries:1.0
              command:
                - "python"
                - "/var/tf_mnist/mnist_with_summaries.py"
                - "--log_dir=/train/logs"
                - "--learning_rate=0.01"
                - "--batch_size=150"
              volumeMounts:
                - mountPath: "/train"
                  name: "training"
          volumes:
            - name: "training"
              persistentVolumeClaim:
                claimName: "tfevent-volume"</pre></div>

<p>Note that the <code>tfReplicaSpecs</code> field now contains a few different replica types. In a typical TensorFlow training cluster, there are a few possible possibilities:</p>
<dl>
<dt>Chief</dt>
<dd>
<p>Responsible for orchestrating computational tasks, emitting events, and checkpointing the model</p>
</dd>
<dt>Parameter servers</dt>
<dd>
<p>Provide a distributed data store for the model parameters</p>
</dd>
<dt>Worker</dt>
<dd>
<p>This is where the computations and training actually happen. When a chief node is not explicitly defined (as in the preceding example), one of the workers acts as the chief node.</p>
</dd>
<dt>Evaluator</dt>
<dd>
<p>The evaluators can be used to compute evaluation metrics as the model is trained.</p>
</dd>
</dl>

<p class="less_space pagebreak-before">Note also that a replica spec contains a number of properties that describe its desired state:</p>
<dl>
<dt><code>replicas</code></dt>
<dd>
<p>How many replicas should be spawned for this replica type</p>
</dd>
<dt><code>template</code></dt>
<dd>
<p>A <code>PodTemplateSpec</code> that describes the pod to create for each replica</p>
</dd>
<dt><code>restartPolicy</code></dt>
<dd>
<p>Determines whether pods will be restarted when they exit. The allowed values are as follows:</p>
<dl>
<dt><code>Always</code></dt>
<dd>
<p>Means the pod will always be restarted. This policy is good for parameter servers since they never exit and should always be restarted in the event of failure.</p>
</dd>
<dt><code>OnFailure</code></dt>
<dd>
<p>Means the pod will be restarted if the pod exits due to failure. A nonzero exit code indicates a failure.
An exit code of 0 indicates success and the pod will not be restarted.
This policy is good for the chief and workers.</p>
</dd>
<dt><code>ExitCode</code></dt>
<dd>
<p>Means the restart behavior is dependent on the exit code of the TensorFlow container as follows:</p>

<ul>
<li>
<p>0 indicates the process completed successfully and will not be restarted.</p>
</li>
<li>
<p>1–127 indicates a permanent error and that the container will not be 
restarted.</p>
</li>
<li>
<p>128–255 indicates a retryable error and the container will be restarted. This policy is good for the chief and workers.</p>
</li>
</ul>
</dd>
</dl>
</dd>
<dt><code>Never</code></dt>
<dd>
<p>This means pods that terminate will never be restarted. This policy should rarely be used, because Kubernetes will terminate pods for any number of reasons (e.g., node becomes unhealthy) and this will prevent the job from recovering.</p>
</dd>
</dl>

<p>Once you have the TFJob spec written, deploy it to your Kubeflow cluster:</p>

<pre data-type="programlisting" id="untitled_programlisting_12" title2="(no caption)" no2="">kubectl apply -f dist-mnist.yaml</pre>

<p>Monitoring the job status is similar to a single-container job:</p>

<pre data-type="programlisting" id="untitled_programlisting_13" title2="(no caption)" no2="">kubectl describe tfjob mnist</pre>

<p>This should output something like <a data-type="xref" href="#TFJob_execution_result_ex">EXAMPLE 7-16</a>.</p>
<div id="TFJob_execution_result_ex" data-type="example" title2="TFJob execution result" no2="7-16">
<h5>Example 7-16. TFJob execution result</h5>

<pre data-type="programlisting">Status:
  Completion Time:  2019-05-12T00:58:27Z
  Conditions:
    Last Transition Time:  2019-05-12T00:57:31Z
    Last Update Time:      2019-05-12T00:57:31Z
    Message:               TFJob dist-mnist-example is created.
    Reason:                TFJobCreated
    Status:                True
    Type:                  Created
    Last Transition Time:  2019-05-12T00:58:21Z
    Last Update Time:      2019-05-12T00:58:21Z
    Message:               TFJob dist-mnist-example is running.
    Reason:                TFJobRunning
    Status:                False
    Type:                  Running
    Last Transition Time:  2019-05-12T00:58:27Z
    Last Update Time:      2019-05-12T00:58:27Z
    Message:               TFJob dist-mnist-example successfully completed.
    Reason:                TFJobSucceeded
    Status:                True
    Type:                  Succeeded
  Replica Statuses:
    Worker:
      Succeeded:  2</pre></div>

<p>Notice that the <code>Replica Statuses</code> field shows a breakdown of status by each replica type. The TFJob is successfully completed when all of its workers complete. If any worker has failed, then the TFJob’s status would be failed as well.<a data-type="indexterm" data-startref="ch07-dist" id="idm45831171949608"></a><a data-type="indexterm" data-startref="ch07-dr" id="idm45831171948904"></a><a data-type="indexterm" data-startref="ch07-MN" id="idm45831171948232"></a><a data-type="indexterm" data-startref="ch07-MN2" id="idm45831171947560"></a></p>








<section data-type="sect2" data-pdf-bookmark="Using GPUs"><div class="sect2" id="idm45831171946760" title2="Using GPUs" no2="7.3.1">
<h2>7.3.1. Using GPUs</h2>

<p>GPUs are processors that are composed of many smaller and specialized cores. Originally<a data-type="indexterm" data-primary="training" data-secondary="distributed training" data-tertiary="GPUs for" id="idm45831171945464"></a><a data-type="indexterm" data-primary="GPUs" data-secondary="training using" id="idm45831171944216"></a> designed to render graphics, GPUs are increasingly used for massively parallel computational tasks, such as machine learning. Unlike CPUs, GPUs are ideal for distributing large workloads over its many cores and executing them concurrently.</p>

<p>To use GPUs for training, your Kubeflow cluster needs to be preconfigured to enable GPUs. Refer to your cloud provider’s documentation on enabling GPU usage. After enabling GPUs on the cluster, you can enable GPUs on the specific replica type in the training spec by modifying the command-line arguments, as in <a data-type="xref" href="#TFJob_w_GPU_ex">EXAMPLE 7-17</a>.</p>
<div id="TFJob_w_GPU_ex" data-type="example" title2="TFJob with GPU example" no2="7-17">
<h5>Example 7-17. TFJob with GPU example</h5>

<pre data-type="programlisting">    Worker:
      replicas: 4
      restartPolicy: Never
      template:
        spec:
          containers:
            - name: tensorflow
              image: kubeflow/tf-dist-mnist-test:1.0
              args:
            - python
            - /var/tf_dist_mnist/dist_mnist.py
            - --num_gpus=1</pre></div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Using Other Frameworks for Distributed Training"><div class="sect2" id="idm45831171938760" title2="Using Other Frameworks for Distributed Training" no2="7.3.2">
<h2>7.3.2. Using Other Frameworks for Distributed Training</h2>

<p>Kubeflow is designed to be a multiframework machine learning platform. That means the schema<a data-type="indexterm" data-primary="training" data-secondary="distributed training" data-tertiary="other frameworks for" id="idm45831171937144"></a><a data-type="indexterm" data-primary="PyTorch for distributed training" id="idm45831171935896"></a><a data-type="indexterm" data-primary="Caffe2 for distributed training" id="idm45831171935208"></a> for distributed training can easily be extended to other frameworks. As of the time of this writing, there are a number of operators written to provide first-class support for other frameworks, including PyTorch and Caffe2.</p>

<p><a data-type="xref" href="#pytorch_distrib">EXAMPLE 7-18</a> shows what a PyTorch training job spec looks like.<a data-type="indexterm" data-primary="PyTorch for distributed training" data-secondary="job spec example" id="idm45831171933000"></a></p>
<div id="pytorch_distrib" data-type="example" title2="Pytorch Distributed Training Example" no2="7-18">
<h5>Example 7-18. Pytorch Distributed Training Example</h5>

<pre data-type="programlisting">apiVersion: "kubeflow.org/v1"
kind: "PyTorchJob"
metadata:
  name: "pytorch-dist"
spec:
  pytorchReplicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          containers:
            - name: pytorch
              image: gcr.io/kubeflow-ci/pytorch-dist-sendrecv-test:1.0
    Worker:
      replicas: 3
      restartPolicy: OnFailure
      template:
        spec:
          containers:
            - name: pytorch
              image: gcr.io/kubeflow-ci/pytorch-dist-sendrecv-test:1.0</pre></div>

<p>As you can see, the format is very similar to that of TFJobs. The only difference is in the replica types.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Training a Model Using Scikit-Learn"><div class="sect1" id="model_building_income" title2="Training a Model Using Scikit-Learn" no2="7.4">
<h1>7.4. Training a Model Using Scikit-Learn</h1>

<p>Thus far we have seen how to use the built-in operators in Kubeflow to train machine learning models.<a data-type="indexterm" data-primary="training" data-secondary="Scikit-learn" data-tertiary="about" id="idm45831171927416"></a><a data-type="indexterm" data-primary="Scikit-learn" data-secondary="about" id="idm45831171926168"></a><a data-type="indexterm" data-primary="Jupyter notebooks" data-secondary="Scikit-learn notebook setup" id="ch07-sci"></a><a data-type="indexterm" data-primary="Scikit-learn Jupyter notebook setup" id="ch07-sci2"></a> However, there are many frameworks and libraries for which there are no Kubeflow operators. In these cases you can still use your favorite frameworks in Jupyter notebooks<sup><a data-type="noteref" id="idm45831171922664-marker" href="#idm45831171922664">[3]</a></sup> or in custom Docker images.</p>

<p>Scikit-learn is an open source Python library for machine learning built on top of NumPy<a data-type="indexterm" data-primary="Python" data-secondary="Scikit-learn" id="idm45831171921400"></a> for high-performance linear algebra and array operations. The project started as scikits.learn, a Google Summer of Code project by David Cournapeau. Its name stems from the notion that it is a “SciKit” (SciPy Toolkit), a separately developed and distributed third-party extension to SciPy. Scikit-learn is one of the most popular machine learning libraries on GitHub, and one of the best-maintained. Training models with Scikit-learn is supported in Kubeflow as generic Python code, with no specific operators for distributed training.</p>

<p>The library supports state-of-the-art algorithms such as KNN, XGBoost, Random Forest, and SVM. Scikit-learn is widely used in Kaggle competitions and by prominent tech companies. Scikit-learn helps in preprocessing, dimensionality reduction (parameter selection), classification, regression, clustering, and model selection.</p>

<p>In this section, we will explore how to train models in Kubeflow by using Scikit-learn on the<a data-type="indexterm" data-primary="US Census dataset" data-secondary="about dataset" id="idm45831171918264"></a> <a href="https://oreil.ly/9nQrt">1994 US Census dataset</a>. This example is based on <a href="https://oreil.ly/9hnha">this implementation</a> of Anchor explanations for income prediction, and leverages an extract from the 1994 census dataset. The dataset includes several categorical variables and continuous features, including age, education, marital status, occupation, salary, relationship, race, sex, native country, and capital gains and losses.
We will use a Random Forest algorithm—an ensemble learning method<a data-type="indexterm" data-primary="Random Forest algorithm" data-secondary="about" id="idm45831171915384"></a><a data-type="indexterm" data-primary="training" data-secondary="Scikit-learn" data-tertiary="about Random Forest" id="idm45831171914408"></a><a data-type="indexterm" data-primary="US Census dataset" data-secondary="about Random Forest" id="idm45831171913192"></a> for classification, regression, and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.</p>

<p>You can download the notebook from <a href="https://oreil.ly/Kubeflow_for_ML_ch07_notebook">this book’s GitHub repo</a>.<a data-type="indexterm" data-startref="ch07-sci" id="idm45831171910760"></a><a data-type="indexterm" data-startref="ch07-sci2" id="idm45831171910056"></a></p>








<section data-type="sect2" data-pdf-bookmark="Starting a New Notebook Session"><div class="sect2" id="idm45831171909256" title2="Starting a New Notebook Session" no2="7.4.1">
<h2>7.4.1. Starting a New Notebook Session</h2>

<p>Let’s start by creating a new notebook. Similar to TensorFlow training, you can do this by navigating to the “Notebook Servers” panel in your Kubeflow dashboard, then clicking “New Server” and following the instructions. For this example, we can use the <code>tensorFlow-1.15.2-notebook-cpu:1.0 image</code>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>When working in Kubeflow, an easy way to take advantage of GPU resources to accelerate your Scikit model is to switch to GPU type.</p>
</div>

<p>When the notebook server starts up, click the “Upload” button in the top right corner and upload the <em>IncomePrediction.ipynb</em> file. Click the file to start a new session.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Data Preparation"><div class="sect2" id="idm45831171904680" title2="Data Preparation" no2="7.4.2">
<h2>7.4.2. Data Preparation</h2>

<p>The first few sections of the notebook involve importing libraries and reading the data.<a data-type="indexterm" data-primary="US Census dataset" data-secondary="data preparation" id="ch07-sk3"></a><a data-type="indexterm" data-primary="training" data-secondary="Scikit-learn" data-tertiary="data preparation" id="ch07-sk4"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="Random Forest algorithm" id="ch07-sk"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="US Census dataset" id="ch07-sksk"></a> Then we proceed to feature preparation.<sup><a data-type="noteref" id="idm45831171897544-marker" href="#idm45831171897544">[4]</a></sup> For feature transformation we are using Scikit-learn pipelines. The pipeline makes it easier to feed the model with consistent data.</p>

<p>For our Random Forest training, we need to define ordinal (standardize data) and categorical (one-hot encoding) features, as in <a data-type="xref" href="#Feature_pre_Ex">EXAMPLE 7-19</a>.<a data-type="indexterm" data-primary="Random Forest algorithm" data-secondary="data preparation" id="ch07-rfdp"></a></p>
<div id="Feature_pre_Ex" data-type="example" title2="Feature preparation" no2="7-19">
<h5>Example 7-19. Feature preparation</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">ordinal_features</code> <code class="o">=</code> <code class="p">[</code><code class="n">x</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">feature_names</code><code class="p">))</code>
                <code class="k">if</code> <code class="n">x</code> <code class="ow">not</code> <code class="ow">in</code> <code class="nb">list</code><code class="p">(</code><code class="n">category_map</code><code class="o">.</code><code class="n">keys</code><code class="p">())]</code>
<code class="n">ordinal_transformer</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">(</code><code class="n">steps</code><code class="o">=</code><code class="p">[</code>
    <code class="p">(</code><code class="s1">'imputer'</code><code class="p">,</code>  <code class="n">SimpleImputer</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s1">'median'</code><code class="p">)),</code>
    <code class="p">(</code><code class="s1">'scaler'</code><code class="p">,</code> <code class="n">StandardScaler</code><code class="p">())])</code>

<code class="n">categorical_features</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">category_map</code><code class="o">.</code><code class="n">keys</code><code class="p">())</code>
<code class="n">categorical_transformer</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">(</code><code class="n">steps</code><code class="o">=</code><code class="p">[(</code><code class="s1">'imputer'</code><code class="p">,</code>
    <code class="n">SimpleImputer</code><code class="p">(</code><code class="n">strategy</code><code class="o">=</code><code class="s1">'median'</code><code class="p">)),</code>
    <code class="p">(</code><code class="s1">'onehot'</code><code class="p">,</code> <code class="n">OneHotEncoder</code><code class="p">(</code><code class="n">handle_unknown</code><code class="o">=</code><code class="s1">'ignore'</code><code class="p">))])</code></pre></div>
<div data-type="tip"><h6>Tip</h6>
<p>Many real-world datasets contain missing values, which are encoded by data-specific <a data-type="indexterm" data-primary="data preparation" data-secondary="Scikit-learn and missing data" id="idm45831171888328"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="missing data" data-tertiary="Scikit-learn and" id="idm45831171815720"></a><a data-type="indexterm" data-primary="Scikit-learn" data-secondary="missing data and" id="idm45831171814504"></a>placeholders, such as blanks and NaNs. Such datasets are typically incompatible with Scikit-learn estimators, which assume that all values are numerical. There are multiple strategies to deal with such missing data. One basic strategy would be to discard entire rows and/or columns containing missing values, which comes at the price of losing data. A better strategy is to impute the missing values—to infer them from the known part of the data.
<code>Simple imputer</code> is a Scikit-learn class that allows you to handle the missing data in the predictive model dataset by replacing the NaN values with specified predefined values.</p>
</div>

<p>Once features are defined, we can use a column transformer to combine them, as shown in <a data-type="xref" href="#Comb_cols_using_col_transf">EXAMPLE 7-20</a>.</p>
<div id="Comb_cols_using_col_transf" data-type="example" title2="Combining columns using column transformer" no2="7-20">
<h5>Example 7-20. Combining columns using column transformer</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">preprocessor</code> <code class="o">=</code> <code class="n">ColumnTransformer</code><code class="p">(</code><code class="n">transformers</code><code class="o">=</code><code class="p">[</code>
    <code class="p">(</code><code class="s1">'num'</code><code class="p">,</code> <code class="n">ordinal_transformer</code><code class="p">,</code> <code class="n">ordinal_features</code><code class="p">),</code>
    <code class="p">(</code><code class="s1">'cat'</code><code class="p">,</code> <code class="n">categorical_transformer</code><code class="p">,</code> <code class="n">categorical_features</code><code class="p">)])</code>
<code class="n">preprocessor</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code></pre></div>
<div data-type="tip"><h6>Tip</h6>
<p>Scikit-learn one-hot encoding is used to encode categorical features as a one-hot numeric array.<a data-type="indexterm" data-primary="Scikit-learn" data-secondary="one-hot encoding" id="idm45831171787464"></a><a data-type="indexterm" data-primary="one-hot encoding in Scikit-learn" id="idm45831171786584"></a> The encoder transforms an array of integers or strings, replacing the values by categorical (discrete) features. The features are encoded using a one-hot (aka, one-of-K or dummy) encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the sparse parameter).</p>
</div>

<p>The transformer itself looks like <a data-type="xref" href="#Data_transform_ex">EXAMPLE 7-21</a>.</p>
<div id="Data_transform_ex" data-type="example" title2="Data transformer" no2="7-21">
<h5>Example 7-21. Data transformer</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">ColumnTransformer</code><code class="p">(</code><code class="n">n_jobs</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">remainder</code><code class="o">=</code><code class="s1">'drop'</code><code class="p">,</code> <code class="n">sparse_threshold</code><code class="o">=</code><code class="mf">0.3</code><code class="p">,</code>
  <code class="n">transformer_weights</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code>
  <code class="n">transformers</code><code class="o">=</code><code class="p">[(</code><code class="s1">'num'</code><code class="p">,</code>
    <code class="n">Pipeline</code><code class="p">(</code><code class="n">memory</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code>
      <code class="n">steps</code><code class="o">=</code><code class="p">[</code>
        <code class="p">(</code><code class="s1">'imputer'</code><code class="p">,</code> <code class="n">SimpleImputer</code><code class="p">(</code><code class="n">add_indicator</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code>
          <code class="n">copy</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
          <code class="n">fill_value</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code>
          <code class="n">missing_values</code><code class="o">=</code><code class="n">nan</code><code class="p">,</code>
          <code class="n">strategy</code><code class="o">=</code><code class="s1">'median'</code><code class="p">,</code>
          <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)),</code>
        <code class="p">(</code><code class="s1">'scaler'</code><code class="p">,</code> <code class="n">StandardScaler</code><code class="p">(</code><code class="n">copy</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
          <code class="n">with_mean</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
          <code class="n">with_std</code><code class="o">=</code><code class="bp">True</code><code class="p">))],</code>
        <code class="n">verbose</code><code class="o">=</code><code class="bp">False</code><code class="p">),</code>
      <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">9</code><code class="p">,</code> <code class="mi">10</code><code class="p">]),</code>
    <code class="p">(</code><code class="s1">'cat'</code><code class="p">,</code>
     <code class="n">Pipeline</code><code class="p">(</code><code class="n">memory</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code>
       <code class="n">steps</code><code class="o">=</code><code class="p">[(</code><code class="s1">'imputer'</code><code class="p">,</code> <code class="n">SimpleImputer</code><code class="p">(</code><code class="n">add_indicator</code><code class="o">=</code><code class="bp">False</code><code class="p">,</code>
         <code class="n">copy</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
         <code class="n">fill_value</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code>
         <code class="n">missing_values</code><code class="o">=</code><code class="n">nan</code><code class="p">,</code>
         <code class="n">strategy</code><code class="o">=</code><code class="s1">'median'</code><code class="p">,</code>
         <code class="n">verbose</code><code class="o">=</code><code class="mi">0</code><code class="p">)),</code>
       <code class="p">(</code><code class="s1">'onehot'</code><code class="p">,</code> <code class="n">OneHotEncoder</code><code class="p">(</code><code class="n">categories</code><code class="o">=</code><code class="s1">'auto'</code><code class="p">,</code>
         <code class="n">drop</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code>
         <code class="n">dtype</code><code class="o">=&lt;</code><code class="k">class</code> <code class="err">'</code><code class="nc">numpy</code><code class="o">.</code><code class="n">float64</code><code class="s1">'&gt;,</code>
         <code class="n">handle_unknown</code><code class="o">=</code><code class="s1">'ignore'</code><code class="p">,</code>
         <code class="n">sparse</code><code class="o">=</code><code class="bp">True</code><code class="p">))],</code>
       <code class="n">verbose</code><code class="o">=</code><code class="bp">False</code><code class="p">),</code>
       <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">4</code><code class="p">,</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">11</code><code class="p">])],</code>
    <code class="n">verbose</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code></pre></div>

<p>As a result of this transformation, we have our data in the form of features ready for training.<a data-type="indexterm" data-startref="ch07-sk" id="idm45831171780328"></a><a data-type="indexterm" data-startref="ch07-sk3" id="idm45831171779720"></a><a data-type="indexterm" data-startref="ch07-sk4" id="idm45831171779112"></a><a data-type="indexterm" data-startref="ch07-rfdp" id="idm45831171778472"></a><a data-type="indexterm" data-startref="ch07-sksk" id="idm45831171524216"></a></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Scikit-Learn Training"><div class="sect2" id="idm45831171904056" title2="Scikit-Learn Training" no2="7.4.3">
<h2>7.4.3. Scikit-Learn Training</h2>

<p>Once we have our features prepared we can proceed with the training. Here<a data-type="indexterm" data-primary="US Census dataset" data-secondary="training" id="idm45831171522136"></a><a data-type="indexterm" data-primary="training" data-secondary="Scikit-learn" data-tertiary="running Random Forest" id="idm45831171521160"></a><a data-type="indexterm" data-primary="Random Forest algorithm" data-secondary="running" id="idm45831171519944"></a> we will use <code>RandomForestClassifier</code>, provided by the Scikit-learn library, as shown in <a data-type="xref" href="#Using_RandomForestClass_ex">EXAMPLE 7-22</a>.</p>
<div id="Using_RandomForestClass_ex" data-type="example" title2="Using RandomForestClassifier" no2="7-22">
<h5>Example 7-22. Using RandomForestClassifier</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="n">clf</code> <code class="o">=</code> <code class="n">RandomForestClassifier</code><code class="p">(</code><code class="n">n_estimators</code><code class="o">=</code><code class="mi">50</code><code class="p">)</code>
<code class="n">clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">preprocessor</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">),</code> <code class="n">Y_train</code><code class="p">)</code></pre></div>
<div data-type="tip"><h6>Tip</h6>
<p>The set and specific features of machine learning algorithm(s) is one of the main <a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="framework selection" id="idm45831171481640"></a>drivers behind picking a specific framework for machine learning implementation. Even the same algorithm implementation in different frameworks provides slightly different features that might (or might not) be important for your specific dataset.</p>
</div>

<p>Once prediction is done, we can evaluate training results, as shown in <a data-type="xref" href="#Eval_train_results">EXAMPLE 7-23</a>.<a data-type="indexterm" data-primary="US Census dataset" data-secondary="training with and evaluation" id="idm45831171479032"></a><a data-type="indexterm" data-primary="training" data-secondary="Scikit-learn" data-tertiary="training with and evaluation" id="idm45831171478088"></a></p>
<div id="Eval_train_results" data-type="example" title2="Evaluating training results" no2="7-23">
<h5>Example 7-23. Evaluating training results</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">predict_fn</code> <code class="o">=</code> <code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">clf</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">preprocessor</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">x</code><code class="p">))</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Train accuracy: '</code><code class="p">,</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">Y_train</code><code class="p">,</code> <code class="n">predict_fn</code><code class="p">(</code><code class="n">X_train</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Test accuracy: '</code><code class="p">,</code> <code class="n">accuracy_score</code><code class="p">(</code><code class="n">Y_test</code><code class="p">,</code> <code class="n">predict_fn</code><code class="p">(</code><code class="n">X_test</code><code class="p">)))</code></pre></div>

<p>Which returns the results in <a data-type="xref" href="#Train_results_ex">EXAMPLE 7-24</a>.</p>
<div id="Train_results_ex" data-type="example" title2="Training results" no2="7-24">
<h5>Example 7-24. Training results</h5>

<pre data-type="programlisting" data-code-language="bash">Train accuracy:  0.9655333333333334
Test accuracy:  0.855859375</pre></div>

<p>At this point the model is created and can be directly used by exporting it (see the next section). One of the most important attributes of a model is its explainability.<a data-type="indexterm" data-primary="models" data-secondary="explainability importance" id="idm45831171413944"></a><a data-type="indexterm" data-primary="explaining the model, importance of" id="idm45831171413096"></a><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="explainability importance" id="idm45831171412488"></a><a data-type="indexterm" data-primary="explaining the model" data-secondary="Scikit-learn" id="ch07-exp2"></a><a data-type="indexterm" data-primary="US Census dataset" data-secondary="explaining the model" id="ch07-exp4"></a><a data-type="indexterm" data-primary="training" data-secondary="Scikit-learn" data-tertiary="explaining the model" id="ch07-exp5"></a><a data-type="indexterm" data-primary="model explainability" data-secondary="importance of" id="idm45831171382600"></a><a data-type="indexterm" data-primary="model explainability" data-secondary="Scikit-learn" id="ch07-exxxp"></a><a data-type="indexterm" data-primary="Scikit-learn" data-secondary="explaining the model" id="ch07-exs7"></a> Although model explainability is mostly used in model serving, it is also important for model creation, for two main reasons:</p>

<ul>
<li>
<p>If explainability is important for model serving during model creation, we often need to validate that the model that was created is explainable.</p>
</li>
<li>
<p>Many of the model explanation methods require additional calculations during model creation.</p>
</li>
</ul>

<p>Based on this, we will show how to implement model explainability<sup><a data-type="noteref" id="idm45831171376248-marker" href="#idm45831171376248">[5]</a></sup> during model creation.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Explaining the Model"><div class="sect2" id="idm45831171522952" title2="Explaining the Model" no2="7.4.4">
<h2>7.4.4. Explaining the Model</h2>

<p>For model explanation, we are using anchors, which are part of <a href="https://oreil.ly/VSGxe">Seldon’s Alibi project</a>.</p>

<p>The algorithm provides model-agnostic (black box) and human-interpretable explanations suitable for classification models applied to images, text, and tabular data. The continuous features are discretized into quantiles (e.g., deciles), so they become more interpretable. The features in a candidate anchor are kept constant (same category or bin for discretized features) while we sample the other features from a training set, as in <a data-type="xref" href="#Defining_Tab_Anchor">EXAMPLE 7-25</a>.</p>
<div id="Defining_Tab_Anchor" data-type="example" title2="Defining the tabular anchor" no2="7-25">
<h5>Example 7-25. Defining the tabular anchor</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">explainer</code> <code class="o">=</code> <code class="n">AnchorTabular</code><code class="p">(</code>
    <code class="n">predict_fn</code><code class="p">,</code> <code class="n">feature_names</code><code class="p">,</code> <code class="n">categorical_names</code><code class="o">=</code><code class="n">category_map</code><code class="p">,</code> <code class="n">seed</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="n">explainer</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">disc_perc</code><code class="o">=</code><code class="p">[</code><code class="mi">25</code><code class="p">,</code> <code class="mi">50</code><code class="p">,</code> <code class="mi">75</code><code class="p">])</code></pre></div>

<p>This creates the tabular anchor (<a data-type="xref" href="#Tab_Anchor_ex">EXAMPLE 7-26</a>).</p>
<div id="Tab_Anchor_ex" data-type="example" title2="Tabular anchor" no2="7-26">
<h5>Example 7-26. Tabular anchor</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">AnchorTabular</code><code class="p">(</code><code class="n">meta</code><code class="o">=</code><code class="p">{</code>
    <code class="s1">'name'</code><code class="p">:</code> <code class="s1">'AnchorTabular'</code><code class="p">,</code>
    <code class="s1">'type'</code><code class="p">:</code> <code class="p">[</code><code class="s1">'blackbox'</code><code class="p">],</code>
    <code class="s1">'explanations'</code><code class="p">:</code> <code class="p">[</code><code class="s1">'local'</code><code class="p">],</code>
    <code class="s1">'params'</code><code class="p">:</code> <code class="p">{</code><code class="s1">'seed'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'disc_perc'</code><code class="p">:</code> <code class="p">[</code><code class="mi">25</code><code class="p">,</code> <code class="mi">50</code><code class="p">,</code> <code class="mi">75</code><code class="p">]}</code>
<code class="p">})</code></pre></div>

<p>Now we can get an anchor for the prediction of the first observation in the test set. An <em>anchor</em> is a sufficient condition—that is, when the anchor holds, the prediction should be the same as the prediction for this instance in <a data-type="xref" href="#Prediction_calc_ex">EXAMPLE 7-27</a>.</p>
<div id="Prediction_calc_ex" data-type="example" title2="Prediction calculation" no2="7-27">
<h5>Example 7-27. Prediction calculation</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">idx</code> <code class="o">=</code> <code class="mi">0</code>
<code class="n">class_names</code> <code class="o">=</code> <code class="n">adult</code><code class="o">.</code><code class="n">target_names</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Prediction: '</code><code class="p">,</code> <code class="n">class_names</code><code class="p">[</code><code class="n">explainer</code><code class="o">.</code><code class="n">predictor</code><code class="p">(</code> \
                <code class="n">X_test</code><code class="p">[</code><code class="n">idx</code><code class="p">]</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">))[</code><code class="mi">0</code><code class="p">]])</code></pre></div>

<p class="less_space pagebreak-before">Which returns a prediction calculation result as shown in <a data-type="xref" href="#Prediction_calc_result_ex">EXAMPLE 7-28</a>.</p>
<div id="Prediction_calc_result_ex" data-type="example" title2="Prediction calculation result" no2="7-28">
<h5>Example 7-28. Prediction calculation result</h5>

<pre data-type="programlisting">Prediction:  &lt;=50K</pre></div>

<p>We set the precision threshold to <code>0.95</code>. This means that predictions on observations where the anchor holds will be the same as the prediction on the explained instance at least 95% of the time.
Now we can get an explanation (<a data-type="xref" href="#Model_expl_ex">EXAMPLE 7-29</a>) for this 
prediction.</p>
<div id="Model_expl_ex" data-type="example" title2="Model explanation" no2="7-29">
<h5>Example 7-29. Model explanation</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">explanation</code> <code class="o">=</code> <code class="n">explainer</code><code class="o">.</code><code class="n">explain</code><code class="p">(</code><code class="n">X_test</code><code class="p">[</code><code class="n">idx</code><code class="p">],</code> <code class="n">threshold</code><code class="o">=</code><code class="mf">0.95</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Anchor: </code><code class="si">%s</code><code class="s1">'</code> <code class="o">%</code> <code class="p">(</code><code class="s1">' AND '</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">explanation</code><code class="o">.</code><code class="n">anchor</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Precision: </code><code class="si">%.2f</code><code class="s1">'</code> <code class="o">%</code> <code class="n">explanation</code><code class="o">.</code><code class="n">precision</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Coverage: </code><code class="si">%.2f</code><code class="s1">'</code> <code class="o">%</code> <code class="n">explanation</code><code class="o">.</code><code class="n">coverage</code><code class="p">)</code></pre></div>

<p>Which returns a model explanation result as shown in <a data-type="xref" href="#Model_expl_results_ex">EXAMPLE 7-30</a>.</p>
<div id="Model_expl_results_ex" data-type="example" title2="Model explanation result" no2="7-30">
<h5>Example 7-30. Model explanation result</h5>

<pre data-type="programlisting">Anchor: Marital Status = Separated AND Sex = Female
Precision: 0.95
Coverage: 0.18</pre></div>

<p>This tells us that the main factors for decision are marital status (<code>Separated</code>) and sex (<code>Female</code>).
Anchors might not be found for all points. Let’s try getting an anchor for a different observation in the test set—one for which the prediction is <code>&gt;50K</code>, shown in <a data-type="xref" href="#Model_expl_ex2">EXAMPLE 7-31</a>.</p>
<div id="Model_expl_ex2" data-type="example" title2="Model explanation" no2="7-31">
<h5>Example 7-31. Model explanation</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">idx</code> <code class="o">=</code> <code class="mi">6</code>
<code class="n">class_names</code> <code class="o">=</code> <code class="n">adult</code><code class="o">.</code><code class="n">target_names</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Prediction: '</code><code class="p">,</code> <code class="n">class_names</code><code class="p">[</code><code class="n">explainer</code><code class="o">.</code><code class="n">predictor</code><code class="p">(</code> \
                <code class="n">X_test</code><code class="p">[</code><code class="n">idx</code><code class="p">]</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mi">1</code><code class="p">))[</code><code class="mi">0</code><code class="p">]])</code>

<code class="n">explanation</code> <code class="o">=</code> <code class="n">explainer</code><code class="o">.</code><code class="n">explain</code><code class="p">(</code><code class="n">X_test</code><code class="p">[</code><code class="n">idx</code><code class="p">],</code> <code class="n">threshold</code><code class="o">=</code><code class="mf">0.95</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Anchor: </code><code class="si">%s</code><code class="s1">'</code> <code class="o">%</code> <code class="p">(</code><code class="s1">' AND '</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">explanation</code><code class="o">.</code><code class="n">anchor</code><code class="p">)))</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Precision: </code><code class="si">%.2f</code><code class="s1">'</code> <code class="o">%</code> <code class="n">explanation</code><code class="o">.</code><code class="n">precision</code><code class="p">)</code>
<code class="k">print</code><code class="p">(</code><code class="s1">'Coverage: </code><code class="si">%.2f</code><code class="s1">'</code> <code class="o">%</code> <code class="n">explanation</code><code class="o">.</code><code class="n">coverage</code><code class="p">)</code></pre></div>

<p>Which returns a model explanation result as shown in <a data-type="xref" href="#Model_expl_results_ex2">EXAMPLE 7-32</a>.</p>
<div id="Model_expl_results_ex2" data-type="example" class="less_space pagebreak-before" title2="Model explanation result" no2="7-32">
<h5>Example 7-32. Model explanation result</h5>

<pre data-type="programlisting">Prediction:  &gt;50K
Could not find a result satisfying the 0.95 precision constraint.
Now returning the best non-eligible result.
Anchor: Capital Loss &gt; 0.00 AND Relationship = Husband AND
    Marital Status = Married AND Age &gt; 37.00 AND
    Race = White AND Country = United-States AND Sex = Male
Precision: 0.71
Coverage: 0.05</pre></div>

<p>Due to the imbalanced dataset (roughly 25:75 high:low earner proportion), during the sampling stage feature ranges corresponding to low earners will be oversampled. As a result, the anchor in this case is not found. This is a feature because it can point out an imbalanced dataset, but it can also be fixed by producing balanced datasets to enable anchors to be found for either class.<a data-type="indexterm" data-startref="ch07-exp2" id="idm45831170916376"></a><a data-type="indexterm" data-startref="ch07-exp4" id="idm45831170915672"></a><a data-type="indexterm" data-startref="ch07-exp5" id="idm45831170915000"></a><a data-type="indexterm" data-startref="ch07-mex" id="idm45831170914328"></a><a data-type="indexterm" data-startref="ch07-exxxp" id="idm45831170913656"></a><a data-type="indexterm" data-startref="ch07-exs7" id="idm45831170912984"></a></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Exporting Model"><div class="sect2" id="idm45831171410200" title2="Exporting Model" no2="7.4.5">
<h2>7.4.5. Exporting Model</h2>

<p>In order to use the created model for serving, we need to export the model. This is done using Scikit-learn functionality, as in <a data-type="xref" href="#Exporting_model_ex">EXAMPLE 7-33</a>.<a data-type="indexterm" data-primary="US Census dataset" data-secondary="exporting the model" id="idm45831170910040"></a><a data-type="indexterm" data-primary="training" data-secondary="Scikit-learn" data-tertiary="exporting the model" id="idm45831170909064"></a><a data-type="indexterm" data-primary="models" data-secondary="exporting" data-tertiary="Scikit-learn" id="idm45831170907848"></a><a data-type="indexterm" data-primary="Scikit-learn" data-secondary="exporting the model" id="idm45831170906632"></a></p>
<div id="Exporting_model_ex" data-type="example" title2="Exporting model" no2="7-33">
<h5>Example 7-33. Exporting model</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">dump</code><code class="p">(</code><code class="n">clf</code><code class="p">,</code> <code class="s1">'/tmp/job/income.joblib'</code><code class="p">)</code></pre></div>

<p>This exports a model in Scikit-learn format, that can be used by, for example, Scikit-learn server for inference.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Integration into Pipelines"><div class="sect2" id="idm45831170876680" title2="Integration into Pipelines" no2="7.4.6">
<h2>7.4.6. Integration into Pipelines</h2>

<p>Regardless of which Python-based machine learning library you want to use, if <a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="training integrated into" id="idm45831170875544"></a><a data-type="indexterm" data-primary="training" data-secondary="operators" id="idm45831170874632"></a><a data-type="indexterm" data-primary="training" data-secondary="pipeline integration" id="idm45831170873688"></a>Kubeflow doesn’t have an operator for it, you can simply write your code as normal and then containerize it. To take the notebook we built in this chapter and use it as a pipeline stage, see <a data-type="xref" href="#notebook_as_pipeline_stage">SECTION 5.5</a>. Here we can use <code>file_output</code> to upload the resulting model to our artifact tracking system, but you can also use the persistent volume mechanism.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45831171928792" title2="Conclusion" no2="7.5">
<h1>7.5. Conclusion</h1>

<p>In this chapter, we have taken a look at how to train machine learning models in Kubeflow using two very different frameworks: TensorFlow and Scikit-learn.</p>

<p>We learned how to build a collaborative filtering recommendation system using TensorFlow. We used Kubeflow to create a notebook session, where we’ve prototyped a TensorFlow model with Keras APIs, and then used the TFJob APIs to deploy our training job to a Kubernetes cluster. Finally, we’ve looked at how to use TFJob for distributed training.</p>

<p>We also learned how to train a generic Python model using Scikit-learn, a framework that is not natively supported by Kubeflow.
<a data-type="xref" href="#beyond_tf">CHAPTER 9</a> looks at how to integrate nonsupported non-Python machine learning systems, which is a bit more complicated. While Kubeflow’s first-party training operators can simplify your work, it’s important to remember you aren’t limited by this.</p>

<p>In <a data-type="xref" href="#inference_ch">CHAPTER 8</a> we will look at how to serve the model that we’ve trained in this chapter.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45831173117976"><sup><a href="#idm45831173117976-marker">[1]</a></sup> Currently Kubeflow provides CPU and GPU images with TensorFlow 1.15.2 and 2.1.0, or you can use a custom image.</p><p data-type="footnote" id="idm45831173106920"><sup><a href="#idm45831173106920-marker">[2]</a></sup> The examples in this chapter use TensorFlow 1.15.2. Examples with TensorFlow 2.1.0 can be found on <a href="">this Kubeflow GitHub site</a>.</p><p data-type="footnote" id="idm45831171922664"><sup><a href="#idm45831171922664-marker">[3]</a></sup> The languages currently supported by Jupyter notebooks include Python, R, Julia, and Scala.</p><p data-type="footnote" id="idm45831171897544"><sup><a href="#idm45831171897544-marker">[4]</a></sup> See <a data-type="xref" href="#data_and_feature_prep">CHAPTER 5</a> for an in-depth discussion of feature preparation.</p><p data-type="footnote" id="idm45831171376248"><sup><a href="#idm45831171376248-marker">[5]</a></sup> Refer to this <a href="">blog post by Rui Aguiar</a> for more information on model explainability.</p></div></div></section>
<section data-type="chapter" data-pdf-bookmark="Chapter 8. Model Inference"><div class="chapter" id="inference_ch" data-type="chapter" title2="Model Inference" no2="8">
<h1>Chapter 8. Model Inference</h1>

<div data-type="note"><h6>Note</h6>
<p>We would like to acknowledge Clive Cox and Alejandro Saucedo from <a href="https://www.seldon.io">Seldon</a> for their great contributions to this chapter.</p>
</div>

<p>Most of the attention paid to machine learning has been devoted to algorithm development.
However, models are not created for the sake of their creation, they are created to be put into production.
Usually when people talk about taking a model “to production,” they mean performing inference.<a data-type="indexterm" data-primary="model inference" data-secondary="about" id="idm45831170883480"></a>
As introduced in <a data-type="xref" href="#who_is_kubeflow_for_ch">CHAPTER 1</a> and illustrated in <a data-type="xref" href="#mdlc_figure">FIGURE 1-1</a>, a complete inference solution seeks to provide serving, monitoring, and updating functionality.</p>
<dl>
<dt>Model serving</dt>
<dd>
<p>Puts a trained model behind a service that can handle prediction requests</p>
</dd>
<dt>Model monitoring</dt>
<dd>
<p>Monitors the model server for any irregularities in performance—as well as the underlying model’s accuracy</p>
</dd>
<dt>Model updating</dt>
<dd>
<p>Fully manages the versioning of your models and simplifies the promotion and rollback between versions</p>
</dd>
</dl>

<p>This chapter will explore each of these core components and define expectations for their functionality.
Given concrete expectations, we will establish a list of requirements that your ideal inference solution will satisfy.
Lastly, we will discuss Kubeflow-supported inference offerings and how you can use them to satisfy your inference requirements.</p>






<section data-type="sect1" data-pdf-bookmark="Model Serving"><div class="sect1" id="idm45831170854472" title2="Model Serving" no2="8.1">
<h1>8.1. Model Serving</h1>

<p>The first step of model inference is model serving, which is hosting your model behind a service that you can interface with.<a data-type="indexterm" data-primary="model inference" data-secondary="model serving" id="idm45831170852536"></a><a data-type="indexterm" data-primary="model serving" data-secondary="about" id="idm45831170851560"></a>
Two fundamental approaches to model serving are <em>embedded</em>, where<a data-type="indexterm" data-primary="model serving" data-secondary="embedded" id="idm45831170850104"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model serving" data-tertiary="embedded" id="idm45831170849096"></a><a data-type="indexterm" data-primary="model serving" data-secondary="model serving as a service" id="ch08-maas"></a><a data-type="indexterm" data-primary="MaaS (model serving as a service)" id="ch08-maas3"></a><a data-type="indexterm" data-primary="model serving as a service (MaaS)" id="ch08-maas2"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model serving" data-tertiary="model serving as a service" id="ch08-infyyz"></a> the models are deployed directly into the application, and <em>model serving as a service</em> (MaaS), where a separate service dedicated to model serving can be used from any application in the enterprise.
<a data-type="xref" href="#comparing_embedded_with_model_serving_as_a_service">TABLE 8-1</a> provides a comparison of these approaches.</p>
<table id="comparing_embedded_with_model_serving_as_a_service" data-type="table" title2="Comparing embedded with MaaS" no2="8-1">
<caption>Table 8-1. Comparing embedded with MaaS</caption>
<thead>
<tr>
<th>Serving types</th>
<th>Advantages</th>
<th>Disadvantages</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Embedded</p></td>
<td><div>
<ul>
<li>
<p>Delivers maximum performance</p>
</li>
<li>
<p>Features the simplest infrastructure</p>
</li>
<li>
<p>No need to plan for aberrant user behavior</p>
</li>
</ul></div></td>
<td><div>
<ul>
<li>
<p>Model has to be deployed in every application using it</p>
</li>
<li>
<p>Application updates are required when the model type changes</p>
</li>
<li>
<p>All deployment strategies, for example blue-green, must be explicitly implemented</p>
</li>
</ul></div></td>
</tr>
<tr>
<td><p>MaaS</p></td>
<td><div>
<ul>
<li>
<p>Simplifies integration with other technologies and organizational processes</p>
</li>
<li>
<p>Reuses model deployment across multiple stream-processing applications</p>
</li>
<li>
<p>Allows model serving on lower-power devices (e.g., phones)
incapable of running complex models</p>
</li>
<li>
<p>Enables mini-batching for requests from multiple clients</p>
</li>
<li>
<p>Makes it easier to provide built-in capabilities, including model updates, explainability, drift detection, etc.</p>
</li>
<li>
<p>Enables advanced model deployment strategies like ensembles and multi-armed bandit, which require decoupling from application</p>
</li>
<li>
<p>Allows for separate scaling between application and model server, or running them on different devices like CPU and GPU</p>
</li>
</ul></div></td>
<td><div>
<ul>
<li>
<p>Additional network hops decrease performance</p>
</li>
<li>
<p>Tight temporal coupling to the model server can impact overall service-level agreement</p>
</li>
</ul></div></td>
</tr>
</tbody>
</table>

<p>Kubeflow only supports a MaaS approach. As a result, we will not be discussing model embedding in this book.<sup><a data-type="noteref" id="idm45831170778040-marker" href="#idm45831170778040">[1]</a></sup></p>

<p>There are two main approaches for implementing MaaS: <em>model as code</em>, and <em>model as data</em>.<a data-type="indexterm" data-primary="model inference" data-secondary="as code Maas" id="idm45831170774920"></a><a data-type="indexterm" data-primary="model as data MaaS" id="idm45831170773912"></a>
Model as code uses model code directly in a service’s implementation.
Model as data uses a generic implementation that is driven by a model in an intermediate model format like <a href="https://oreil.ly/ljhYw">PMML</a>,  <a href="https://oreil.ly/SsM9C">PFA</a>, <a href="https://onnx.ai">ONNX</a>, or <a href="https://oreil.ly/KtkQS">TensorFlow’s native format</a>.
Both approaches are used in different model server implementations in Kubeflow.
When determining which implementation to use, we recommended using model as data, as it allows for the exchange of models between serving instances to be standardized,
thus providing portability across systems and the enablement of generic model serving solutions.</p>

<p>Most common serving implementations, like TFServing, ONNX Runtime, Triton, and TorchServe, use a model-as-data approach and leverage an intermediate model format.
Some of these implementations support only one framework, while others support multiple.
Unfortunately, each of these solutions uses different model formats and exposes unique proprietary serving APIs.<a data-type="indexterm" data-primary="MaaS (model serving as a service)" data-secondary="APIs" id="idm45831170769096"></a>
None of these interfaces meet everyone’s needs.
The complexity and divergence of these API interfaces result in a differing UX and an inability to share features effectively.
Furthermore, there is increased friction in swapping between model frameworks, as the interfaces behind these implementations are different.</p>

<p>There are a few strong industry players attempting to unify the open source community of model servers and decrease the friction between toggling model frameworks.
Seldon is pioneering graph inferencing with Seldon Core; Bloomberg and IBM are investigating serverless model serving using solutions like Knative; and Google is further hardening its serving implementation for TensorFlow models.</p>

<p>In <a data-type="xref" href="#inference_in_kubeflow">SECTION 8.5</a>, we will discuss the serving solutions that Kubeflow offers and the work that has been done to unify these solutions into a single interface.<a data-type="indexterm" data-startref="ch08-maas" id="idm45831170765400"></a><a data-type="indexterm" data-startref="ch08-maas2" id="idm45831170764728"></a><a data-type="indexterm" data-startref="ch08-infyyz" id="idm45831170764056"></a><a data-type="indexterm" data-startref="ch08-maas3" id="idm45831170763384"></a></p>








<section data-type="sect2" data-pdf-bookmark="Model Serving Requirements"><div class="sect2" id="idm45831170762584" title2="Model Serving Requirements" no2="8.1.1">
<h2>8.1.1. Model Serving Requirements</h2>

<p>Model serving requires you to understand and manage the developmental <a data-type="indexterm" data-primary="model serving" data-secondary="requirements" id="ch08-req"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model serving requirements" id="ch08-infz"></a>operations (DevOps) and handle the analysis, experimentation, and governance of your models.
This scope is wide, complicated, and universal among data scientists.
We will now start scoping out the expectations you might want from a serving solution.</p>

<p>First, you want framework flexibility.
Solutions like Kubeflow allow for your training to be implementation-agnostic (i.e., TensorFlow versus PyTorch).
If you write an image classification inference service,
it should not matter if the underlying model was trained using PyTorch, Scikit-learn, or TensorFlow—the service interface should be shared so that the user’s API remains consistent.</p>

<p>Second, you want the ability to leverage hardware optimizers that match the needs of the algorithm.
Sometimes fully fitted and tuned neural nets are quite deep, which means that even in the evaluation phase, you would benefit from hardware optimizers like GPUs or TPUs to infer the models.</p>

<p>Third, your model server should seamlessly interact with other components in an inference graph.
An inference graph could comprise feature transformers, predictors, explainers, and drift detectors—all of which we will cover later.</p>

<p>Fourth, you should also have options to scale your serving instance, both explicitly and using autoscalers, regardless of the underlying hardware—i.e., cost per inference, latency.
This is particularly important and difficult because GPU autoscaling relies on a combination of factors including: GPU/CPU utilization metrics, duty cycles, and more, and knowing which metric to use for autoscaling is not obvious.
Also, the scaling of each of the components in your inference graph should be done separately due to differing algorithmic needs.</p>

<p>Fifth, you want a serving instance that exposes representational state transfer (REST) requests or general-purpose remote procedure calls (gRPC).
If you have streaming inputs, you may want to support a streaming interface like Kafka.<a data-type="indexterm" data-startref="ch08-req" id="idm45831170754376"></a><a data-type="indexterm" data-startref="ch08-infz" id="idm45831170753672"></a></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Model Monitoring"><div class="sect1" id="Model_Monitor" title2="Model Monitoring" no2="8.2">
<h1>8.2. Model Monitoring</h1>

<p>Once you have a model served, you must monitor the model server in production.<a data-type="indexterm" data-primary="model inference" data-secondary="model monitoring" id="idm45831170751192"></a><a data-type="indexterm" data-primary="model serving" data-secondary="monitoring" id="idm45831170750216"></a><a data-type="indexterm" data-primary="monitoring" data-secondary="about" id="idm45831170749272"></a><a data-type="indexterm" data-primary="models" data-secondary="monitoring" id="idm45831170748328"></a>
When we are talking about monitoring of the model server, we are talking not only about model serving insights but also about general monitoring used for any Kubernetes-based applications, including memory, CPU, networking, etc.
We will explore model monitoring and model insight in more detail in <a data-type="xref" href="#Monitor_yr_Models">SECTION 8.7.4</a>.</p>








<section data-type="sect2" data-pdf-bookmark="Model Accuracy, Drift, and Explainability"><div class="sect2" id="idm45831170746008" title2="Model Accuracy, Drift, and Explainability" no2="8.2.1">
<h2>8.2.1. Model Accuracy, Drift, and Explainability</h2>

<p>In generating model serving insights, the most common ML attributes to monitor are <a data-type="indexterm" data-primary="models" data-secondary="validation" id="idm45831170744472"></a><a data-type="indexterm" data-primary="validation of models" data-secondary="model accuracy" id="idm45831170743144"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model monitoring" data-tertiary="accuracy, drift, explainability" id="idm45831170742200"></a><a data-type="indexterm" data-primary="model drift" id="idm45831170740968"></a><a data-type="indexterm" data-primary="model drift" id="idm45831170740296"></a><a data-type="indexterm" data-primary="model inference" data-secondary="accuracy" id="idm45831170739624"></a><a data-type="indexterm" data-primary="explaining the model, importance of" id="idm45831170738680"></a><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="explainability importance" id="idm45831170737992"></a><a data-type="indexterm" data-primary="model explainability" data-secondary="importance of" id="idm45831170737032"></a><a data-type="indexterm" data-primary="model explainability" data-secondary="Scikit-learn" id="idm45831170736088"></a><a data-type="indexterm" data-primary="monitoring" data-secondary="model serving" id="idm45831170735144"></a>model accuracy, model drift, and explainability.
<em>Model accuracy</em> refers to the validation accuracy of your training data.
But as live data distributions begin to deviate from those of the original training data, this tends to result in <em>model drift</em>.
In other words, model drift occurs when the feature distribution of the data sent to the model begins to significantly differ from the
data used to train the model, causing the model to perform suboptimally.
ML insight systems implement effective techniques for analyzing and<a data-type="indexterm" data-primary="concept drift" id="idm45831170732728"></a> detecting changes—<em>concept drift</em>—that might happen to your input data,
and the detection of these drifts is critical for models running in production systems.</p>

<p>Another form of model insight that is increasingly gaining attention today is <em>model explainability</em>, or the ability to explain why a certain result was produced.
More precisely, it answers:</p>

<ul class="less_space pagebreak-before">
<li>
<p>What features in the data did the model think are most important?</p>
</li>
<li>
<p>For any single prediction from a model, how did each feature in the data affect that particular prediction?</p>
</li>
<li>
<p>What interactions between features have the greatest effects on a model’s 
predictions?</p>
</li>
</ul>

<p>Beyond model insight, application monitoring traditionally relates to network observability, or telemetry, the enablement of log aggregation, and service-mesh-related metrics collection.
These tools are useful in capturing data from a live serving instance.
This infrastructure exposes enough queryable information for troubleshooting and alerting, should things go awry regarding reachability, utilization, or latency.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Model Monitoring Requirements"><div class="sect2" id="idm45831170725448" title2="Model Monitoring Requirements" no2="8.2.2">
<h2>8.2.2. Model Monitoring Requirements</h2>

<p>Monitoring model accuracy and model drift is hard. Luckily, this is a very active <a data-type="indexterm" data-primary="models" data-secondary="monitoring" data-tertiary="requirements" id="idm45831170723928"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model monitoring" data-tertiary="requirements" id="idm45831170722680"></a><a data-type="indexterm" data-primary="model serving" data-secondary="monitoring" data-tertiary="requirements" id="idm45831170721464"></a><a data-type="indexterm" data-primary="monitoring" data-secondary="requirements" id="idm45831170720248"></a><a data-type="indexterm" data-primary="monitoring" data-secondary="resources on" id="idm45831170719304"></a><a data-type="indexterm" data-primary="model serving" data-secondary="monitoring" data-tertiary="resources on" id="idm45831170718360"></a><a data-type="indexterm" data-primary="models" data-secondary="monitoring" data-tertiary="resources on" id="idm45831170717144"></a>research space with a variety of open source solutions.<sup><a data-type="noteref" id="idm45831170715736-marker" href="#idm45831170715736">[2]</a></sup>
Your inference solution should enable you to plug in solutions that provide your desired functionality out of the box.
Now, we will see what you may wish to have from your model monitoring component.</p>

<p>First, you want your inference service to provide ML insight out of the box and run in a microservice-based architecture in order to simplify the experimentation of drift detection and model explanation solutions.</p>

<p>Second, you want to enable the monitoring, logging, and tracing of your service.
It should also support solutions like Prometheus, Kibana, and Zipkin, respectively, but then also be able to seamlessly support their alternatives.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Model Updating"><div class="sect1" id="idm45831170710472" title2="Model Updating" no2="8.3">
<h1>8.3. Model Updating</h1>

<p>If you wish to update your model and roll out a newer version or roll back to a previous version,<a data-type="indexterm" data-primary="model serving" data-secondary="model updating" id="idm45831170708856"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model updating" id="idm45831170707880"></a><a data-type="indexterm" data-primary="models" data-secondary="updating" id="idm45831170706936"></a><a data-type="indexterm" data-primary="model inference" data-secondary="updating models" id="idm45831170705992"></a> you will want to deploy and run this updated version. However, the relationship between your current deployment and the new deployment can be defined in a variety of ways.
When your inference system introduces multiple versions of your model serving instance, you can use either shadow or competing models:</p>
<dl class="less_space pagebreak-before">
<dt>Shadow models</dt>
<dd>
<p>These are useful when considering the replacement of a model in production.<a data-type="indexterm" data-primary="shadow models" id="idm45831170702840"></a>
You can deploy the new model alongside the current one and send the same production traffic to gather data on how the shadow model performs before promoting it.</p>
</dd>
<dt>Competing models</dt>
<dd>
<p>These are a slightly more complex scenario, where you are trying multiple<a data-type="indexterm" data-primary="competing models" id="idm45831170700456"></a> versions of a model in production to find out which one is better through tools like A/B testing.</p>
</dd>
</dl>

<p>Let’s discuss the three main deployment strategies:</p>
<dl>
<dt><a href="https://oreil.ly/pXHA4">Blue-green deployments</a></dt>
<dd>
<p>These reduce downtime and risk relating to version rollouts by having only<a data-type="indexterm" data-primary="blue-green deployment" id="idm45831170696584"></a> one live environment, which serves all production traffic.</p>
</dd>
<dt><a href="https://oreil.ly/BOEQi">Canary deployments</a></dt>
<dd>
<p>These enable rollout releases by allowing you to do percentage-based traffic between versions.<a data-type="indexterm" data-primary="canary deployment" id="idm45831170693880"></a></p>
</dd>
<dt>Pinned deployments</dt>
<dd>
<p>These allow you to expose experimental traffic to a newer version, while keeping<a data-type="indexterm" data-primary="pinned deployments" id="idm45831170691896"></a> production traffic against the current version.</p>
</dd>
</dl>

<p>The added complexity of canary and pinned over blue-green comes from the infrastructure and routing rules required to ensure that traffic is being redirected to the right models.
With this enablement, you can then gather data to make statistically significant decisions about when to start moving traffic.
One statistical approach for traffic movement is A/B testing.<a data-type="indexterm" data-primary="A/B testing" id="idm45831170690008"></a><a data-type="indexterm" data-primary="models" data-secondary="evaluating competing" id="idm45831170689304"></a>
Another popular approach for evaluating multiple competing models is <a href="https://oreil.ly/eDEsU">multi-armed bandits</a>,
which requires you to define a score or reward for each model and to promote models relative to their respective score.</p>








<section data-type="sect2" data-pdf-bookmark="Model Updating Requirements"><div class="sect2" id="idm45831170687224" title2="Model Updating Requirements" no2="8.3.1">
<h2>8.3.1. Model Updating Requirements</h2>

<p>Upgrading your model must be simple, so the deployment strategy that you<a data-type="indexterm" data-primary="model serving" data-secondary="model updating" data-tertiary="requirements" id="idm45831170685816"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model updating" data-tertiary="requirements" id="idm45831170684536"></a><a data-type="indexterm" data-primary="models" data-secondary="updating" data-tertiary="requirements" id="idm45831170683320"></a> use for upgrading should be easy to configure and simple to change (i.e., from pinned to canary).
Your inference solution should also offer more-complex graph inferencing in its design.
We will elaborate on what you need from your inference solution:</p>

<p class="less_space pagebreak-before">First, the toggle of deployment strategies—i.e., from pinned to canary—should be trivial. You can enable traffic-level routing in an abstracted way by abstracting the service plane, which will be defined in <a data-type="xref" href="#serverless_service_plane">SECTION 8.8.1</a>.</p>

<p>Second, version changes should be tested and validated before promotion, and the corresponding upgrade should be logged.</p>

<p>Third, the underlying stack should enable you to configure the more complex deployment strategies common to graph inferencing literature.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary of Inference Requirements"><div class="sect1" id="idm45831170678696" title2="Summary of Inference Requirements" no2="8.4">
<h1>8.4. Summary of Inference Requirements</h1>

<p>With the requirements of model serving, monitoring, and updating all satisfied, you now have an inference solution that completes your model development life cycle (MDLC) story.<a data-type="indexterm" data-primary="model development life cycle (MDLC)" id="idm45831170677112"></a><a data-type="indexterm" data-primary="model inference" data-secondary="about" id="idm45831170676440"></a>
This enables you to bring a model all the way from lab to production, and even handle the updating of this model should you want to tune or modify its construction.
Now we will discuss the inference solutions that Kubeflow offers.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Some ML practitioners believe that continuous learning (CL) is fundamental in their production ML systems.<a data-type="indexterm" data-primary="models" data-secondary="continuous learning" id="idm45831170673928"></a><a data-type="indexterm" data-primary="continuous learning (CL)" id="idm45831170672952"></a><a data-type="indexterm" data-primary="model inference" data-secondary="continuous learning" id="idm45831170672264"></a><a data-type="indexterm" data-primary="AutoML (automated machine learning)" data-secondary="continuous learning as" id="idm45831170671320"></a><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="AutoML" data-tertiary="continuous learning as" id="idm45831170670360"></a>
CL is the ability of a model to learn continually from streaming data.
In essence, the model will autonomously learn and adapt in production as new data comes in. Some even call this AutoML.
With a complete MDLC solution that enables pipelines and canary deployments, you can design such a system using the tools available in Kubeflow.</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Model Inference in Kubeflow"><div class="sect1" id="inference_in_kubeflow" title2="Model Inference in Kubeflow" no2="8.5">
<h1>8.5. Model Inference in Kubeflow</h1>

<p>Model serving, monitoring, and updating within inference can be quite tricky because<a data-type="indexterm" data-primary="model inference" data-secondary="Kubeflow model inference" id="idm45831170666536"></a>
you need a solution that manages all of these expectations in a way that provides abstraction for first-time users and customizability for power users.</p>

<p>Kubeflow provides many <a href="https://oreil.ly/GXjL4">options</a> for model inference solutions.
In this section, we will describe some of them, including TensorFlow Serving, Seldon Core, and KFServing.<a data-type="indexterm" data-primary="model inference" data-secondary="TensorFlow Serving" data-tertiary="comparison chart" id="idm45831170663784"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="comparison chart" id="idm45831170662504"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="comparison chart" id="idm45831170661288"></a><a data-type="indexterm" data-primary="TensorFlow Serving (TFServing)" data-secondary="comparison chart" id="idm45831170660072"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="comparison chart" id="idm45831170659160"></a><a data-type="indexterm" data-primary="Seldon Core" data-secondary="comparison chart" id="idm45831170658216"></a>
<a data-type="xref" href="#comparing_different_model_inference_approaches">TABLE 8-2</a> presents a quick comparison of these 
solutions.</p>
<table id="comparing_different_model_inference_approaches" class="less_space pagebreak-before" data-type="table" title2="Comparing different model inference approaches" no2="8-2">
<caption>Table 8-2. Comparing different model inference approaches</caption>
<thead>
<tr>
<th>Solution</th>
<th>Approach</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>TensorFlow Serving</p></td>
<td><div>
<ul>
<li>
<p>Single model type (TensorFlow) support</p>
</li>
<li>
<p>Some support for monitoring metrics (Prometheus)</p>
</li>
<li>
<p>With version 2.3, support for canarying via model version labels</p>
</li>
<li>
<p>Simplest infrastructure dependencies</p>
</li>
</ul></div></td>
</tr>
<tr>
<td><p>Seldon Core</p></td>
<td><div>
<ul>
<li>
<p>Optimized Docker containers for popular libraries like TensorFlow, H2O, XGBoost, MXNet, etc.</p>
</li>
<li>
<p>Language wrappers that convert a Python file or a Java JAR into a fully fledged microservice</p>
</li>
<li>
<p>Support for inference pipelines that can consist of models, transformers, combiners and routers</p>
</li>
<li>
<p>Support for monitoring metrics and auditable request logs</p>
</li>
<li>
<p>Support for advanced deployment techniques—canary, blue-green, etc.</p>
</li>
<li>
<p>Support for advanced ML insights: explainers, outlier detectors, and adversarial attack detectors</p>
</li>
<li>
<p>More complex infrastructure dependencies</p>
</li>
</ul></div></td>
</tr>
<tr>
<td><p>KFServing</p></td>
<td><div>
<ul>
<li>
<p>Adding serverless (Knative) and a standardized inference experience to Seldon Core, while providing extensibility for other model servers</p>
</li>
<li>
<p>Most complex infrastructure dependencies</p>
</li>
</ul></div></td>
</tr>
</tbody>
</table>
</div></section>













<section data-type="sect1" data-pdf-bookmark="TensorFlow Serving"><div class="sect1" id="idm45831170633000" title2="TensorFlow Serving" no2="8.6">
<h1>8.6. TensorFlow Serving</h1>

<p>One of the most popular serving implementations is<a data-type="indexterm" data-primary="model inference" data-secondary="TensorFlow Serving" id="ch08-tfs"></a><a data-type="indexterm" data-primary="TensorFlow Serving (TFServing)" data-secondary="inference" id="ch08-tfs2"></a><a data-type="indexterm" data-primary="TensorFlow Serving (TFServing)" data-secondary="about" id="idm45831170628392"></a><a data-type="indexterm" data-primary="model inference" data-secondary="TensorFlow Serving" data-tertiary="about" id="idm45831170627480"></a> <a href="https://oreil.ly/AV0jU">TensorFlow Serving</a> (TFServing), a model-serving implementation based on the <a href="https://oreil.ly/WOV9j">TensorFlow export format</a>.
TFServing implements a flexible, high-performance serving system for ML models, designed for production environments. The TFServing architecture is shown in <a data-type="xref" href="#tfserving_figure">FIGURE 8-1</a>.</p>

<figure><div id="tfserving_figure" class="figure" data-type="figure" title2="TFServing architecture" no2="8-1">
<img src="assets/kfml_0801.png" alt="TFServing architecture" width="1414" height="467">
<h6>Figure 8-1. TFServing architecture</h6>
</div></figure>

<p>TFServing uses exported TensorFlow models as inputs and supports running predictions on them using HTTP or gRPC.
TFServing can be <a href="https://oreil.ly/O8oE-">configured</a> to use either:</p>

<ul class="less_space pagebreak-before">
<li>
<p>A single (latest) version of the model</p>
</li>
<li>
<p>Multiple, specific versions of the model</p>
</li>
</ul>

<p>TensorFlow can be used both locally<sup><a data-type="noteref" id="idm45831170617576-marker" href="#idm45831170617576">[3]</a></sup> and in Kubernetes.<sup><a data-type="noteref" id="idm45831170616056-marker" href="#idm45831170616056">[4]</a></sup> A typical TFServing implementation within Kubeflow includes the following components:</p>

<ul>
<li>
<p>A Kubernetes deployment running the required amount of replicas</p>
</li>
<li>
<p>A Kubernetes service providing access to the deployment</p>
</li>
<li>
<p>An Istio virtual service that exposes the service through the Istio ingress gateway</p>
</li>
<li>
<p>An Istio <code>DestinationRule</code> that defines policies for traffic routed to the service (These rules can specify configurations for load balancing, connection pool size, and outlier detection settings so that you can detect and evict unhealthy hosts from the load balancing pool.)</p>
</li>
</ul>

<p>We will walk through an example of how these components <a href="https://oreil.ly/copcG">are implemented</a> by extending our recommender example.<a data-type="indexterm" data-primary="model inference" data-secondary="TensorFlow Serving" data-tertiary="recommendation system" id="ch08-myyz"></a><a data-type="indexterm" data-primary="TensorFlow Serving (TFServing)" data-secondary="recommendation system" id="ch08-tfsrec"></a><a data-type="indexterm" data-primary="model inference" data-secondary="TensorFlow recommender deployment" data-tertiary="with TensorFlow Serving" id="ch08-tfsrec2"></a><a data-type="indexterm" data-primary="recommendation systems" data-secondary="TensorFlow" data-tertiary="deployment with TFServing" id="ch08-tfsrec3"></a><a data-type="indexterm" data-primary="TensorFlow" data-secondary="recommender" data-tertiary="deployment with TFServing" id="ch08-tfsrec4"></a><a data-type="indexterm" data-primary="training" data-secondary="TensorFlow recommender" data-tertiary="deployment with TFServing" id="ch08-mzzzy"></a>
To simplify your initial inference service, your example TFServing instance will be scoped to a deployment and a service that enables HTTP access.
The Helm chart for this example can be found in  <a href="https://oreil.ly/Kubeflow_for_ML_ch08_Helm">the GitHub repo for this book</a>.</p>

<p>The chart defines a Kubernetes deployment and service.
The deployment uses the “standard” TFServing Docker image and, in its configuration spec, points to a serialized model at an <code>S3</code> source location.
This <code>S3</code> bucket is managed by a local MinIO instance.
The service exposes this deployment inside the Kubernetes cluster.</p>

<p>The chart can be deployed using the following command (assuming you are running Helm 3):</p>

<pre data-type="programlisting" data-code-language="bash" id="untitled_programlisting_14" title2="(no caption)" no2="">helm install &lt;chart_location&gt;</pre>

<p>Now that you have the chart deployed, you need a way to interface with your inference solution.
One method is to <a href="https://oreil.ly/jWjfV">port forward</a> your service, so that the traffic can be redirected to your localhost for testing.
You can port-forward your service with <a data-type="xref" href="#Portforward_TFServ_servs">EXAMPLE 8-1</a>.</p>
<div id="Portforward_TFServ_servs" data-type="example" title2="Port-forwarding TFServing services" no2="8-1">
<h5>Example 8-1. Port-forwarding TFServing services</h5>

<pre data-type="programlisting" data-code-language="bash">kubectl port-forward service/recommendermodelserver 8501:8501</pre></div>

<p class="less_space pagebreak-before">The resulting traffic will be rerouted to <code>localhost:8051</code>.</p>

<p>You are now ready to interact with your TFServing inference solution. To start, you should validate the deployment by requesting model deployment information from your service:</p>

<pre data-type="programlisting" data-code-language="bash" id="untitled_programlisting_15" title2="(no caption)" no2="">curl http://localhost:8501/v1/models/recommender/versions/1</pre>

<p>The expected output is shown in <a data-type="xref" href="#TFServRec_model_version_status">EXAMPLE 8-2</a>.</p>
<div id="TFServRec_model_version_status" data-type="example" title2="TFServing Recommender model version status" no2="8-2">
<h5>Example 8-2. TFServing Recommender model version status</h5>

<pre data-type="programlisting" data-code-language="json"><code class="p">{</code>
 <code class="nt">"model_version_status"</code><code class="p">:</code> <code class="p">[</code>
  <code class="p">{</code>
   <code class="nt">"version"</code><code class="p">:</code> <code class="s2">"1"</code><code class="p">,</code>
   <code class="nt">"state"</code><code class="p">:</code> <code class="s2">"AVAILABLE"</code><code class="p">,</code>
   <code class="nt">"status"</code><code class="p">:</code> <code class="p">{</code>
    <code class="nt">"error_code"</code><code class="p">:</code> <code class="s2">"OK"</code><code class="p">,</code>
    <code class="nt">"error_message"</code><code class="p">:</code> <code class="s2">""</code>
   <code class="p">}</code>
  <code class="p">}</code>
 <code class="p">]</code>
<code class="p">}</code></pre></div>

<p>You can also get the model’s metadata, including its signature definition, by issuing the following curl command:</p>

<pre data-type="programlisting" data-code-language="bash" id="untitled_programlisting_16" title2="(no caption)" no2="">curl http://localhost:8501/v1/models/recommender/versions/1/metadata</pre>

<p>Now that your model is available and has the correct signature definition, you can predict against the service with the command seen in <a data-type="xref" href="#req_TFServ_Rec_service">EXAMPLE 8-3</a>.</p>
<div id="req_TFServ_Rec_service" data-type="example" title2="Sending a request to your TFServing Recommender service" no2="8-3">
<h5>Example 8-3. Sending a request to your TFServing Recommender service</h5>

<pre data-type="programlisting" data-code-language="bash">curl -X POST http://localhost:8501/v1/models/recommender/versions/1:predict<code class="se">\</code>
-d <code class="s1">'{"signature_name":"serving_default","inputs":\</code>
<code class="s1">{"products": [[1],[2]],"users" : [[25], [3]]}}'</code></pre></div>

<p>The result from executing <a data-type="xref" href="#req_TFServ_Rec_service">EXAMPLE 8-3</a> is shown in <a data-type="xref" href="#Output_fr_TFServ_Rec_service">EXAMPLE 8-4</a>.</p>
<div id="Output_fr_TFServ_Rec_service" data-type="example" title2="Output from your TFServing Recommender service" no2="8-4">
<h5>Example 8-4. Output from your TFServing Recommender service</h5>

<pre data-type="programlisting" data-code-language="json"><code class="p">{</code>
    <code class="nt">"outputs"</code><code class="p">:</code> <code class="p">{</code>
        <code class="nt">"model-version"</code><code class="p">:</code> <code class="p">[</code>
            <code class="s2">"1"</code>
        <code class="p">],</code>
        <code class="nt">"recommendations"</code><code class="p">:</code> <code class="p">[</code>
            <code class="p">[</code>
                <code class="mf">0.140973762</code>
            <code class="p">],</code>
            <code class="p">[</code>
                <code class="mf">0.0441606939</code>
            <code class="p">]</code>
        <code class="p">]</code>
    <code class="p">}</code>
<code class="p">}</code></pre></div>

<p>Your TensorFlow model is now behind a live inference solution.
TFServing makes it easy to deploy new TensorFlow algorithms and experiments, while keeping the same server architecture and APIs.
But the journey does not end there.
For one, these deployment instructions create a service but do not enable access from outside of the cluster.<sup><a data-type="noteref" id="idm45831170410888-marker" href="#idm45831170410888">[5]</a></sup>
But we will now take a further look into all the capabilities of this particular solution against your inference requirements.</p>








<section data-type="sect2" data-pdf-bookmark="Review"><div class="sect2" id="idm45831170390264" title2="Review" no2="8.6.1">
<h2>8.6.1. Review</h2>

<p>If you are looking to deploy your TensorFlow model with the lowest infrastructure requirement, TFServing is your solution.
However, this has limitations when you consider your inference requirements.<a data-type="indexterm" data-primary="TFServing" data-see="TensorFlow Serving" id="idm45831170388488"></a><a data-type="indexterm" data-startref="ch08-tfs" id="idm45831170387512"></a><a data-type="indexterm" data-startref="ch08-tfs2" id="idm45831170386840"></a><a data-type="indexterm" data-startref="ch08-tfsrec" id="idm45831170386168"></a><a data-type="indexterm" data-startref="ch08-tfsrec2" id="idm45831170385496"></a><a data-type="indexterm" data-startref="ch08-tfsrec3" id="idm45831170384824"></a><a data-type="indexterm" data-startref="ch08-tfsrec4" id="idm45831170384152"></a><a data-type="indexterm" data-startref="ch08-myyz" id="idm45831170383480"></a><a data-type="indexterm" data-startref="ch08-mzzzy" id="idm45831170382808"></a></p>










<section data-type="sect3" data-pdf-bookmark="Model serving"><div class="sect3" id="idm45831170382008" title2="Model serving" no2="8.6.1.1">
<h3>8.6.1.1. Model serving</h3>

<p>Because TFServing only has production-level support for TensorFlow, it does<a data-type="indexterm" data-primary="TensorFlow Serving (TFServing)" data-secondary="model serving" id="idm45831170380712"></a><a data-type="indexterm" data-primary="model inference" data-secondary="TensorFlow Serving" data-tertiary="model serving" id="idm45831170379768"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model serving" data-tertiary="TensorFlow Serving" id="idm45831170378552"></a><a data-type="indexterm" data-primary="model serving" data-secondary="TensorFlow Serving" id="idm45831170377336"></a> not have the desired flexibility you would expect from a framework-agnostic inference service.
It does, however, support REST, gRPC, GPU acceleration, mini-batching, and “lite” versions for serving on edge devices.
Regardless of the underlying hardware, this support does not extend to streaming inputs or to built-in auto scaling.<sup><a data-type="noteref" id="idm45831170375912-marker" href="#idm45831170375912">[6]</a></sup>
Furthermore, the ability to extend the inference graph—beyond a Fairness Indicator—to include more advanced ML insights isn’t supported in a first-class way.
Despite providing basic serving and model analysis features for TensorFlow models, this inference solution does not satisfy your more advanced serving requirements.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Model monitoring"><div class="sect3" id="idm45831170374648" title2="Model monitoring" no2="8.6.1.2">
<h3>8.6.1.2. Model monitoring</h3>

<p>TFServing supports traditional monitoring via its<a data-type="indexterm" data-primary="TensorFlow Serving (TFServing)" data-secondary="model monitoring" id="idm45831170373128"></a><a data-type="indexterm" data-primary="monitoring" data-secondary="TensorFlow Serving" id="idm45831170372184"></a><a data-type="indexterm" data-primary="model serving" data-secondary="monitoring" data-tertiary="TensorFlow Serving" id="idm45831170371240"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model monitoring" data-tertiary="TensorFlow Serving" id="idm45831170370024"></a><a data-type="indexterm" data-primary="model inference" data-secondary="TensorFlow Serving" data-tertiary="model monitoring" id="idm45831170368808"></a><a data-type="indexterm" data-primary="models" data-secondary="monitoring" data-tertiary="TensorFlow Serving" id="idm45831170367592"></a> <a href="https://oreil.ly/WigdN">integration</a> with Prometheus.
This exposes both system information—such as CPU, memory, and networking—and TFServing-specific metrics; unfortunately, there is very little documentation (see the best source, on the  <a href="https://oreil.ly/czq_V">TensorFlow site</a>).
Also, there is no first-class integration with data visualization tools like Kibana or distributed tracing libraries like Jaeger.
As such, TFServing does not provide the managed network observability capabilities you desire.</p>

<p>When it comes to advanced model serving insights, including model drift and explainability, some of them are <a href="https://oreil.ly/_yunS">available in TensorFlow 2.0</a>.
Furthermore, the vendor lock-in to a proprietary serving solution complicates the plugability of model insight components.
Since the deployment strategy of TFServing uses Kubeflow’s infrastructure stack, it leverages a microservice approach.
This allows TFServing deployments to be easily coupled with auxiliary ML components.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Model updating"><div class="sect3" id="idm45831170335096" title2="Model updating" no2="8.6.1.3">
<h3>8.6.1.3. Model updating</h3>

<p>TFServing is quite advanced in that it enables canary, pinned, and even rollback <a data-type="indexterm" data-primary="model inference" data-secondary="TensorFlow Serving" data-tertiary="model updating" id="idm45831170333896"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model updating" data-tertiary="TensorFlow Serving" id="idm45831170332648"></a><a data-type="indexterm" data-primary="TensorFlow Serving (TFServing)" data-secondary="model updating" id="idm45831170331432"></a><a data-type="indexterm" data-primary="model inference" data-secondary="updating models" data-tertiary="TensorFlow Serving" id="idm45831170330520"></a><a data-type="indexterm" data-primary="model serving" data-secondary="model updating" data-tertiary="TensorFlow Serving" id="idm45831170329304"></a><a data-type="indexterm" data-primary="models" data-secondary="updating" data-tertiary="TensorFlow Serving" id="idm45831170328088"></a>deployment strategies.<sup><a data-type="noteref" id="idm45831170326744-marker" href="#idm45831170326744">[7]</a></sup>
However, the strategies are limited to the manual labeling of existing model versions and do not include support for the introduction of in-flight model versions.
So version promotion does not have a safe-rollout guarantee.
Lastly, the strategies are embedded in the server and aren’t extensible for other deployment strategies that might exist outside of TFServing.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Summary"><div class="sect3" id="idm45831170324728" title2="Summary" no2="8.6.1.4">
<h3>8.6.1.4. Summary</h3>

<p>TFServing provides extremely performant and sophisticated out-of-the-box integration for TensorFlow models,
but it falls short on enabling more advanced features like framework extensibility, advanced telemetry, and plugable deployment strategies.
Seeing these requirements unsatisfied, we will now look at how Seldon Core attempts to fill these gaps.</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Seldon Core"><div class="sect1" id="idm45831170632088" title2="Seldon Core" no2="8.7">
<h1>8.7. Seldon Core</h1>

<p>Instead of just serving up single models behind an endpoint, Seldon Core<a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" id="ch08-sc"></a><a data-type="indexterm" data-primary="Seldon Core" data-secondary="inference" id="ch08-sc2"></a><a data-type="indexterm" data-primary="Seldon Core" data-secondary="about" id="idm45831170319176"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="about" id="idm45831170318232"></a> enables data scientists to compose complex runtime
inference graphs—by converting their machine learning code or artifacts into microservices.
An inference graph, as visualized in <a data-type="xref" href="#seldoninference_figure">FIGURE 8-2</a>, can be composed of:<a data-type="indexterm" data-primary="Seldon Core" data-secondary="inference graph" id="idm45831170315896"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="inference graph" id="idm45831170314920"></a></p>
<dl>
<dt>Models</dt>
<dd>
<p>Runtime inference executable for one or more ML models</p>
</dd>
<dt>Routers</dt>
<dd>
<p>Route requests to subgraphs, i.e., enabling A/B tests or multi-armed bandits</p>
</dd>
<dt>Combiners</dt>
<dd>
<p>Combine the responses from subgraphs, i.e., model ensemble</p>
</dd>
<dt>Transformers</dt>
<dd>
<p>Transform requests or responses, i.e., transform feature requests</p>
</dd>
</dl>

<figure><div id="seldoninference_figure" class="figure" data-type="figure" title2="Seldon inference graph example" no2="8-2">
<img src="assets/kfml_0802.png" alt="Seldon Inference Graph" width="1282" height="536">
<h6>Figure 8-2. Seldon inference graph example</h6>
</div></figure>

<p>To understand how Seldon achieves this, we will explore its core components and feature set:</p>
<dl>
<dt>Prepackaged model servers</dt>
<dd>
<p>Optimized Docker containers for popular libraries such as TensorFlow, XGBoost, H2O, etc., which can load and serve model artifacts/binaries</p>
</dd>
<dt>Language wrappers</dt>
<dd>
<p>Tools to enable more custom machine learning models to be wrapped using a
set of CLIs, which allow data scientists to convert a Python file or a Java JAR into a fully fledged microservice</p>
</dd>
<dt>Standardized API</dt>
<dd>
<p>Out-of-the-box APIs that can be REST or gRPC</p>
</dd>
<dt>Out of the box observability</dt>
<dd>
<p>Monitoring metrics and auditable request logs</p>
</dd>
<dt>Advanced machine learning insights</dt>
<dd>
<p>Complex ML concepts such as explainers, outlier detectors, and adversarial attack detectors abstracted into infrastructural components that can be extended when desired</p>
</dd>
</dl>

<p>Using all of these components, we walk through how to design an inference graph using Seldon.</p>








<section data-type="sect2" data-pdf-bookmark="Designing a Seldon Inference Graph"><div class="sect2" id="idm45831170297032" title2="Designing a Seldon Inference Graph" no2="8.7.1">
<h2>8.7.1. Designing a Seldon Inference Graph</h2>

<p>First, you will need to decide what components you want your inference graph to consist of. Will it be just a model server, or will you add a set of transformers, explainers, or outlier detectors to the model server?
Luckily, it’s really easy to add or remove components as you see fit,
so we will start with just a simple model server.</p>

<p>Second, you need to containerize your processing steps.
You can build each step of your inference graph with <em>model as data</em> or <em>model as code</em>.
For model as data, you could use a prepackaged model server to load your model artifacts/binaries and avoid building a Docker container every time your model changes.
For model as code, you would build your own prepackaged model server based on a custom implementation.
Your implementation is enabled via a language wrapper that would containerize your code by exposing a high-level interface to your model’s logic.
This can be used for more complex cases, even use cases that may require custom OS-specific, or even external-system dependencies.</p>

<p>Next, you need to test your implementation. You can run your implementation locally, leveraging
Seldon tools to verify that it works correctly.
Local development is enabled by the underlying portability of Kubernetes and by Seldon’s compatibility with Kubeflow’s infrastructure stack.</p>

<p>Then, you can enable Seldon Core extensions. Some extensions include: Jaeger tracing integration, ELK request logging integration,
Seldon Core analytics integration, or Istio/Ambassador ingress integration, to name a few.<sup><a data-type="noteref" id="idm45831170291704-marker" href="#idm45831170291704">[8]</a></sup></p>

<p>After enabling extensions, you can promote your local graph deployment to be hosted against a live Kubernetes cluster.</p>

<p>Lastly, you can hook up your inference graph into a continuous integration/continuous delivery (CI/CD) pipeline.
Seldon components allow you to integrate seamlessly into CI/CD workflows, which enables you to use your preferred CI tool to connect your model sources into Seldon Core.</p>

<p>Now that you have scoped out a rather robust inference graph, we will walk through some examples after getting set up with Seldon on your Kubeflow cluster.</p>










<section data-type="sect3" data-pdf-bookmark="Setting up Seldon Core"><div class="sect3" id="idm45831170286872" title2="Setting up Seldon Core" no2="8.7.1.1">
<h3>8.7.1.1. Setting up Seldon Core</h3>

<p>Seldon Core 1.0 comes prepackaged with Kubeflow, so it should already be available to you.<a data-type="indexterm" data-primary="Seldon Core" data-secondary="setting up" id="idm45831170285256"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="setting up" id="idm45831170284280"></a> The Seldon Core installation will create a Kubernetes operator which will watch for SeldonDeployment resources that describe your inference graph. However, you can install a custom version of Seldon Core, as per the installation instructions, with <a data-type="xref" href="#Helm_install_cust_SeldonCore_vers">EXAMPLE 8-5</a>.</p>
<div id="Helm_install_cust_SeldonCore_vers" data-type="example" class="less_space pagebreak-before" title2="Helm install for a custom Seldon Core version" no2="8-5">
<h5>Example 8-5. Helm install for a custom Seldon Core version</h5>

<pre data-type="programlisting" data-code-language="bash">helm install seldon-core-operator <code class="se">\</code>
    --repo https://storage.googleapis.com/seldon-charts  <code class="se">\</code>
    --namespace default <code class="se">\</code>
    --set istio.gateway<code class="o">=</code>istio-system/seldon-gateway <code class="se">\</code>
    --set istio.enabled<code class="o">=</code><code class="nb">true</code></pre></div>

<p>You must ensure that the namespace where your models will be served<a data-type="indexterm" data-primary="namespaces" data-secondary="Seldon Core installation" id="idm45831170276920"></a> has an Istio gateway and an InferenceServing namespace label. An example label application would be:</p>

<pre data-type="programlisting" data-code-language="bash" id="untitled_programlisting_17" title2="(no caption)" no2="">kubectl label namespace kubeflow serving.kubeflow.org/inferenceservice<code class="o">=</code>enabled</pre>

<p>An example Istio gateway is shown in <a data-type="xref" href="#Seldon_Core_Istio_Gateway">EXAMPLE 8-6</a>.</p>
<div id="Seldon_Core_Istio_Gateway" data-type="example" title2="Seldon Core Istio Gateway" no2="8-6">
<h5>Example 8-6. Seldon Core Istio Gateway</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Gateway</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">seldon-gateway</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">istio-system</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">selector</code><code class="p">:</code>
    <code class="nt">istio</code><code class="p">:</code> <code class="l-Scalar-Plain">ingressgateway</code>
  <code class="nt">servers</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">hosts</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="s">'*'</code>
    <code class="nt">port</code><code class="p">:</code>
      <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">http</code>
      <code class="nt">number</code><code class="p">:</code> <code class="l-Scalar-Plain">80</code>
      <code class="nt">protocol</code><code class="p">:</code> <code class="l-Scalar-Plain">HTTP</code></pre></div>

<p>You should save <a data-type="xref" href="#Seldon_Core_Istio_Gateway">EXAMPLE 8-6</a> to a file and apply it using <code>kubectl</code>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Packaging your model"><div class="sect3" id="package_your_model" title2="Packaging your model" no2="8.7.1.2">
<h3>8.7.1.2. Packaging your model</h3>

<p>As mentioned before, to run a model with Seldon Core you can either
package<a data-type="indexterm" data-primary="Seldon Core" data-secondary="packaging the model" id="idm45831170172680"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="packaging the model" id="idm45831170171704"></a> it using a prepackaged model server<sup><a data-type="noteref" id="idm45831170170360-marker" href="#idm45831170170360">[9]</a></sup> or a language wrapper.<sup><a data-type="noteref" id="idm45831170169576-marker" href="#idm45831170169576">[10]</a></sup></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Creating a SeldonDeployment"><div class="sect3" id="idm45831170168648" title2="Creating a SeldonDeployment" no2="8.7.1.3">
<h3>8.7.1.3. Creating a SeldonDeployment</h3>

<p>After packaging your model you need to define an inference graph that connects a set of model components into a<a data-type="indexterm" data-primary="Seldon Core" data-secondary="deployment" id="idm45831170166856"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="deployment" id="idm45831170165880"></a>
single inference system. Each of the model components can be one of the two options outlined in <a data-type="xref" href="#package_your_model">SECTION 8.7.1.2</a>.</p>

<p class="less_space pagebreak-before">Some example graphs are shown in <a data-type="xref" href="#seldongraph_figure">FIGURE 8-3</a>.<a data-type="indexterm" data-primary="Seldon Core" data-secondary="example graphs" id="ch08-sceg"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="example graphs" id="ch08-sceg2"></a></p>

<figure><div id="seldongraph_figure" class="figure" data-type="figure" title2="Seldon graph examples" no2="8-3">
<img src="assets/kfml_0803.png" alt="Seldon Inference Graph example" width="1447" height="561">
<h6>Figure 8-3. Seldon graph examples</h6>
</div></figure>

<p>The following list expands on the example inference graphs (a) to (e), as shown in <a data-type="xref" href="#seldongraph_figure">FIGURE 8-3</a>:</p>

<ul>
<li>
<p>(a) A single model</p>
</li>
<li>
<p>(b) Two models in sequence. The output of the first will be fed into the input of the second.</p>
</li>
<li>
<p>(c) A model with input and output transformers: the input transformer will be called, then the model and the response will be transformed by the output transformer</p>
</li>
<li>
<p>(d) A router that will choose whether to send to model A or B</p>
</li>
<li>
<p>(e) A combiner that takes the response from model A and B and combines into a single response<sup><a data-type="noteref" id="idm45831170150904-marker" href="#idm45831170150904">[11]</a></sup></p>
</li>
</ul>

<p>In addition, SeldonDeployment can specify methods for each component.
When your SeldonDeployment is deployed, Seldon Core adds a service orchestrator to manage the request and response flow through your graph.</p>

<p>An example SeldonDeployment, for inference graph (a) in <a data-type="xref" href="#seldongraph_figure">FIGURE 8-3</a>, appears in <a data-type="xref" href="#Simple_Seldon_Core_prep_model_server">EXAMPLE 8-7</a> as an example of what a prepackaged model server looks like.</p>
<div id="Simple_Seldon_Core_prep_model_server" data-type="example" class="less_space pagebreak-before" title2="Simple Seldon Core prepackaged model server" no2="8-7">
<h5>Example 8-7. Simple Seldon Core prepackaged model server</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">machinelearning.seldon.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">SeldonDeployment</code>
<code class="nt">metadata</code><code class="p">:</code>
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">seldon-model</code>
<code class="nt">spec</code><code class="p">:</code>
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">test-deployment</code>
 <code class="nt">predictors</code><code class="p">:</code>
 <code class="p-Indicator">-</code> <code class="nt">componentSpecs</code><code class="p">:</code>
   <code class="nt">graph</code><code class="p">:</code>
     <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">classifier</code>
     <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">SKLEARN_SERVER</code>
     <code class="nt">modelUri</code><code class="p">:</code> <code class="l-Scalar-Plain">gs://seldon-models/sklearn/income/model</code>
     <code class="nt">children</code><code class="p">:</code> <code class="p-Indicator">[]</code>
   <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">example</code>
   <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code></pre></div>

<p>In the example you see that the SeldonDeployment has a list of <code>predictors</code>, each of which describes an inference graph.
Each predictor has some core fields:</p>
<dl>
<dt>componentSpecs</dt>
<dd>
<p>A list of Kubernetes PodSpecs, each of which will be used for a Kubernetes deployment.</p>
</dd>
<dt>graph</dt>
<dd>
<p>A representation of the inference graph containing the name of each component, its type, and the protocol it respects. The name must match one container name from the componentSpecs section, unless it is a prepackaged model server (see subsequent examples).</p>
</dd>
<dt>Name</dt>
<dd>
<p>The name of the predictor.</p>
</dd>
<dt>Replicas</dt>
<dd>
<p>The number of replicas to create for each deployment in the predictor.</p>
</dd>
<dt>Type</dt>
<dd>
<p>The detail on whether it is a prepackaged model server or a custom language wrapper model.</p>
</dd>
<dt>modelUri</dt>
<dd>
<p>A URL where the model binary or weight are stored, which would be relevant for the respective prepackaged model server.</p>
</dd>
</dl>

<p>Another example for SeldonDeployment for (a) is shown in <a data-type="xref" href="#Simple_Seldon_Core_cust_lang_wrapper">EXAMPLE 8-8</a>, using in this instance a custom language wrapper model.</p>
<div id="Simple_Seldon_Core_cust_lang_wrapper" data-type="example" title2="Simple Seldon Core custom language wrapper" no2="8-8">
<h5>Example 8-8. Simple Seldon Core custom language wrapper</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">machinelearning.seldon.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">SeldonDeployment</code>
<code class="nt">metadata</code><code class="p">:</code>
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">seldon-model</code>
<code class="nt">spec</code><code class="p">:</code>
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">test-deployment</code>
 <code class="nt">predictors</code><code class="p">:</code>
 <code class="p-Indicator">-</code> <code class="nt">componentSpecs</code><code class="p">:</code>
   <code class="p-Indicator">-</code> <code class="nt">spec</code><code class="p">:</code>
       <code class="nt">containers</code><code class="p">:</code>
       <code class="p-Indicator">-</code> <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">seldonio/mock_classifier_rest:1.3</code>
         <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">classifier</code>
   <code class="nt">graph</code><code class="p">:</code>
     <code class="nt">children</code><code class="p">:</code> <code class="p-Indicator">[]</code>
     <code class="nt">endpoint</code><code class="p">:</code>
       <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">REST</code>
     <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">classifier</code>
     <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">MODEL</code>
   <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">example</code>
   <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code></pre></div>

<p>In this example you have a small set of new sections:</p>
<dl>
<dt>Containers</dt>
<dd>
<p>This is your Kubernetes container definition, where you are able to provide overrides to the details of your container, together with your Docker image and tag.</p>
</dd>
<dt>Endpoint</dt>
<dd>
<p>In this case you can specify if the endpoint of your model will be REST or gRPC.</p>
</dd>
</dl>

<p>The definition of your inference graph is now complete.<a data-type="indexterm" data-startref="ch08-sceg" id="idm45831169978760"></a><a data-type="indexterm" data-startref="ch08-sceg2" id="idm45831169978056"></a>
We will now discuss how to test your components individually or in unison on the cluster.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Testing Your Model"><div class="sect2" id="idm45831170296568" title2="Testing Your Model" no2="8.7.2">
<h2>8.7.2. Testing Your Model</h2>

<p>In order to test your components, you must interface with each using some request input.<a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="testing the model" id="idm45831169975448"></a><a data-type="indexterm" data-primary="Seldon Core" data-secondary="testing the model" id="idm45831169974200"></a><a data-type="indexterm" data-primary="testing" data-secondary="Seldon Core inference model" id="idm45831169973256"></a>
You can send requests directly using <code>curl</code>, <code>grpcurl</code>, or a similar utility, as well as by using the Python <code>SeldonClient</code> SDK.</p>

<p>There are several options for testing your model before deploying it.</p>
<dl>
<dt>Running your model directly with the Python client</dt>
<dd>
<p>This allows for easy local testing outside of a cluster.</p>
</dd>
<dt>Running your model as a Docker container</dt>
<dd>
<p>This can be used for all language wrappers—but not prepackaged inference servers—to test that your image has the required dependencies and behaves as you would expect.</p>
</dd>
<dt>Running your <code>SeldonDeployment</code> in a Kubernetes dev client such as <a href="https://oreil.ly/U-5XT">KIND</a></dt>
<dd>
<p>This can be used for any models and is a final test that your model will run as expected.</p>
</dd>
</dl>










<section data-type="sect3" data-pdf-bookmark="Python client for Python language wrapped models"><div class="sect3" id="idm45831169964632" title2="Python client for Python language wrapped models" no2="8.7.2.1">
<h3>8.7.2.1. Python client for Python language wrapped models</h3>

<p>You can define your Python model in a file called <em>MyModel.py</em>, as seen in <a data-type="xref" href="#Seldon_Core_Py_model_class">EXAMPLE 8-9</a>.<a data-type="indexterm" data-primary="Python" data-secondary="client for Python-wrapped models" id="idm45831169961736"></a><a data-type="indexterm" data-primary="testing" data-secondary="Seldon Core inference model" data-tertiary="Python-wrapped models" id="idm45831169960744"></a></p>
<div id="Seldon_Core_Py_model_class" data-type="example" title2="Seldon Core Python model class" no2="8-9">
<h5>Example 8-9. Seldon Core Python model class</h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">MyModel</code><code class="p">:</code>
    <code class="k">def</code> <code class="nf-Magic">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
      <code class="k">pass</code>
    <code class="k">def</code> <code class="nf">predict</code><code class="p">(</code><code class="o">*</code><code class="n">args</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code><code class="p">):</code>
      <code class="k">return</code> <code class="p">[</code><code class="s2">"hello, "</code><code class="n">world</code><code class="s2">"]</code></pre></div>

<p>You are able to test your model by running the <code>microservice</code> CLI that is provided by the <a href="https://oreil.ly/RH1Dg">Python module</a>.
Once you install the Python <code>seldon-core</code> module you will be able to run the model with the following command:</p>

<pre data-type="programlisting" data-code-language="bash" id="untitled_programlisting_18" title2="(no caption)" no2="">&gt; seldon-core-microservice MyModel REST --service-type MODEL
...
2020-03-23 16:59:17,366 - werkzeug:_log:122
- INFO: * Running on http://0.0.0.0:5000/
<code class="o">(</code>Press CTRL+C to quit<code class="o">)</code></pre>

<p>Now that your model microservice is running, you can send a request using curl, as seen in <a data-type="xref" href="#Send_req_to_SeldonCore_custom_microservice">EXAMPLE 8-10</a>.</p>
<div id="Send_req_to_SeldonCore_custom_microservice" data-type="example" title2="Sending a request to your Seldon Core custom microservice" no2="8-10">
<h5>Example 8-10. Sending a request to your Seldon Core custom microservice</h5>

<pre data-type="programlisting" data-code-language="bash">&gt; curl -X POST <code class="se">\</code>
&gt; 	-H <code class="s1">'Content-Type: application/json'</code> <code class="se">\</code>
&gt; 	-d <code class="s1">'{"data": { "ndarray": [[1,2,3,4]]}}'</code> <code class="se">\</code>
&gt;     	http://localhost:5000/api/v1.0/predictions
<code class="o">{</code><code class="s2">"data"</code>:<code class="o">{</code><code class="s2">"names"</code>:<code class="o">[]</code>,<code class="s2">"ndarray"</code>:<code class="o">[</code><code class="s2">"hello"</code>,<code class="s2">"world"</code><code class="o">]}</code>,<code class="s2">"meta"</code>:<code class="o">{}}</code></pre></div>

<p>You can see that the output of the model is returned through the API.<sup><a data-type="noteref" id="idm45831169891080-marker" href="#idm45831169891080">[12]</a></sup></p>
</div></section>













<section data-type="sect3" class="less_space pagebreak-before" data-pdf-bookmark="Local testing with Docker"><div class="sect3" id="idm45831169890504" title2="Local testing with Docker" no2="8.7.2.2">
<h3>8.7.2.2. Local testing with Docker</h3>

<p>If you are building language models with other wrappers, you can run the containers you build through your local Docker client.<a data-type="indexterm" data-primary="testing" data-secondary="Seldon Core inference model" data-tertiary="local testing with Docker" id="idm45831169888744"></a><a data-type="indexterm" data-primary="Seldon Core" data-secondary="testing the model" data-tertiary="local testing with Docker" id="idm45831169887560"></a><a data-type="indexterm" data-primary="Docker" data-secondary="Seldon Core local testing" id="idm45831169886328"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="testing the model" id="idm45831169829160"></a>
A good tool for building Docker containers from source code is <a href="https://oreil.ly/Kgx_Q">S2I</a>.  For this, you just have to run the Docker client with the command seen in <a data-type="xref" href="#Exp_Seldon_Core_micros_loc_Docker_client">EXAMPLE 8-11</a>.</p>
<div id="Exp_Seldon_Core_micros_loc_Docker_client" data-type="example" title2="Exposing Seldon Core microservice in a local Docker client" no2="8-11">
<h5>Example 8-11. Exposing Seldon Core microservice in a local Docker client</h5>

<pre data-type="programlisting" data-code-language="bash">docker run --rm --name mymodel -p 5000:5000 mymodel:0.1</pre></div>

<p>This will run the model and export it on port 5000, so now you can send a request using curl, as seen in <a data-type="xref" href="#Send_req_local_Seldon_Core_micros">EXAMPLE 8-12</a>.</p>
<div id="Send_req_local_Seldon_Core_micros" data-type="example" title2="Sending a request to your local Seldon Core microservice" no2="8-12">
<h5>Example 8-12. Sending a request to your local Seldon Core microservice</h5>

<pre data-type="programlisting" data-code-language="bash">&gt; curl -X POST <code class="se">\</code>
&gt; 	-H <code class="s1">'Content-Type: application/json'</code> <code class="se">\</code>
&gt; 	-d <code class="s1">'{"data": { "ndarray": [[1,2,3,4]]}}'</code> <code class="se">\</code>
&gt;     	http://localhost:5000/api/v1.0/predictions

<code class="o">{</code><code class="s2">"data"</code>:<code class="o">{</code><code class="s2">"names"</code>:<code class="o">[]</code>,<code class="s2">"ndarray"</code>:<code class="o">[</code><code class="s2">"hello"</code>,<code class="s2">"world"</code><code class="o">]}</code>,<code class="s2">"meta"</code>:<code class="o">{}}</code></pre></div>

<p>With this environment, you can rapidly prototype and effectively test, before serving your model in a live cluster.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Serving Requests"><div class="sect2" id="idm45831169779576" title2="Serving Requests" no2="8.7.3">
<h2>8.7.3. Serving Requests</h2>

<p>Seldon Core supports two ingress gateways, Istio and Ambassador. Because <a data-type="indexterm" data-primary="Seldon Core" data-secondary="serving requests" id="idm45831169777880"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="serving requests" id="idm45831169776904"></a><a data-type="indexterm" data-primary="Seldon Core" data-secondary="Istio ingress gateway and" id="idm45831169775688"></a><a data-type="indexterm" data-primary="Ambassador" id="idm45831169774776"></a> <a data-type="indexterm" data-primary="Seldon Core" id="idm45831169756072"></a>Kubeflow’s installation uses Istio, we will focus on how Seldon Core works with the <code>Istio Ingress Gateway</code>. We will assume that the Istio gateway is at <code><em>&lt;istioGateway&gt;</em></code> and has a SeldonDeployment name <code><em>&lt;deploymentName&gt;</em></code> in namespace <code><em>&lt;namespace&gt;</em></code>. This means a <code>REST</code> endpoint will be exposed at:</p>

<pre data-type="programlisting" data-code-language="bash" id="untitled_programlisting_19" title2="(no caption)" no2="">http://&lt;istioGateway&gt;/seldon/&lt;namespace&gt;/&lt;deploymentName&gt;/api/v1.0/predictions.</pre>

<p>A gRPC endpoint will be exposed at <code><em>&lt;istioGateway&gt;</em></code> and you should send header metadata in your request with:</p>

<ul>
<li>
<p>Key <code>seldon</code> and value <code><em>&lt;deploymentName&gt;</em></code>.</p>
</li>
<li>
<p>Key <code>namespace</code> and value <code><em>&lt;namespace&gt;</em></code>.</p>
</li>
</ul>

<p>The payload for these requests will be a <a data-type="indexterm" data-primary="Seldon Core" data-secondary="SeldonMessage" id="idm45831169738600"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="SeldonMessage" id="idm45831169737736"></a>SeldonMessage.<sup><a data-type="noteref" id="idm45831169736392-marker" href="#idm45831169736392">[13]</a></sup></p>

<p>A sample SeldonMessage, say for a simple <code>ndarray</code> representation, is shown in <a data-type="xref" href="#SeldonMessage_contain_ndarray">EXAMPLE 8-13</a>.</p>
<div id="SeldonMessage_contain_ndarray" data-type="example" title2="SeldonMessage containing an ndarray" no2="8-13">
<h5>Example 8-13. SeldonMessage containing an ndarray</h5>

<pre data-type="programlisting" data-code-language="json"><code class="p">{</code>
   <code class="nt">"data"</code><code class="p">:</code> <code class="p">{</code>
   <code class="nt">"ndarray"</code><code class="p">:[[</code><code class="mf">1.0</code><code class="p">,</code> <code class="mf">2.0</code><code class="p">,</code> <code class="mf">5.0</code><code class="p">]]</code>
   <code class="p">}</code>
<code class="p">}</code></pre></div>

<p>Payloads can also include simple tensors, TFTensors, as well as binary, string, or JSON data. An example request containing JSON data is shown in <a data-type="xref" href="#SeldonMessage_containing_JSON_data">EXAMPLE 8-14</a>.</p>
<div id="SeldonMessage_containing_JSON_data" data-type="example" title2="SeldonMessage containing JSON data" no2="8-14">
<h5>Example 8-14. SeldonMessage containing JSON data</h5>

<pre data-type="programlisting" data-code-language="json"><code class="p">{</code>
   <code class="nt">"jsonData"</code><code class="p">:</code> <code class="p">{</code>
     <code class="nt">"field1"</code><code class="p">:</code> <code class="s2">"some text"</code><code class="p">,</code>
     <code class="nt">"field2"</code><code class="p">:</code> <code class="mi">3</code>
   <code class="p">}</code>
<code class="p">}</code></pre></div>

<p>Now that your inference graph is defined, tested, and running, you will want to get predictions back from it,
and you also might want to monitor it in production to ensure it is running as expected.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Monitoring Your Models"><div class="sect2" id="Monitor_yr_Models" title2="Monitoring Your Models" no2="8.7.4">
<h2>8.7.4. Monitoring Your Models</h2>

<p>In Seldon Core’s design, deploying ML models is not treated differently from how one would deploy traditional applications.<a data-type="indexterm" data-primary="Seldon Core" data-secondary="monitoring models" id="idm45831169664184"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="monitoring models" id="idm45831169663208"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model monitoring" data-tertiary="Seldon Core" id="idm45831169661992"></a><a data-type="indexterm" data-primary="models" data-secondary="monitoring" data-tertiary="Seldon Core" id="idm45831169660776"></a><a data-type="indexterm" data-primary="model serving" data-secondary="monitoring" data-tertiary="Seldon Core" id="idm45831169659560"></a><a data-type="indexterm" data-primary="monitoring" data-secondary="Seldon Core" id="idm45831169658344"></a>
The same applies to monitoring and governance once the deployments are live.
Traditional application monitoring metrics like request latency, load, and status code distribution are provided by exposing Prometheus metrics in Grafana.<sup><a data-type="noteref" id="idm45831169631800-marker" href="#idm45831169631800">[14]</a></sup></p>

<p>However, as data scientists we are mostly interested in how well the models are performing—the relationship between the live data coming in and the data the model was trained on and the reasons why specific predictions were made.</p>

<p>To address these concerns, Seldon Core provides the additional open source projects <a href="https://oreil.ly/tXxQr">Alibi:Explain</a> and
<a href="https://oreil.ly/iowRX">Alibi:Detect</a>, which focus specifically on advanced ML insights.
These two projects implement the core algorithms for model explainability, outlier detection, data drift, and adversarial attack detection, respectively.
We will now walk through examples of how Seldon Core enables model explainability and drift detection, via its integration of Alibi:Explain and Alibi:Detect.</p>










<section data-type="sect3" data-pdf-bookmark="Model explainability"><div class="sect3" id="idm45831169627560" title2="Model explainability" no2="8.7.4.1">
<h3>8.7.4.1. Model explainability</h3>

<p>Model explainability algorithms seek to answer the question:<a data-type="indexterm" data-primary="Seldon Core" data-secondary="explaining the model" id="idm45831169625992"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="explaining the model" id="idm45831169625016"></a><a data-type="indexterm" data-primary="models" data-secondary="explainability importance" data-tertiary="Seldon Core" id="idm45831169623800"></a><a data-type="indexterm" data-primary="model explainability" data-secondary="Seldon Core" id="idm45831169622616"></a>
“Why did my model make this prediction on this instance?” The answer can come in many shapes, i.e., the most important features contributing to the model’s prediction or the minimum change to features necessary to induce a different prediction.</p>

<p>Explainability algorithms are also distinguished by how much access to the underlying model they have.
On one end of the spectrum there are “black box” algorithms that only have access to the model prediction endpoint and nothing else.
In contrast, you have “white box” algorithms that have full access to the internal model architecture and allow for much greater insight (such as taking gradients).
In the production scenario, however, the black-box case is much more prominent, so we will focus on that here.</p>

<p>Before discussing an example, we will describe the integration patterns that would arise from the use of black-box explanation algorithms.
These algorithms typically work by generating a lot of similar-looking instances to the one being explained and then send both batch and
sequential requests to the model to map out a picture of the model’s decision-making process in the vicinity of the original instance.
Thus, an explainer component will communicate with the underlying model, as the explanation is being computed.
<a data-type="xref" href="#seldonexplainer_figure">FIGURE 8-4</a> shows how this pattern is implemented. A model configured as a SeldonDeployment sits alongside an explainer component, which comes with its own endpoint.
When the explainer endpoint is called internally the explainer communicates with the model to produce an explanation.</p>

<figure><div id="seldonexplainer_figure" class="figure" data-type="figure" title2="Seldon explainer component" no2="8-4">
<img src="assets/kfml_0804.png" alt="Seldon Explainer Component" width="619" height="501">
<h6>Figure 8-4. Seldon explainer component</h6>
</div></figure>
<div data-type="warning"><h6>Warning</h6>
<p>In <a data-type="xref" href="#seldonexplainer_figure">FIGURE 8-4</a>, the explainer communicates directly with the production model. However, in a more realistic scenario, the underlying model would be a separate but identical deployment (i.e., in staging) to ensure that calls to the explainer don’t degrade the performance of the production inference system.</p>
</div>

<p>To illustrate these techniques we will show a few examples.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Sentiment prediction model"><div class="sect3" id="idm45831169589976" title2="Sentiment prediction model" no2="8.7.4.2">
<h3>8.7.4.2. Sentiment prediction model</h3>

<p>Our first example is a sentiment prediction model that is<a data-type="indexterm" data-primary="Seldon Core" data-secondary="sentiment prediction model" id="ch08-issz2"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="sentiment prediction model" id="ch08-issz"></a><a data-type="indexterm" data-primary="sentiment prediction model" id="ch08-issz3"></a>
trained on <a href="https://oreil.ly/qOe2_">movie review data hosted by Cornell University</a>. You can launch this with an associated anchors explainer, using a SeldonDeployment like in <a data-type="xref" href="#SeldonDeployment_wAnchor_Expl">EXAMPLE 8-15</a>.</p>
<div id="SeldonDeployment_wAnchor_Expl" data-type="example" title2="SeldonDeployment with Anchor Explainers" no2="8-15">
<h5>Example 8-15. SeldonDeployment with Anchor Explainers</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">machinelearning.seldon.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">SeldonDeployment</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">movie</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">movie</code>
  <code class="nt">annotations</code><code class="p">:</code>
    <code class="nt">seldon.io/rest-timeout</code><code class="p">:</code> <code class="s">"10000"</code>
  <code class="nt">predictors</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">graph</code><code class="p">:</code>
      <code class="nt">children</code><code class="p">:</code> <code class="p-Indicator">[]</code>
      <code class="nt">implementation</code><code class="p">:</code> <code class="l-Scalar-Plain">SKLEARN_SERVER</code>
      <code class="nt">modelUri</code><code class="p">:</code> <code class="l-Scalar-Plain">gs://seldon-models/sklearn/moviesentiment</code>
      <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">classifier</code>
    <code class="nt">explainer</code><code class="p">:</code>
      <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">AnchorText</code>
    <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
    <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code></pre></div>

<p>Once deployed, this model can be queried via the Istio ingress as usual.
You can then send the simple review <code>"This film has great actors"</code> to the model, as in <a data-type="xref" href="#Send_pred_request_to_SeldonCore_moviesent">EXAMPLE 8-16</a>.</p>
<div id="Send_pred_request_to_SeldonCore_moviesent" data-type="example" title2="Sending a prediction request to your Seldon Core movie sentiment model" no2="8-16">
<h5>Example 8-16. Sending a prediction request to your Seldon Core movie sentiment model</h5>

<pre data-type="programlisting" data-code-language="bash">curl -d <code class="s1">'{"data": {"ndarray":["This film has great actors"]}}'</code> <code class="se">\</code>
   -X POST http://&lt;istio-ingress&gt;/seldon/seldon/movie/api/v1.0/predictions <code class="se">\</code>
   -H <code class="s2">"Content-Type: application/json"</code></pre></div>

<p>The response to the prediction request in <a data-type="xref" href="#Send_pred_request_to_SeldonCore_moviesent">EXAMPLE 8-16</a> is seen in <a data-type="xref" href="#Pred_resp_frSeldonCore_moviesentiment">EXAMPLE 8-17</a>.</p>
<div id="Pred_resp_frSeldonCore_moviesentiment" data-type="example" class="less_space pagebreak-before" title2="Prediction response from your Seldon Core movie sentiment model" no2="8-17">
<h5>Example 8-17. Prediction response from your Seldon Core movie sentiment model</h5>

<pre data-type="programlisting" data-code-language="json"><code class="p">{</code>
  <code class="nt">"data"</code><code class="p">:</code> <code class="p">{</code>
    <code class="nt">"names"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"t:0"</code><code class="p">,</code><code class="s2">"t:1"</code><code class="p">],</code>
    <code class="nt">"ndarray"</code><code class="p">:</code> <code class="p">[[</code><code class="mf">0.21266916924914636</code><code class="p">,</code><code class="mf">0.7873308307508536</code><code class="p">]]</code>
  <code class="p">},</code>
  <code class="nt">"meta"</code><code class="p">:</code> <code class="p">{}</code>
<code class="p">}</code></pre></div>

<p>The model is a classifier and it is predicting with 78% accuracy that this is a positive
review, which is correct. You can now try to explain the request, as seen in <a data-type="xref" href="#expl_req_to_SeldonC_moviesent">EXAMPLE 8-18</a>.</p>
<div id="expl_req_to_SeldonC_moviesent" data-type="example" title2="Sending an explanation request to your Seldon Core movie sentiment model" no2="8-18">
<h5>Example 8-18. Sending an explanation request to your Seldon Core movie sentiment model</h5>

<pre data-type="programlisting" data-code-language="bash">curl -d <code class="s1">'{"data": {"ndarray":["This movie has great actors"]}}'</code> <code class="se">\</code>
   -X POST http://&lt;istio-ingress&gt;/seldon/seldon/movie/explainer/api/v1.0/explain <code class="se">\</code>
   -H <code class="s2">"Content-Type: application/json"</code></pre></div>

<p>The response to the explanation request in <a data-type="xref" href="#expl_req_to_SeldonC_moviesent">EXAMPLE 8-18</a> is seen in <a data-type="xref" href="#Expl_resp_fr_SeldonC_moviesent">EXAMPLE 8-19</a> (curtailed without the examples section).</p>
<div id="Expl_resp_fr_SeldonC_moviesent" data-type="example" title2="Explanation response from your Seldon Core movie sentiment model" no2="8-19">
<h5>Example 8-19. Explanation response from your Seldon Core movie sentiment model</h5>

<pre data-type="programlisting" data-code-language="json"><code class="p">{</code>
  <code class="nt">"names"</code><code class="p">:</code> <code class="p">[</code>
    <code class="s2">"great"</code>
  <code class="p">],</code>
  <code class="nt">"precision"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code>
  <code class="nt">"coverage"</code><code class="p">:</code> <code class="mf">0.5007</code><code class="p">,</code>
  <code class="err">...</code>
  <code class="nt">"instance"</code><code class="p">:</code> <code class="s2">"This movie has great actors"</code><code class="p">,</code>
  <code class="nt">"prediction"</code><code class="p">:</code> <code class="mi">1</code>
  <code class="p">}</code><code class="err">,</code>
  <code class="s2">"meta"</code><code class="err">:</code> <code class="p">{</code>
    <code class="nt">"name"</code><code class="p">:</code> <code class="s2">"AnchorText"</code>
  <code class="p">}</code>
<code class="err">}</code></pre></div>

<p>The key element in this example is that the explainer has identified the word <em>great</em> as being the reason the model predicted positive sentiment and suggests that this would occur 100% of the time for this model if a sentence contains the word <em>great</em> (reflected by the precision value).<a data-type="indexterm" data-startref="ch08-issz" id="idm45831169353144"></a><a data-type="indexterm" data-startref="ch08-issz2" id="idm45831169352648"></a><a data-type="indexterm" data-startref="ch08-issz3" id="idm45831169352008"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="US Census income predictor model example"><div class="sect3" id="idm45831169589384" title2="US Census income predictor model example" no2="8.7.4.3">
<h3>8.7.4.3. US Census income predictor model example</h3>

<p>Here is a second example, trained on the<a data-type="indexterm" data-primary="US Census dataset" data-secondary="income predictor model" id="ch08-incp2"></a><a data-type="indexterm" data-primary="income predictor model" id="ch08-incp"></a><a data-type="indexterm" data-primary="Seldon Core" data-secondary="income predictor model" id="ch08-incp4"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="income predictor model" id="ch08-incp3"></a>  <a href="https://oreil.ly/kcmRm">1996 US Census data</a>, which predicts whether a person will have high or low income.<sup><a data-type="noteref" id="idm45831169343736-marker" href="#idm45831169343736">[15]</a></sup>
For this example, you also need to have an Alibi explainer sample the input dataset and identify categorical features to allow the explainer to give more intuitive results.
The details for configuring an Alibi explainer can be found in the  <a href="https://oreil.ly/4i10y">Alibi documentation</a> along with an in-depth review of the following data science example.</p>

<p>The SeldonDeployment resource is defined in <a data-type="xref" href="#SeldonDeployment_inc_pred">EXAMPLE 8-20</a>.</p>
<div id="SeldonDeployment_inc_pred" data-type="example" title2="SeldonDeployment for income predictor" no2="8-20">
<h5>Example 8-20. SeldonDeployment for income predictor</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">machinelearning.seldon.io/v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">SeldonDeployment</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">income</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">income</code>
  <code class="nt">annotations</code><code class="p">:</code>
    <code class="nt">seldon.io/rest-timeout</code><code class="p">:</code> <code class="s">"100000"</code>
  <code class="nt">predictors</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">graph</code><code class="p">:</code>
      <code class="nt">children</code><code class="p">:</code> <code class="p-Indicator">[]</code>
      <code class="nt">implementation</code><code class="p">:</code> <code class="l-Scalar-Plain">SKLEARN_SERVER</code>
      <code class="nt">modelUri</code><code class="p">:</code> <code class="l-Scalar-Plain">gs://seldon-models/sklearn/income/model</code>
      <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">classifier</code>
    <code class="nt">explainer</code><code class="p">:</code>
      <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">AnchorTabular</code>
      <code class="nt">modelUri</code><code class="p">:</code> <code class="l-Scalar-Plain">gs://seldon-models/sklearn/income/explainer</code>
    <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
    <code class="nt">replicas</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code></pre></div>

<p>Once deployed, you can ask for a prediction with a curl request seen in <a data-type="xref" href="#Send_pred_req_SeldonC_inc_pred_model">EXAMPLE 8-21</a>.</p>
<div id="Send_pred_req_SeldonC_inc_pred_model" data-type="example" title2="Sending a prediction request to your Seldon Core income predictor model" no2="8-21">
<h5>Example 8-21. Sending a prediction request to your Seldon Core income predictor model</h5>

<pre data-type="programlisting" data-code-language="bash">curl -d <code class="s1">'{"data": {"ndarray":[[39, 7, 1, 1, 1, 1, 4, 1, 2174, 0, 40, 9]]}}'</code> <code class="se">\</code>
   -X POST http://&lt;istio-ingress&gt;/seldon/seldon/income/api/v1.0/predictions <code class="se">\</code>
   -H <code class="s2">"Content-Type: application/json"</code></pre></div>

<p>The response to the prediction request in <a data-type="xref" href="#Send_pred_req_SeldonC_inc_pred_model">EXAMPLE 8-21</a> is seen in <a data-type="xref" href="#Pred_resp_SeldonC_inc_pred_model">EXAMPLE 8-22</a>.</p>
<div id="Pred_resp_SeldonC_inc_pred_model" data-type="example" class="less_space pagebreak-before" title2="Prediction response from your Seldon Core income predictor model" no2="8-22">
<h5>Example 8-22. Prediction response from your Seldon Core income predictor model</h5>

<pre data-type="programlisting" data-code-language="json"><code class="p">{</code>
    <code class="nt">"data"</code><code class="p">:</code> <code class="p">{</code>
      <code class="nt">"names"</code><code class="p">:[</code><code class="s2">"t:0"</code><code class="p">,</code><code class="s2">"t:1"</code><code class="p">],</code>
      <code class="nt">"ndarray"</code><code class="p">:[[</code><code class="mf">1.0</code><code class="p">,</code><code class="mf">0.0</code><code class="p">]]</code>
     <code class="p">},</code>
     <code class="nt">"meta"</code><code class="p">:{}</code>
 <code class="p">}</code></pre></div>

<p>The model is predicting low income for this person. You can now get an explanation for this prediction with <a data-type="xref" href="#Send_expl_req_to_SeldonC_incpredmod">EXAMPLE 8-23</a>.</p>
<div id="Send_expl_req_to_SeldonC_incpredmod" data-type="example" title2="Sending a explanation request to your Seldon Core income predictor model" no2="8-23">
<h5>Example 8-23. Sending a explanation request to your Seldon Core income predictor model</h5>

<pre data-type="programlisting" data-code-language="bash">curl -d <code class="s1">'{"data": {"ndarray":[[39, 7, 1, 1, 1, 1, 4, 1, 2174, 0, 40, 9]]}}'</code> <code class="se">\</code>
   -X POST http://&lt;istio-ingress&gt;/seldon/seldon/income/explainer/api/v1.0/explain <code class="se">\</code>
   -H <code class="s2">"Content-Type: application/json"</code></pre></div>

<p>The response to the explanation request in <a data-type="xref" href="#Send_expl_req_to_SeldonC_incpredmod">EXAMPLE 8-23</a> is seen in <a data-type="xref" href="#Explanation_response_fr_SeldonCore_income_predictor">EXAMPLE 8-24</a>, which we have shortened to not show all the examples returned.</p>
<div id="Explanation_response_fr_SeldonCore_income_predictor" data-type="example" title2="Explanation response from your Seldon Core income predictor model" no2="8-24">
<h5>Example 8-24. Explanation response from your Seldon Core income predictor model</h5>

<pre data-type="programlisting" data-code-language="json"><code class="p">{</code>
  <code class="nt">"names"</code><code class="p">:</code> <code class="p">[</code>
    <code class="s2">"Marital Status = Never-Married"</code><code class="p">,</code>
    <code class="s2">"Occupation = Admin"</code><code class="p">,</code>
    <code class="s2">"Relationship = Not-in-family"</code>
  <code class="p">],</code>
  <code class="nt">"precision"</code><code class="p">:</code> <code class="mf">0.9766081871345029</code><code class="p">,</code>
  <code class="nt">"coverage"</code><code class="p">:</code> <code class="mf">0.022</code><code class="p">,</code>
  <code class="err">...</code>
<code class="p">}</code></pre></div>

<p>The key takeaway is that this model will predict a low income classification 97% of the time if the input features are <code>"Marital Status = Never-Married"</code>, <code>"Occupation = Admin"</code>, and <code>"Relationship = Not-in-family"</code>.
So these are the key features from the input that influenced the model.<a data-type="indexterm" data-startref="ch08-incp" id="idm45831169086808"></a><a data-type="indexterm" data-startref="ch08-incp2" id="idm45831169086200"></a><a data-type="indexterm" data-startref="ch08-incp3" id="idm45831169085528"></a><a data-type="indexterm" data-startref="ch08-incp4" id="idm45831169084856"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Outlier and drift detection"><div class="sect3" id="idm45831169350744" title2="Outlier and drift detection" no2="8.7.4.4">
<h3>8.7.4.4. Outlier and drift detection</h3>

<p>ML models traditionally do not extrapolate well outside of the training data distribution, and that impacts model drift.<a data-type="indexterm" data-primary="Seldon Core" data-secondary="monitoring models" id="idm45831169082440"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="monitoring models" id="idm45831169081464"></a><a data-type="indexterm" data-primary="Seldon Core" data-secondary="outlier and drift detection in" id="idm45831169080248"></a><a data-type="indexterm" data-primary="model drift" data-secondary="Seldon Core" id="idm45831169079288"></a><a data-type="indexterm" data-primary="model drift" data-secondary="Seldon Core" id="idm45831169078344"></a><a data-type="indexterm" data-primary="monitoring" data-secondary="Seldon Core" id="idm45831169077400"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model monitoring" data-tertiary="Seldon Core" id="idm45831169076456"></a><a data-type="indexterm" data-primary="model serving" data-secondary="monitoring" data-tertiary="Seldon Core" id="idm45831169075240"></a><a data-type="indexterm" data-primary="models" data-secondary="monitoring" data-tertiary="Seldon Core" id="idm45831169074024"></a>
In order to trust and reliably act on model predictions, you must monitor the distribution of incoming requests via different types of detectors.
Outlier detectors aim to flag individual instances that do not follow the original training distribution.
An adversarial detector tries to spot and correct a carefully crafted attack with the intent to fool the model.
Drift detectors check when the distribution of the incoming requests is diverging from a reference distribution, such as that of the training data.</p>

<p>If data drift occurs, the model performance can deteriorate, and it should be retrained. The ML model predictions on instances flagged by any of the detectors we’ve looked at should be verified before being used in real-life applications. Detectors typically return an outlier score at the instance or even the feature level. If the score is above a predefined threshold, the instance is flagged.</p>

<p>Outlier and drift detection are usually done asynchronously to the actual prediction request.
In Seldon Core you can activate payload logging and send the requests to an external service that will do the outlier and drift
detection outside the main request/response flow.
An example architecture is shown in <a data-type="xref" href="#seldoneventing_figure">FIGURE 8-5</a>, where Seldon Core’s payload logger passes requests to components that process them asynchronously.
The components that do the processing and alerting are managed via Knative Eventing, which is described in<a data-type="indexterm" data-primary="Knative" data-secondary="Eventing" data-tertiary="Seldon Core" id="idm45831169069688"></a><a data-type="indexterm" data-primary="events via Knative Eventing" data-secondary="Seldon Core" id="idm45831169068472"></a>
<a data-type="xref" href="#knative_eventing">SECTION 8.8.4.5</a>. The use of Knative Eventing here is to provide late-binding event sources and event consumers, enabling asynchronous processing. The results can be passed on to alerting systems.</p>

<figure><div id="seldoneventing_figure" class="figure" data-type="figure" title2="Data science monitoring of models with Seldon Core and Knative" no2="8-5">
<img src="assets/kfml_0805.png" alt="Data Science Monitoring of Models with Seldon Core + Knative" width="1430" height="574">
<h6>Figure 8-5. Data science monitoring of models with Seldon Core and Knative</h6>
</div></figure>
<div data-type="note"><h6>Note</h6>
<p>Following are some examples that leverage outlier and drift detection using the architecture in <a data-type="xref" href="#seldoneventing_figure">FIGURE 8-5</a>:</p>

<ul>
<li>
<p>An <a href="https://oreil.ly/p-Lfw">outlier detection example</a> for CIFAR10</p>
</li>
<li>
<p>A <a href="https://oreil.ly/z8jRG">drift detection example</a> for CIFAR10</p>
</li>
</ul>
</div>
</div></section>



</div></section>













<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Review"><div class="sect2" id="idm45831169038520" title2="Review" no2="8.7.5">
<h2>8.7.5. Review</h2>

<p>Seldon Core is a solid choice as an inference solution when building an inference graph and hoping to simultaneously achieve model serving, monitoring, and updating guarantees.
It sufficiently fills most of the gaps of TFServing while enabling data scientists to organically grow their inference graph as their use cases become more complex.
It also allows many more features outside the scope of this overview, such as <code>Canaries</code>, <code>Shadows</code>, and powerful multistage inference pipelines.<sup><a data-type="noteref" id="idm45831169035576-marker" href="#idm45831169035576">[16]</a></sup></p>

<p>However, we will take a look at how it satisfies your inference requirements.</p>










<section data-type="sect3" data-pdf-bookmark="Model serving"><div class="sect3" id="idm45831169033784" title2="Model serving" no2="8.7.5.1">
<h3>8.7.5.1. Model serving</h3>

<p>Seldon Core clearly provides the functionality to extend an inference graph and support advanced ML insights in a first-class way.<a data-type="indexterm" data-primary="Seldon Core" data-secondary="model serving" id="idm45831169032312"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="model serving" id="idm45831169031336"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model serving" data-tertiary="Seldon Core" id="idm45831169030120"></a><a data-type="indexterm" data-primary="model serving" data-secondary="Seldon Core" id="idm45831169028904"></a>
The architecture is also flexible enough to leverage other advanced ML insights outside of its managed offering.
And Seldon Core is quite versatile, providing the expected serving flexibility because it is framework-agnostic.
It provides support for both REST and gRPC, and GPU acceleration.
It also can interface with streaming inputs using Knative Eventing.
However, because the SeldonDeployment is running as a bare Kubernetes deployment, it does not provide <a data-type="indexterm" data-primary="GPUs" data-secondary="autoscaling lacking in Seldon Core" id="idm45831169027352"></a>GPU autoscaling, which we
expect from hardware-agnostic autoscaling.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Model monitoring"><div class="sect3" id="idm45831169025960" title2="Model monitoring" no2="8.7.5.2">
<h3>8.7.5.2. Model monitoring</h3>

<p>Seldon Core seems to satisfy all of your model monitoring needs.<a data-type="indexterm" data-primary="monitoring" data-secondary="Seldon Core" id="idm45831169024664"></a><a data-type="indexterm" data-primary="Seldon Core" data-secondary="monitoring models" id="idm45831169023688"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="monitoring models" id="idm45831169022744"></a><a data-type="indexterm" data-primary="model serving" data-secondary="monitoring" data-tertiary="Seldon Core" id="idm45831169021528"></a><a data-type="indexterm" data-primary="models" data-secondary="monitoring" data-tertiary="Seldon Core" id="idm45831169020312"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model monitoring" data-tertiary="Seldon Core" id="idm45831169019096"></a>
Seldon Core’s deployment strategy also uses Kubeflow’s infrastructure stack, so it leverages a microservice approach. This is especially noticeable with Seldon Core’s explainers and detectors being represented as separate microservices within a flexible inference graph. Seldon Core makes monitoring first-class by enabling monitoring, logging, and tracing with its support of Prometheus and <a href="https://oreil.ly/stSIe">Zipkin</a>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Model updating"><div class="sect3" id="idm45831169016424" title2="Model updating" no2="8.7.5.3">
<h3>8.7.5.3. Model updating</h3>

<p>Seldon Core is advanced in that it supports a variety of deployment strategies,<a data-type="indexterm" data-primary="Seldon Core" data-secondary="model updating" id="idm45831169015064"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="model updating" id="idm45831169014088"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model updating" data-tertiary="Seldon Core" id="idm45831169012872"></a><a data-type="indexterm" data-primary="model serving" data-secondary="model updating" data-tertiary="Seldon Core" id="idm45831169011656"></a><a data-type="indexterm" data-primary="models" data-secondary="updating" data-tertiary="Seldon Core" id="idm45831169010440"></a> including canary, pinned, and even multi-armed bandits.
However, similar to TFServing, revision or version management isn’t managed in a first-class way.
This, again, means that version promotion does not have a safe-rollout guarantee.
Lastly, as you can see by the options available for graph inferencing, in <a data-type="xref" href="#seldongraph_figure">FIGURE 8-3</a>,
Seldon Core provides complete flexibility in growing your inference graph to support more complex deployment strategies.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Summary"><div class="sect3" id="idm45831169007560" title2="Summary" no2="8.7.5.4">
<h3>8.7.5.4. Summary</h3>

<p>Seldon Core works to fill in the gaps by providing extensibility and sophisticated out-of-the-box support for complex inference graphs and model insight.
But it falls short with regards to the autoscaling of GPUs, its scale-to-zero capabilities, and revision management for safe model updating—features that are common to serverless applications.<a data-type="indexterm" data-startref="ch08-sc" id="idm45831169005896"></a><a data-type="indexterm" data-startref="ch08-sc2" id="idm45831169005192"></a>
We will now explore how KFServing works to fill this gap by adding some recent Kubernetes additions, provided by Knative, to enable serverless workflows for TFServing, Seldon Core, and many more serving solutions.</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="KFServing"><div class="sect1" id="idm45831170322648" title2="KFServing" no2="8.8">
<h1>8.8. KFServing</h1>

<p>As seen with TFServing and Seldon Core, the production-grade <a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" id="ch08-kfs"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="inference" id="ch08-kfs2"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="about" id="idm45831168999912"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="about" id="idm45831168998696"></a>serving of ML models is not a unique problem to any one research team or company.
Unfortunately, this means that every in-house solution will use different model formats and expose unique proprietary serving APIs.
Another problem facing both TFServing and Seldon Core is<a data-type="indexterm" data-primary="TensorFlow Serving (TFServing)" data-secondary="serverless primitives lacking" id="idm45831168997336"></a><a data-type="indexterm" data-primary="model inference" data-secondary="TensorFlow Serving" data-tertiary="serverless primitives lacking" id="idm45831168996456"></a><a data-type="indexterm" data-primary="Seldon Core" data-secondary="serverless primitives lacking" id="idm45831168995272"></a><a data-type="indexterm" data-primary="model inference" data-secondary="Seldon Core" data-tertiary="serverless primitives lacking" id="idm45831168994312"></a> the lack of serverless primitives, like revision management and more sophisticated forms of autoscaling.
These shortcomings are also found in most inference services.
In order to unify the open source community of model servers, while filling the gaps that each model server had, Seldon, Google, Bloomberg, and IBM engaged with the open source community to collaboratively develop KFServing.</p>

<p>KFServing is a serverless inferencing solution that provides performant,<a data-type="indexterm" data-primary="serverless" data-secondary="KFServing" id="idm45831168992280"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="serverless inferencing" id="idm45831168991304"></a><a data-type="indexterm" data-primary="serverless" data-secondary="Knative Serving" id="idm45831168990360"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="serverless inferencing" id="idm45831168989416"></a> high-abstraction interfaces for common ML frameworks like TensorFlow, XGBoost, Scikit-learn, PyTorch, and ONNX.
By placing Knative on top of Kubeflow’s cloud native stack, KFServing<a data-type="indexterm" data-primary="Knative" data-secondary="Serving" id="idm45831168987864"></a><a data-type="indexterm" data-primary="scalability of Kubeflow" data-secondary="KFServing for inferencing" id="idm45831168986920"></a><a data-type="indexterm" data-primary="GPUs" data-secondary="autoscaling in KFServing" id="idm45831168986008"></a> encapsulates the complexity of autoscaling, networking, health checking, and server configuration and brings cutting-edge serving features like GPU autoscaling, scale to zero, and canary rollouts to ML prediction services.
This allows ML engineers to focus on critical data-science–related tooling like prediction services, transformers, explainability, and drift detectors.</p>








<section data-type="sect2" data-pdf-bookmark="Serverless and the Service Plane"><div class="sect2" id="serverless_service_plane" title2="Serverless and the Service Plane" no2="8.8.1">
<h2>8.8.1. Serverless and the Service Plane</h2>

<p>KFServing’s design primarily borrows from serverless web development.<a data-type="indexterm" data-primary="serverless" data-secondary="about" id="idm45831168982664"></a>
Serverless allows you to build and run applications and services without provisioning, scaling, or managing any servers.
These server configurations are commonly referred to as the service plane,<a data-type="indexterm" data-primary="service plane" id="idm45831168981352"></a><a data-type="indexterm" data-primary="control plane" id="idm45831168980680"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="service plane" id="idm45831168980008"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="service plane" id="idm45831168979064"></a> or control plane.</p>

<p>Naturally, serverless abstractions come with deployment simplicity and fluidity as there is limited infrastructure administration.
However, serverless architecture depends heavily on event-based triggers for scaling its replicas, which we will talk about in <a data-type="xref" href="#escape_hatches">SECTION 8.8.4.2</a>.
It allows you to focus solely on your application code.</p>

<p>One of the primary tenancies of KFServing is extending serverless application development to model serving.
This is particularly advantageous for data scientists, as you want to only focus on the ML model that you are developing and the resulting input and output layers.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Data Plane"><div class="sect2" id="idm45831168975320" title2="Data Plane" no2="8.8.2">
<h2>8.8.2. Data Plane</h2>

<p>KFServing defines the <em>data plane</em>, which links all of the standard model <a data-type="indexterm" data-primary="KFServing" data-secondary="data plane" id="ch08-inf2"></a><a data-type="indexterm" data-primary="data plane of KFServing" id="ch08-inf3"></a><a data-type="indexterm" data-primary="Knative" data-secondary="KFServing infrastructure" id="idm45831168971112"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="Knative" id="idm45831168970152"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="data plane" id="ch08-infzzy"></a>serving components together and uses Knative to provide serverless abstractions for the service plane. A data plane is the protocol for how packets and requests are forwarded from one interface to another while also providing agency over service discovery, health checking, routing, load balancing, authentication/authorization, and KFServing’s data plane architecture consists of a static graph of components—similar to Seldon Core’s InferenceGraph—to coordinate requests for a single model.
Advanced features like ensembling, A/B testing, and multi-armed bandits connect these services together, again taking inspiration from Seldon Core’s deployment extensibility.</p>

<p>In order to understand the data plane’s static graph, let’s review some terminology used in <a data-type="xref" href="#kfservingdataplane_figure">FIGURE 8-6</a>.</p>

<figure><div id="kfservingdataplane_figure" class="figure" data-type="figure" title2="KFServing data plane" no2="8-6">
<img src="assets/kfml_0806.png" alt="KFServing Data Plane" width="1442" height="529">
<h6>Figure 8-6. KFServing data plane</h6>
</div></figure>
<dl>
<dt>Endpoint</dt>
<dd>
<p>KFServing instances are divided into two endpoints: <em>default</em> and <em>canary</em>.<a data-type="indexterm" data-primary="KFServing" data-secondary="data plane" data-tertiary="endpoint in" id="idm45831168960520"></a><a data-type="indexterm" data-primary="canary deployments" data-secondary="KFServing endpoints" id="idm45831168959240"></a><a data-type="indexterm" data-primary="pinned deployments" data-secondary="KFServing endpoints" id="idm45831168958296"></a><a data-type="indexterm" data-primary="blue-green deployment" data-secondary="KFServing endpoints" id="idm45831168957352"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="endpoints" data-tertiary="blue-green deployment" id="idm45831168956408"></a>
The endpoints allow users to safely make changes using the <code>pinned</code> and <code>canary</code> rollout strategies.
Canarying is completely optional, enabling users to simply deploy with a blue-green deployment strategy against the default endpoint.</p>
</dd>
<dt>Component</dt>
<dd>
<p>Each endpoint has multiple components: <em>predictor</em>, <em>explainer</em>, and <em>transformer</em>. <a data-type="indexterm" data-primary="KFServing" data-secondary="data plane" data-tertiary="component in" id="idm45831168951384"></a></p>
</dd>
</dl>

<p>The only required component is the predictor, which is the core of the system. As KFServing evolves, it can seamlessly increase the number of supported components to enable use cases like Seldon Core’s outlier detection. If you want, you can even introduce your own components and wire them together using the power of Knative’s abstractions.</p>
<dl>
<dt>Predictor</dt>
<dd>
<p>The predictor is the workhorse of the KFServing instance.<a data-type="indexterm" data-primary="KFServing" data-secondary="data plane" data-tertiary="predictor in" id="idm45831168947368"></a>
It is simply a model and a model server that is made available at a network endpoint.</p>
</dd>
<dt>Explainer</dt>
<dd>
<p>The explainer enables an optional alternative data plane that provides <a data-type="indexterm" data-primary="KFServing" data-secondary="data plane" data-tertiary="explainer in" id="idm45831168944632"></a> model explanations in addition to predictions.
Users may define their own explanation container, which KFServing configures with relevant environment variables like a prediction endpoint.
For common use cases, KFServing provides out-of-the-box explainers like Seldon Core’s Alibi:Explain, which we learned about earlier.</p>
</dd>
<dt>Transformer</dt>
<dd>
<p>The transformer enables users to define a pre- and postprocessing step<a data-type="indexterm" data-primary="KFServing" data-secondary="data plane" data-tertiary="transformer in" id="idm45831168941560"></a> before the prediction and explanation workflows. Like the explainer, it is configured with relevant environment variables.</p>
</dd>
</dl>

<p>The last portion of the data plane is the prediction protocol<sup><a data-type="noteref" id="idm45831168939448-marker" href="#idm45831168939448">[17]</a></sup> that KFServing uses. KFServing worked to define a set of HTTP/REST and gRPC APIs that must be implemented by compliant inference/prediction services. It is worth noting that KFServing standardized this prediction workflow, described in <a data-type="xref" href="#kfserving_v1_data_plane">TABLE 8-3</a>, across all model 
frameworks.<a data-type="indexterm" data-startref="ch08-infzzy" id="idm45831168934744"></a><a data-type="indexterm" data-startref="ch08-inf2" id="idm45831168934040"></a><a data-type="indexterm" data-startref="ch08-inf3" id="idm45831168933368"></a></p>
<table id="kfserving_v1_data_plane" data-type="table" title2="KFServing V1 data plane" no2="8-3">
<caption>Table 8-3. KFServing V1 data plane</caption>
<thead>
<tr>
<th>API</th>
<th>Verb</th>
<th>Path</th>
<th>Payload</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Readiness</p></td>
<td><p>GET</p></td>
<td><div>
<p><code>/v1/models/&lt;model_name&gt;</code></p></div></td>
<td><div>
<pre data-type="programlisting" id="untitled_programlisting_20" title2="(no caption)" no2="">{
  Response:{"name":&lt;model_name&gt;,"ready": true/false}
}</pre></div></td>
</tr>
<tr>
<td><p>Predict</p></td>
<td><p>POST</p></td>
<td><div>
<p><code>/v1/models/&lt;model_name&gt;:predict</code></p></div></td>
<td><div>
<pre data-type="programlisting" id="untitled_programlisting_21" title2="(no caption)" no2="">{
  Request:{"instances": []},
  Response:{"predictions": []}
}</pre></div></td>
</tr>
<tr>
<td><p>Explain</p></td>
<td><p>POST</p></td>
<td><div>
<p><code>/v1/models/&lt;model_name&gt;:explain</code></p></div></td>
<td><div>
<pre data-type="programlisting" id="untitled_programlisting_22" title2="(no caption)" no2="">{
  Request:{"instances": []},
  Response:{"predictions": [],"explanations": []}
}</pre></div></td>
</tr>
</tbody>
</table>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Example Walkthrough"><div class="sect2" id="idm45831168916472" title2="Example Walkthrough" no2="8.8.3">
<h2>8.8.3. Example Walkthrough</h2>

<p>With the data plane defined, we will now walk through an example of how you can interface with a model served by KFServing.</p>










<section data-type="sect3" data-pdf-bookmark="Setting up KFServing"><div class="sect3" id="idm45831168914760" title2="Setting up KFServing" no2="8.8.3.1">
<h3>8.8.3.1. Setting up KFServing</h3>

<p>KFServing provides InferenceService, a serverless inference resource that <a data-type="indexterm" data-primary="KFServing" data-secondary="setting up" id="ch08-kfinf"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="setting up" id="ch08-kfinf2"></a>describes your static graph, by providing a Kubernetes CRD for serving ML models on arbitrary frameworks.
KFServing comes prepackaged with Kubeflow, so it should already be available.
The KFServing installation<sup><a data-type="noteref" id="idm45831168910168-marker" href="#idm45831168910168">[18]</a></sup> will create a Kubernetes operator in the <code>kubeflow</code> namespace, which will watch for <code>InferenceService</code> resources.</p>
<div data-type="warning"><h6>Warning</h6>
<p>Because Kubeflow’s Kubernetes minimal requirement is <code>1.14</code>, which does not support object selector,<a data-type="indexterm" data-primary="namespaces" data-secondary="KFServing" data-tertiary="setup" id="idm45831168907080"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="setting up" data-tertiary="namespaces and" id="idm45831168905832"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="setting up and namespaces" id="idm45831168904616"></a> <code>ENABLE_WEBHOOK_NAMESPACE_SELECTOR</code> is enabled in the Kubeflow installation by default.
If you are using Kubeflow’s dashboard or profile controller to create user namespaces, labels are automatically added to enable KFServing to deploy models.
If you are creating namespaces manually, you will need to run:</p>

<pre data-type="programlisting" data-code-language="bash" id="untitled_programlisting_23" title2="(no caption)" no2="">kubectl label namespace <code class="se">\</code>
my-namespace serving.kubeflow.org/inferenceservice<code class="o">=</code>enabled</pre>

<p>to allow KFServing to deploy <code>InferenceService</code> in the namespace <code>my-namespace</code>, for example.</p>
</div>

<p>To check whether the KFServing controller is installed correctly, run the following command:</p>

<pre data-type="programlisting" data-code-language="bash" id="untitled_programlisting_24" title2="(no caption)" no2="">kubectl get pods -n kubeflow <code class="p">|</code> grep kfserving</pre>

<p>You can confirm that the controller is running by seeing a pod in the <code>Running</code> state.<a data-type="indexterm" data-startref="ch08-kfinf" id="idm45831168894584"></a><a data-type="indexterm" data-startref="ch08-kfinf2" id="idm45831168894056"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="troubleshooting guide online" id="idm45831168893384"></a> There is also a detailed troubleshooting guide you can follow on  <a href="https://oreil.ly/YG_ut">this Kubeflow GitHub site</a>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Simplicity and extensibility"><div class="sect3" id="idm45831168881912" title2="Simplicity and extensibility" no2="8.8.3.2">
<h3>8.8.3.2. Simplicity and extensibility</h3>

<p>KFServing was fashioned to be simple for day-one users and customizable for seasoned data scientists.
This is enabled via the interface that KFServing designed.</p>

<p>Now we will take a look at three examples of <code>InferenceService</code>.<a data-type="indexterm" data-primary="KFServing" data-secondary="InferenceService" data-tertiary="examples" id="ch08-exkfs"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="InferenceService examples(((range=endofrange" data-startref="ch08-exkfs2)))" id="idm45831168856808"></a></p>

<p><a data-type="xref" href="#sklearn_ex">EXAMPLE 8-25</a> is for <code>sklearn</code>.</p>
<div id="sklearn_ex" data-type="example" title2="Simple sklearn KFServing InferenceService" no2="8-25">
<h5>Example 8-25. Simple sklearn KFServing InferenceService</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="s">"serving.kubeflow.org/v1alpha2"</code>
<code class="nt">kind</code><code class="p">:</code> <code class="s">"InferenceService"</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="s">"sklearn-iris"</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">default</code><code class="p">:</code>
    <code class="nt">predictor</code><code class="p">:</code>
      <code class="nt">sklearn</code><code class="p">:</code>
        <code class="nt">storageUri</code><code class="p">:</code> <code class="s">"gs://kfserving-samples/models/sklearn/iris"</code></pre></div>

<p><a data-type="xref" href="#tensorflow_ex">EXAMPLE 8-26</a> is for <code>tensorflow</code>.</p>
<div id="tensorflow_ex" data-type="example" title2="Simple TensorFlow KFServing InferenceService" no2="8-26">
<h5>Example 8-26. Simple TensorFlow KFServing InferenceService</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="s">"serving.kubeflow.org/v1alpha2"</code>
<code class="nt">kind</code><code class="p">:</code> <code class="s">"InferenceService"</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="s">"flowers-sample"</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">default</code><code class="p">:</code>
    <code class="nt">predictor</code><code class="p">:</code>
      <code class="nt">tensorflow</code><code class="p">:</code>
        <code class="nt">storageUri</code><code class="p">:</code> <code class="s">"gs://kfserving-samples/models/tensorflow/flowers"</code></pre></div>

<p><a data-type="xref" href="#pytorch_example">EXAMPLE 8-27</a> is for <code>pytorch</code>.</p>
<div id="pytorch_example" data-type="example" title2="Simple PyTorch KFServing InferenceService" no2="8-27">
<h5>Example 8-27. Simple PyTorch KFServing InferenceService</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="s">"serving.kubeflow.org/v1alpha2"</code>
<code class="nt">kind</code><code class="p">:</code> <code class="s">"InferenceService"</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="s">"pytorch-cifar10"</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">default</code><code class="p">:</code>
    <code class="nt">predictor</code><code class="p">:</code>
      <code class="nt">pytorch</code><code class="p">:</code>
        <code class="nt">storageUri</code><code class="p">:</code> <code class="s">"gs://kfserving-samples/models/pytorch/cifar10/"</code>
        <code class="nt">modelClassName</code><code class="p">:</code> <code class="s">"Net"</code></pre></div>

<p>Each of these will give you a serving instance—with an HTTP endpoint—that will serve a model using a requested framework server type.
In each of these examples, a <code>storageUri</code> points to a serialized asset.
The interface is mostly consistent across different models.
The differences are in the framework specifications, i.e., <code>tensorflow</code> and <code>pytorch</code>.
These framework specifications are common enough in that they share information like <code>storageUri</code> and Kubernetes resources requests, but they’re also extensible in that they can enable framework-specific information like PyTorch’s 
ModelClassName.</p>

<p>Clearly, this interface is simple enough to get started quite easily, but how extensible is it toward more complex deployment configurations and strategies?
<a data-type="xref" href="#Soph_Canary_KFServing_InferenceServ">EXAMPLE 8-28</a> exhibits some of the features that KFServing has to offer.</p>
<div id="Soph_Canary_KFServing_InferenceServ" data-type="example" title2="Sophisticated Canary KFServing InferenceService" no2="8-28">
<h5>Example 8-28. Sophisticated Canary KFServing InferenceService</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="s">"serving.kubeflow.org/v1alpha2"</code>
<code class="nt">kind</code><code class="p">:</code> <code class="s">"InferenceService"</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="s">"my-model"</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">default</code><code class="p">:</code>
    <code class="nt">predictor</code><code class="p">:</code>
      <code class="c1"># 90% of traffic is sent to this model</code>
      <code class="nt">tensorflow</code><code class="p">:</code>
        <code class="nt">storageUri</code><code class="p">:</code> <code class="s">"gs://kfserving-samples/models/tensorflow/flowers"</code>
        <code class="nt">serviceAccount</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
        <code class="nt">minReplicas</code><code class="p">:</code> <code class="l-Scalar-Plain">2</code>
        <code class="nt">maxReplicas</code><code class="p">:</code> <code class="l-Scalar-Plain">10</code>
        <code class="nt">resources</code><code class="p">:</code>
          <code class="nt">requests</code><code class="p">:</code>
            <code class="nt">cpu</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>
            <code class="nt">gpu</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>
            <code class="nt">memory</code><code class="p">:</code> <code class="l-Scalar-Plain">8Gi</code>
  <code class="nt">canaryTrafficPercent</code><code class="p">:</code> <code class="l-Scalar-Plain">10</code>
  <code class="nt">canary</code><code class="p">:</code>
    <code class="nt">predictor</code><code class="p">:</code>
      <code class="c1"># 10% of traffic is sent to this model</code>
      <code class="nt">tensorflow</code><code class="p">:</code>
        <code class="nt">storageUri</code><code class="p">:</code> <code class="s">"gs://kfserving-samples/models/tensorflow/flowers-2"</code>
        <code class="nt">serviceAccount</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
        <code class="nt">minReplicas</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>
        <code class="nt">maxReplicas</code><code class="p">:</code> <code class="l-Scalar-Plain">5</code>
        <code class="nt">resources</code><code class="p">:</code>
          <code class="nt">requests</code><code class="p">:</code>
            <code class="nt">cpu</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>
            <code class="nt">gpu</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>
            <code class="nt">memory</code><code class="p">:</code> <code class="l-Scalar-Plain">8Gi</code></pre></div>

<p>The first extension is the <code>ServiceAccount</code>, which is used for authentication in the form of managed identities. If you wish to authenticate to <code>S3</code> because your <code>S3</code> should not be public, you need an identity attached to your <code>InferenceService</code> that validates you as a user. KFServing allows you to pass an identity mounted on the container and wires up the credentials through the <code>ServiceAccount</code> in a managed way. For example, say you are trying to access a model that may be stored on <code>Minio</code>. You would use your <code>Minio</code> identity information to create a secret beforehand, and then attach it to the service account.
If you recall, we created a secret in MinIO in <a data-type="xref" href="#minio_walkthrough">SECTION 3.2.1</a>, so we just need to include KFServing-related annotations like in <a data-type="xref" href="#KFServing_ann_Minio_secret">EXAMPLE 8-29</a>.</p>
<div id="KFServing_ann_Minio_secret" data-type="example" title2="KFServing-annotated MinIO secret" no2="8-29">
<h5>Example 8-29. KFServing-annotated MinIO secret</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">data</code><code class="p">:</code>
 <code class="nt">awsAccessKeyID</code><code class="p">:</code> <code class="l-Scalar-Plain">xxxx</code>
 <code class="nt">awsSecretAccessKey</code><code class="p">:</code> <code class="l-Scalar-Plain">xxxxxxxxx</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Secret</code>
<code class="nt">metadata</code><code class="p">:</code>
 <code class="nt">annotations</code><code class="p">:</code>
   <code class="nt">serving.kubeflow.org/s3-endpoint</code><code class="p">:</code> <code class="l-Scalar-Plain">minio-service.kubeflow.svc.cluster.local:9000</code>
   <code class="nt">serving.kubeflow.org/s3-verifyssl</code><code class="p">:</code> <code class="s">"0"</code>
   <code class="nt">serving.kubeflow.org/s3-usehttps</code><code class="p">:</code> <code class="s">"0"</code>
   <code class="nt">serving.kubeflow.org/s3-region</code><code class="p">:</code> <code class="l-Scalar-Plain">us-east-1</code>
 <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">minioaccess</code>
 <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">my-namespace</code></pre></div>

<p>And attach it to a service account like the one seen in <a data-type="xref" href="#Serv_Acc_w_attMinio_secret">EXAMPLE 8-30</a>.</p>
<div id="Serv_Acc_w_attMinio_secret" data-type="example" title2="Service Account with attached MinIO secret" no2="8-30">
<h5>Example 8-30. Service Account with attached MinIO secret</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">ServiceAccount</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">my-namespace</code>
<code class="nt">secrets</code><code class="p">:</code>
<code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">default-token-rand6</code>
<code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">minioaccess</code></pre></div>

<p>The second extension to notice is the min and max replicas. You would use these to control provisioning to allow you to meet demand, neither dropping requests nor overallocating.</p>

<p>The third extension is resource requests, which have preset defaults that
you will almost always need to customize for your model.
As you can see, this interface enables the use of hardware accelerators, like GPUs.</p>

<p>The last extension showcases the mechanism that KFServing uses to enable canary deployments.
This deployment strategy assumes that you only want to focus on a two-way traffic split, as opposed to an <em>n</em>-way traffic split.
In order to customize your deployment strategy, do the following:</p>

<ul>
<li>
<p>If you use just the default, like in your initial template,
you get a standard <code>blue-green</code> deployment that comes with a Kubernetes deployment resource.</p>
</li>
<li>
<p>If you include a canary, with <code>canaryTrafficPercent == 0</code>, you get a pinned deployment where you have an addressable <code>default</code> and <code>canary</code> endpoint. This is useful if you wish to send experimental traffic to your new endpoint, while keeping your production traffic pointed to your old endpoint.</p>
</li>
<li>
<p>If you include canary, with <code>canaryTrafficPercent &gt; 0</code>, you get a <code>canary</code> deployment that enables you to slowly increment traffic to your canary deployment, in a transparent way.
In the previous example, you are experimenting with <code>flowers-2</code>, and as you slowly increment this <code>canaryTrafficPercentage</code>
you can gain confidence that your new model will not break your current users.<sup><a data-type="noteref" id="idm45831168432584-marker" href="#idm45831168432584">[19]</a></sup> Eventually, you would go to <code>100</code>, thereby flipping the canary and default, and you should then delete your old version.<a data-type="indexterm" data-startref="ch08-exkfs" id="idm45831168430552"></a><a data-type="indexterm" data-startref="ch08-exkfs2" id="idm45831168429848"></a></p>
</li>
</ul>

<p>Now that we understand some of the powerful abstractions that KFServing offers, let’s use KFServing to host your product recommender example.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Recommender example"><div class="sect3" id="idm45831168880968" title2="Recommender example" no2="8.8.3.3">
<h3>8.8.3.3. Recommender example</h3>

<p>We will now put your product recommender example, from <a data-type="xref" href="#recommender_example">SECTION 7.1</a>, behind an <code>InferenceService</code>.<a data-type="indexterm" data-primary="recommendation systems" data-secondary="KFServing InferenceService" id="ch08-rex3"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="InferenceService" data-tertiary="recommender" id="ch08-rex"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="InferenceService recommender" id="ch08-rex2"></a></p>
<div data-type="warning"><h6>Warning</h6>
<p>Because the <code>kubeflow</code> namespace is a system namespace, you are unable<a data-type="indexterm" data-primary="namespaces" data-secondary="KFServing" data-tertiary="InferenceService" id="idm45831168518376"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="InferenceService" data-tertiary="namespace" id="idm45831168517128"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="InferenceService namespace" id="idm45831168515912"></a> to create an <code>InferenceService</code> in the <code>kubeflow</code> namespace.
As such, you must deploy your <code>InferenceService</code> in another namespace.</p>
</div>

<p>First, you’ll define your <code>InferenceService</code> with the following 11 lines of YAML, as seen in <a data-type="xref" href="#KFServing_Recommender_InfServ">EXAMPLE 8-31</a>.<a data-type="indexterm" data-primary="YAML" data-secondary="KFServing InferenceService" id="idm45831168511480"></a></p>
<div id="KFServing_Recommender_InfServ" data-type="example" title2="KFServing Recommender InferenceService" no2="8-31">
<h5>Example 8-31. KFServing Recommender InferenceService</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="s">"serving.kubeflow.org/v1alpha2"</code>
<code class="nt">kind</code><code class="p">:</code> <code class="s">"InferenceService"</code>
<code class="nt">metadata</code><code class="p">:</code>
 <code class="nt">name</code><code class="p">:</code> <code class="s">"recommender"</code>
 <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">my-namespace</code>
<code class="nt">spec</code><code class="p">:</code>
 <code class="nt">default</code><code class="p">:</code>
   <code class="nt">predictor</code><code class="p">:</code>
     <code class="nt">tensorflow</code><code class="p">:</code>
       <code class="nt">serviceAccount</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
       <code class="nt">storageUri</code><code class="p">:</code> <code class="s">"s3://models/recommender"</code></pre></div>

<p class="less_space pagebreak-before">After running <code>kubectl apply</code> and waiting until your <code>InferenceService</code> is <code>Ready</code>, you should see:</p>

<pre data-type="programlisting" data-code-language="bash" id="untitled_programlisting_25" title2="(no caption)" no2=""><code class="nv">$ </code>kubectl get inferenceservices -n my-namespace
NAME        URL                                                               READY DEFAULT
recommender http://recommender.my-namespace.example.com/v1/models/recommender True  100</pre>

<p>You can then curl your <code>InferenceService</code> as in <a data-type="xref" href="#SendpredreqtoKFS_RecInfServ">EXAMPLE 8-32</a>.</p>
<div id="SendpredreqtoKFS_RecInfServ" data-type="example" title2="Sending a prediction request to your KFServing Recommender InferenceService" no2="8-32">
<h5>Example 8-32. Sending a prediction request to your KFServing Recommender InferenceService</h5>

<pre data-type="programlisting" data-code-language="bash">kubectl port-forward --namespace istio-system <code class="se">\</code>
 <code class="k">$(</code>kubectl get pod --namespace istio-system <code class="se">\</code>
 --selector<code class="o">=</code><code class="s2">"app=istio-ingressgateway"</code> <code class="se">\</code>
 --output <code class="nv">jsonpath</code><code class="o">=</code><code class="s1">'{.items[0].metadata.name}'</code><code class="k">)</code> <code class="se">\</code>
 8080:80

curl -v -H <code class="s2">"Host: recommender.my-namespace.example.com"</code> <code class="se">\</code>
http://localhost:8080/v1/models/recommender:predict -d <code class="se">\</code>
<code class="s1">'{"signature_name":"serving_default",</code>
<code class="s1">  "inputs": {"products": [[1],[2]],"users" : [[25], [3]]}}'</code></pre></div>
<div data-type="warning"><h6>Warning</h6>
<p>If your <code>curl</code> returns a <code>404 Not Found</code> error, this is a known Istio gateway issue that is<a data-type="indexterm" data-primary="KFServing" data-secondary="curl 404 Not Found" id="idm45831168352840"></a>
present in Kubeflow <code>1.0.x</code>. We recommend that you use Kubeflow <code>1.1</code> or above. A possible workaround is described in this <a href="https://oreil.ly/oyTi-">GitHub issue</a>.</p>
</div>

<p>As an alternative to <code>curl</code>, you can also use the KFServing <code>PythonSDK</code> to send requests in Python.<sup><a data-type="noteref" id="idm45831168348568-marker" href="#idm45831168348568">[20]</a></sup> In addition to an HTTP endpoint, this simple interface<a data-type="indexterm" data-primary="KFServing" data-secondary="serverless inferencing" id="idm45831168290968"></a><a data-type="indexterm" data-primary="serverless" data-secondary="KFServing" id="idm45831168289992"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="serverless inferencing" id="idm45831168289048"></a> also provides all the serverless features that come with Kubeflow’s stack and Knative, among them:</p>

<ul>
<li>
<p>Scale to zero</p>
</li>
<li>
<p>GPU autoscaling</p>
</li>
<li>
<p>Revision management (safe rollouts)</p>
</li>
<li>
<p>Optimized containers</p>
</li>
<li>
<p>Network policy and authentication</p>
</li>
<li>
<p>Tracing</p>
</li>
<li>
<p>Metrics</p>
</li>
</ul>

<p>As such, with only a few lines of YAML, KFServing provides production ML features,
while also allowing data scientists to scale their deployments into the future.
But how does KFServing enable these features in such an abstracted way?<a data-type="indexterm" data-startref="ch08-rex" id="idm45831168280328"></a><a data-type="indexterm" data-startref="ch08-rex2" id="idm45831168279624"></a><a data-type="indexterm" data-startref="ch08-rex3" id="idm45831168278952"></a></p>

<p>We will now look at KFServing’s underlying infrastructure stack and see how it promotes serverless, how its layers can be further customized, and what additional features exist.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Peeling Back the Underlying Infrastructure"><div class="sect2" id="idm45831168428248" title2="Peeling Back the Underlying Infrastructure" no2="8.8.4">
<h2>8.8.4. Peeling Back the Underlying Infrastructure</h2>

<p>By dissecting its infrastructure stack, you can see how KFServing<a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" id="idm45831168276040"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="infrastructure stack" id="idm45831168275064"></a> enables serverless ML while also educating you on how to debug your inference solutions. KFServing is built in a cloud native way, as is Kubeflow.
It benefits from the features of every layer below it.
As seen in <a data-type="xref" href="#kfservinginfra_figure">FIGURE 8-7</a>, KFServing is built on the same stack as Kubeflow but is one of the few Kubeflow solutions
that leverage <code>Istio</code> and <code>Knative</code> functionality quite heavily.</p>

<p>We will now walk through the role of each of these components, in greater detail than we did in previous chapters,
to see what parts of these layers KFServing utilizes.</p>

<figure><div id="kfservinginfra_figure" class="figure" data-type="figure" title2="KFServing infrastructure stack" no2="8-7">
<img src="assets/kfml_0807.png" alt="KFServing Infrastructure Stack" width="1330" height="867">
<h6>Figure 8-7. KFServing infrastructure stack</h6>
</div></figure>










<section data-type="sect3" class="less_space pagebreak-before" data-pdf-bookmark="Going layer by layer"><div class="sect3" id="idm45831168244680" title2="Going layer by layer" no2="8.8.4.1">
<h3>8.8.4.1. Going layer by layer</h3>

<p>Hardware that runs your compute cluster is the base-building block for all the layers above.
Your cluster could run a variety of hardware devices including CPUs, GPUs, or even TPUs.
It is the responsibility of the layers above to simplify the toggling of hardware types and to abstract as much complexity as possible.</p>

<p>Kubernetes is the critical layer, right above the compute cluster,<a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="Kubernetes" id="idm45831168241944"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="KFServing infrastructure" id="idm45831168240696"></a> that manages, orchestrates, and deploys a variety of resources—successfully abstracting the underlying hardware.
The main resources we will focus on are deployments, horizontal pod autoscalers (HPA), and ingresses.
And since Kubernetes abstracts the underlying hardware, upon which deployments are run, this enables you to use hardware optimizers like GPUs within the upper levels of the stack.</p>

<p>Istio has been alluded to throughout this book, but we will talk about a few<a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="Istio" id="idm45831168238584"></a><a data-type="indexterm" data-primary="Istio" data-secondary="KFServing" data-tertiary="infrastructure" id="idm45831168237336"></a> of its features that are particularly relevant to KFServing.
Istio is an open source service mesh<a data-type="indexterm" data-primary="Istio" data-secondary="about" id="idm45831168235880"></a><a data-type="indexterm" data-primary="service mesh" data-secondary="with Istio" id="idm45831168234936"></a><a data-type="indexterm" data-primary="service mesh" data-secondary="about" id="idm45831168233992"></a> that layers transparently onto the Kubernetes cluster.
It integrates into any logging platform, telemetry system, or policy system and promotes a uniform way to secure, connect, and monitor microservices.
But what is a service mesh? Traditionally, each service instance is co-located with a sidecar network proxy.
All network traffic (<code>HTTP</code>, <code>REST</code>, <code>gRPC</code>, etc.) from an individual service instance flows via its local sidecar proxy to the appropriate destination.
Thus, the service instance is not aware of the network at large and only knows about its local proxy.
In effect, the distributed system network has been abstracted away from the service programmer.
Primarily, Istio expands upon Kubernetes resources, like ingresses, to provides service mesh fundamentals like:</p>

<ul>
<li>
<p>Authentication/Access control</p>
</li>
<li>
<p>Ingress and egress policy management</p>
</li>
<li>
<p>Distributed tracing</p>
</li>
<li>
<p>Federation via multicluster ingress and routing</p>
</li>
<li>
<p>Intelligent traffic management</p>
</li>
</ul>

<p>These tools are all critical for production inference applications that require administration, security, and monitoring.</p>

<p>The last component of the KFServing infrastructure stack is Knative, <a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="Knative" id="idm45831168224856"></a><a data-type="indexterm" data-primary="Knative" data-secondary="KFServing infrastructure" id="idm45831168223608"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="Knative" id="idm45831168222696"></a>which takes advantage of the abstractions that Istio provides.
The KFServing project primarily borrows from Knative Serving and Eventing, the latter of which will be expanded on in <a data-type="xref" href="#knative_eventing">SECTION 8.8.4.5</a>.
As we described in <a data-type="xref" href="#knative_serving">SECTION 3.2.3</a>, Knative Serving builds<a data-type="indexterm" data-primary="Knative" data-secondary="Serving" data-tertiary="KFServing" id="idm45831168219816"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="Knative Serving" id="idm45831168218728"></a> on Kubernetes and Istio to support deploying and serving serverless applications.
By building atop Kubernetes resources like deployments and HPAs, and Istio resources, like virtual services, Knative Serving provides:</p>

<ul>
<li>
<p>An abstracted service mesh</p>
</li>
<li>
<p>CPU/GPU autoscaling (either queries per second (QPS) or metric-based)</p>
</li>
<li>
<p>Revision<a data-type="indexterm" data-primary="GPUs" data-secondary="autoscaling in KFServing" id="idm45831168214312"></a> management for safe rollouts and canary/pinned deployment strategies</p>
</li>
</ul>

<p>These offerings are desirable for data scientists who want to limit their focus and energy to model development, and have scaling and versioning be handled for them in a managed way.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Escape hatches"><div class="sect3" id="escape_hatches" title2="Escape hatches" no2="8.8.4.2">
<h3>8.8.4.2. Escape hatches</h3>

<p>KFServing’s extensibility features escape hatches to the underlying layers of its stack.<a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="escape hatches" id="ch08-esch3"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="InferenceService" data-tertiary="escape hatches" id="ch08-esch"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="InferenceService escape hatches" id="ch08-esch2"></a>
By building escape hatches into the <code>InferenceService</code> CRD, data scientists can further tune their production inference
offering for security at the Istio level and their performance at the Knative level.</p>

<p>We will now walk through one example of how you can leverage these escape hatches,<a data-type="indexterm" data-primary="KFServing" data-secondary="InferenceService" data-tertiary="autoscaling via escape hatches" id="ch08-ausch"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="autoscaling via escape hatches" id="ch08-ausch2"></a><a data-type="indexterm" data-primary="scalability of Kubeflow" data-secondary="KFServing for inferencing" data-tertiary="autoscaling via escape hatches" id="ch08-ausch3"></a> by tuning the autoscaling of your InferenceService.</p>

<p>To understand how to use this escape hatch, you need to understand how Knative enables autoscaling.
There is a proxy in Knative Serving Pods called the queue proxy, which is responsible for enforcing request queue parameters (concurrency limits),
and reporting concurrent client metrics to the autoscaler.
The autoscaler, in turn, reacts to these metrics by bringing pods up and down.
Every second, the queue proxy publishes the observed number of concurrent requests in that time period.
KFServing by default sets the target concurrency (average number of in-flight requests per pod) to one.
If we were to load the service with five concurrent requests, the autoscaler would try to scale up to five pods.
You can customize the target concurrency by adding the example annotation <code>autoscaling.knative.dev/target</code>.</p>

<p>Let’s look again at your InferenceService from <a data-type="xref" href="#KFServing_Recommender_InfServ">EXAMPLE 8-31</a>.</p>

<pre data-type="programlisting" data-code-language="yaml" id="untitled_programlisting_26" title2="(no caption)" no2=""><code class="nt">apiVersion</code><code class="p">:</code> <code class="s">"serving.kubeflow.org/v1alpha2"</code>
<code class="nt">kind</code><code class="p">:</code> <code class="s">"InferenceService"</code>
<code class="nt">metadata</code><code class="p">:</code>
 <code class="nt">name</code><code class="p">:</code> <code class="s">"recommender"</code>
 <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">my-namespace</code>
<code class="nt">spec</code><code class="p">:</code>
 <code class="nt">default</code><code class="p">:</code>
   <code class="nt">predictor</code><code class="p">:</code>
     <code class="nt">tensorflow</code><code class="p">:</code>
       <code class="nt">serviceAccount</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
       <code class="nt">storageUri</code><code class="p">:</code> <code class="s">"s3://models/recommender"</code></pre>

<p>If you test this service by sending traffic in 30-second spurts while maintaining 5 in-flight requests,
you will see that the autoscaler scales up your inference services to 5 pods.<sup><a data-type="noteref" id="idm45831168193512-marker" href="#idm45831168193512">[21]</a></sup></p>
<div data-type="note"><h6>Note</h6>
<p>There will be a cold-start time cost as a result of initially spawning pods and downloading the model, before being ready to serve.
The cold start may take longer (to pull the serving image) if the image is not cached on the node that the pod is scheduled on.</p>
</div>

<p>By applying the annotation <code>autoscaling.knative.dev/target</code>, as seen in <a data-type="xref" href="#target_concurrency_annos_KFSInfServ">EXAMPLE 8-33</a>, the target concurrency will be set to five.</p>
<div id="target_concurrency_annos_KFSInfServ" data-type="example" title2="Custom target concurrency via annotations in KFServing InferenceService" no2="8-33">
<h5>Example 8-33. Custom target concurrency via annotations in KFServing InferenceService</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="s">"serving.kubeflow.org/v1alpha2"</code>
<code class="nt">kind</code><code class="p">:</code> <code class="s">"InferenceService"</code>
<code class="nt">metadata</code><code class="p">:</code>
 <code class="nt">name</code><code class="p">:</code> <code class="s">"recommender"</code>
 <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">my-namespace</code>
 <code class="nt">annotations</code><code class="p">:</code>
   <code class="nt">autoscaling.knative.dev/target</code><code class="p">:</code> <code class="s">"5"</code>
<code class="nt">spec</code><code class="p">:</code>
 <code class="nt">default</code><code class="p">:</code>
   <code class="nt">predictor</code><code class="p">:</code>
     <code class="nt">tensorflow</code><code class="p">:</code>
       <code class="nt">serviceAccount</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
       <code class="nt">storageUri</code><code class="p">:</code> <code class="s">"s3://models/recommender"</code></pre></div>

<p>Which means, that if you load the service with five concurrent requests,
you will see that you only need one pod for your inference service.<a data-type="indexterm" data-startref="ch08-esch" id="idm45831168051768"></a><a data-type="indexterm" data-startref="ch08-esch2" id="idm45831168094216"></a><a data-type="indexterm" data-startref="ch08-esch3" id="idm45831168093608"></a><a data-type="indexterm" data-startref="ch08-ausch" id="idm45831168092968"></a><a data-type="indexterm" data-startref="ch08-ausch2" id="idm45831168092296"></a><a data-type="indexterm" data-startref="ch08-ausch3" id="idm45831168091624"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Debugging an InferenceService"><div class="sect3" id="idm45831168211608" title2="Debugging an InferenceService" no2="8.8.4.3">
<h3>8.8.4.3. Debugging an InferenceService</h3>

<p>With a fully abstracted interface, InferenceService enables many features<a data-type="indexterm" data-primary="KFServing" data-secondary="InferenceService" data-tertiary="debugging" id="idm45831168089448"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="InferenceService debugging" id="idm45831168088200"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="debugging InferenceService" id="idm45831168086968"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="debugging InferenceService" id="idm45831168085736"></a><a data-type="indexterm" data-primary="debugging" data-secondary="KFServing" data-tertiary="InferenceService" id="idm45831168084776"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="debugging InferenceService" id="idm45831168083560"></a> while giving minimal exposure to the complexity under the hood.
To properly debug your InferenceService, let’s look at the request flow upon hitting your InferenceService.</p>

<p>The request flow when hitting your inference service, illustrated in <a data-type="xref" href="#kfservingrequest_figure">FIGURE 8-8</a>, is as follows:</p>
<ol class="less_space pagebreak-before">
<li>
<p>Traffic arrives through the Istio ingress gateway when traffic is external and through the Istio cluster local gateway when traffic is internal.</p>
</li>
<li>
<p>KFServing creates an Istio VirtualService to specify its top-level routing rules for all of its components. As such, traffic routes to that top-level VirtualService from the gateway.</p>
</li>
<li>
<p>Knative creates an Istio virtual service to configure the gateway to route the user traffic to the desired revision. Upon opening up the destination rules, you will see that the destination is a Kubernetes service for the latest ready Knative revision.</p>
</li>
<li>
<p>Once the revision pods are ready, the Kubernetes service will send the request to the queue-proxy.</p>

<ul>
<li>
<p>If the queue proxy has more requests than it can handle, based on the concurrency of the KFServing container, then the autoscaler will create more pods to handle the additional requests.</p>
</li>
</ul>
</li>
<li>
<p>Lastly, the queue proxy will send traffic to the KFServing controller.</p>
</li>

</ol>

<figure><div id="kfservingrequest_figure" class="figure" data-type="figure" title2="KFServing request flow" no2="8-8">
<img src="assets/kfml_0811.png" alt="KFServing Request Flow" width="1418" height="1126">
<h6>Figure 8-8. KFServing request flow</h6>
</div></figure>

<p>Where does this come in handy? Well, say you create your InferenceService but the <code>Ready</code> status is <code>false</code>:</p>

<pre data-type="programlisting" data-code-language="bash" id="untitled_programlisting_27" title2="(no caption)" no2="">kubectl get inferenceservice -n my-namespace recommender
NAME          URL   READY   DEFAULT TRAFFIC   CANARY TRAFFIC   AGE
recommender         False                                      3s</pre>

<p>You can step through the resources that are created in the request flow and view each of their status objects to understand what the blocker is.<sup><a data-type="noteref" id="idm45831168056504-marker" href="#idm45831168056504">[22]</a></sup></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Debugging performance"><div class="sect3" id="idm45831168055256" title2="Debugging performance" no2="8.8.4.4">
<h3>8.8.4.4. Debugging performance</h3>

<p>What if you deployed your InferenceService but its performance does<a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="debugging performance" id="idm45831168068664"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="debugging performance" id="idm45831168067416"></a><a data-type="indexterm" data-primary="debugging" data-secondary="KFServing" data-tertiary="performance" id="idm45831168066472"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="debugging performance" id="idm45831168065256"></a> not meet your expectations?
KFServing provides various dashboards and tools to help investigate such issues. Using Knative, KFServing has many resources in its <a href="https://oreil.ly/R5ASm">“debugging performance issues” guide</a>. You can also follow <a href="https://oreil.ly/MSiNX">this Knative guide</a> to access Prometheus and Grafana. Lastly, you can use request tracing, also known as distributed tracing, to see how much time is spent in each step of KFServing’s request flow in <a data-type="xref" href="#kfservingrequest_figure">FIGURE 8-8</a>.
You can use <a href="https://oreil.ly/STu7g">this Knative guide</a> to access request traces.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Knative Eventing"><div class="sect3" id="knative_eventing" title2="Knative Eventing" no2="8.8.4.5">
<h3>8.8.4.5. Knative Eventing</h3>

<p>By bringing Knative into its stack, KFServing enabled serverless via <a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="Knative Eventing" id="idm45831168057960"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="serverless inferencing" id="idm45831168026360"></a><a data-type="indexterm" data-primary="serverless" data-secondary="KFServing" id="idm45831168025416"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="Knative Serving" id="idm45831168024472"></a><a data-type="indexterm" data-primary="Knative" data-secondary="Serving" data-tertiary="KFServing" id="idm45831168023256"></a><a data-type="indexterm" data-primary="Knative" data-secondary="KFServing infrastructure" id="idm45831168022040"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="Knative" id="idm45831168021128"></a><a data-type="indexterm" data-primary="Knative" data-secondary="Eventing" data-tertiary="online documentation" id="idm45831168019912"></a><a data-type="indexterm" data-primary="Knative" data-secondary="Eventing" data-tertiary="KFServing" id="idm45831168018696"></a><a data-type="indexterm" data-primary="events via Knative Eventing" data-secondary="KFServing" id="idm45831168017480"></a><a data-type="indexterm" data-primary="events via Knative Eventing" data-secondary="online documentation" id="idm45831168016568"></a>Knative Serving and the use of event sources and event consumers via Knative Eventing.<sup><a data-type="noteref" id="idm45831168015528-marker" href="#idm45831168015528">[23]</a></sup>
We will take a look at how Knative Eventing works, and how you can extend your inference service with an event source.</p>

<p>Knative Eventing enforces a lambda-style architecture of event sources and event consumers with the following design principles:</p>

<ul>
<li>
<p>Knative Eventing services are loosely coupled.</p>
</li>
<li>
<p>Event producers and event consumers are independent. Any producer or Source can generate events before there are active event consumers listening. Any event consumer can express interest in an event before there are producers that are creating those events.</p>
</li>
<li>
<p>Other services can be connected to any Eventing system that:</p>

<ul>
<li>
<p>Creates new applications without modifying the event producer or event-consumer.</p>
</li>
<li>
<p>Selects and targets specific subsets of events from their producers.</p>
</li>
</ul>
</li>
</ul>

<p>Knative Eventing delivers events in two flavors: direct delivery from a source to a single service and fan-out delivery
from a source to multiple endpoints using channels and subscriptions.</p>

<p>There are a variety of sources<sup><a data-type="noteref" id="idm45831167990536-marker" href="#idm45831167990536">[24]</a></sup> that come out-of-the-box when installing Knative Eventing, one of which is KafkaSource.<sup><a data-type="noteref" id="idm45831167989016-marker" href="#idm45831167989016">[25]</a></sup> If you look at <a data-type="xref" href="#KafkaS_sends_events_KFS_Rec_InfServ">EXAMPLE 8-34</a>, you will see how you would use <a data-type="indexterm" data-primary="Knative" data-secondary="Eventing" data-tertiary="KafkaSource to send events" id="idm45831167986792"></a><a data-type="indexterm" data-primary="KafkaSource to send Knative events" id="idm45831167985512"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="InferenceService" data-tertiary="KafkaSource to send Knative events" id="idm45831167984824"></a><a data-type="indexterm" data-primary="YAML" data-secondary="KafkaSource to send Knative events" id="idm45831167983592"></a><a data-type="indexterm" data-primary="events via Knative Eventing" data-secondary="KafkaSource to send events" id="idm45831167982632"></a>KafkaSource to send events, received by Kafka, to your recommender example.</p>
<div id="KafkaS_sends_events_KFS_Rec_InfServ" data-type="example" title2="KafkaSource that sends events to a KFServing Recommender InferenceService" no2="8-34">
<h5>Example 8-34. KafkaSource that sends events to a KFServing Recommender InferenceService</h5>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">sources.knative.dev/v1alpha1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">KafkaSource</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">kafka-source</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">consumerGroup</code><code class="p">:</code> <code class="l-Scalar-Plain">knative-group</code>
  <code class="c1"># Broker URL. Replace this with the URLs for your Kafka cluster, which</code>
  <code class="c1"># is in the format of my-cluster-kafka-bootstrap.my-kafka-namespace:9092.</code>
  <code class="nt">bootstrapServers</code><code class="p">:</code> <code class="l-Scalar-Plain">my-cluster-kafka-bootstrap.my-kafka-namespace:9092.</code>
  <code class="nt">topics</code><code class="p">:</code> <code class="l-Scalar-Plain">recommender</code>
  <code class="nt">sink</code><code class="p">:</code>
    <code class="nt">ref</code><code class="p">:</code>
      <code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">serving.kubeflow.org/v1alpha2</code>
      <code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">InferenceService</code>
      <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">recommender</code></pre></div>

<p>As you can see by the simplicity of this specification, after setting up your Kafka resources, hooking Kafka into
your InferenceService is as simple as 13 lines of YAML.
You can find a more advanced end-to-end example with MinIO and Kafka on <a href="https://oreil.ly/_8oSZ">this Kubeflow GitHub site</a>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Additional features"><div class="sect3" id="idm45831167937496" title2="Additional features" no2="8.8.4.6">
<h3>8.8.4.6. Additional features</h3>

<p>KFServing contains a host of features that are continuously being improved.<a data-type="indexterm" data-primary="KFServing" data-secondary="capabilities of" id="idm45831167936296"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="capabilities of" id="idm45831167935320"></a> A comprehensive list of its capabilities can be found
<a href="https://oreil.ly/nJTCV">on this GitHub site</a>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="API documentation"><div class="sect3" id="idm45831167933064" title2="API documentation" no2="8.8.4.7">
<h3>8.8.4.7. API documentation</h3>

<p>For more on the APIs, consult the references for the<a data-type="indexterm" data-primary="KFServing" data-secondary="API documentation" id="idm45831167931640"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="API documentation" id="idm45831167930664"></a><a data-type="indexterm" data-primary="Python" data-secondary="KFServing API documentation" id="idm45831167929720"></a> <a href="https://oreil.ly/iCBAD">KFServing Kubernetes APIs</a> and the KFServing Python <a href="https://oreil.ly/klfBR">KFServing Python APIs</a>.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Review"><div class="sect2" id="idm45831168277112" title2="Review" no2="8.8.5">
<h2>8.8.5. Review</h2>

<p>Building serverless on top of Seldon Core’s graph inferencing, KFServing<a data-type="indexterm" data-primary="KFServing" data-secondary="about" id="idm45831167925656"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="about" id="idm45831167924680"></a> has produced a complete inference solution
that sufficiently fills all the gaps of TFServing and Seldon Core. KFServing works to unify the entire community of model servers by running model servers as Knative components.
With all of its functionality and promise, we will take a look at how KFServing manages to satisfy all your inference requirements.</p>










<section data-type="sect3" data-pdf-bookmark="Model serving"><div class="sect3" id="idm45831167922840" title2="Model serving" no2="8.8.5.1">
<h3>8.8.5.1. Model serving</h3>

<p>KFServing makes graph inference and advanced ML insights first-class<a data-type="indexterm" data-primary="KFServing" data-secondary="model serving" id="idm45831167921544"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="model serving" id="idm45831167920568"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model serving" data-tertiary="KFServing" id="idm45831167919352"></a><a data-type="indexterm" data-primary="model serving" data-secondary="KFServing" id="idm45831167918136"></a><a data-type="indexterm" data-primary="data plane of KFServing" id="idm45831167917192"></a> while also defining a data plane that is extremely extensible for pluggable components. This flexibility allows data scientists to focus on ML insights without having to strain over how to include them in the graph.</p>

<p>KFServing is not only versatile in that it provides serving flexibility for a variety of frameworks, but it also standardizes the data plane across differing frameworks to reduce complexity in switching between model servers.
It codifies the Kubernetes design pattern by moving common functionalities like request batching, logging, and pipelining into a sidecar. This, in turn, slims down the model server and creates a separation of concerns, as model services without these features can immediately benefit from deploying onto KFServing.
It also provides support for <code>REST</code>, <code>gRPC</code>, and GPU acceleration and can interface with streaming inputs using Knative Eventing.
And lastly, thanks to Knative Serving, KFServing provides <a data-type="indexterm" data-primary="GPUs" data-secondary="autoscaling in KFServing" id="idm45831167914152"></a>GPU autoscaling, which you expect from  hardware-agnostic autoscaling.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Model monitoring"><div class="sect3" id="idm45831167912776" title2="Model monitoring" no2="8.8.5.2">
<h3>8.8.5.2. Model monitoring</h3>

<p>By taking from Seldon Core and its infrastructure stack, KFServing <a data-type="indexterm" data-primary="KFServing" data-secondary="model monitoring" id="idm45831167911480"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="model monitoring" id="idm45831167910504"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model monitoring" data-tertiary="KFServing" id="idm45831167909288"></a><a data-type="indexterm" data-primary="models" data-secondary="monitoring" data-tertiary="KFServing" id="idm45831167908072"></a><a data-type="indexterm" data-primary="model serving" data-secondary="monitoring" data-tertiary="KFServing" id="idm45831167906856"></a><a data-type="indexterm" data-primary="monitoring" data-secondary="KFServing" id="idm45831167905640"></a>meets all of your model monitoring needs.
KFServing leverages the sophisticated model explainers and drift detectors of Seldon Core in a first-class way,
while also paving a way for developers to define their own monitoring components in a highly flexible yet powerful data plane.</p>

<p>Furthermore, with all the networking capabilities enabled by having Istio and Knative in its infrastructure stack, KFServing provides extensible <a data-type="indexterm" data-primary="KFServing" data-secondary="network monitoring and telemetry" id="idm45831167903720"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="network monitoring and telemetry" id="idm45831167902728"></a>network monitoring and telemetry with support for Prometheus, Grafana, Kibana, Zipkin, and Jaeger, to name a few. These all satisfy your needs to monitor for Kubernetes metrics (memory/CPU container limits) and server metrics (queries per second and distributed tracing).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Model updating"><div class="sect3" id="idm45831167900824" title2="Model updating" no2="8.8.5.3">
<h3>8.8.5.3. Model updating</h3>

<p>KFServing’s use of Knative was strategic in providing sophisticated model updating features.<a data-type="indexterm" data-primary="KFServing" data-secondary="model updating" id="idm45831167899176"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="model updating" id="idm45831167898200"></a><a data-type="indexterm" data-primary="model inference" data-secondary="model updating" data-tertiary="KFServing" id="idm45831167896984"></a><a data-type="indexterm" data-primary="models" data-secondary="updating" data-tertiary="KFServing" id="idm45831167895768"></a><a data-type="indexterm" data-primary="model serving" data-secondary="model updating" data-tertiary="KFServing" id="idm45831167894552"></a>
As such, KFServing satisfies all of your requirements regarding <a data-type="indexterm" data-primary="KFServing" data-secondary="deployment strategies" id="idm45831167893208"></a><a data-type="indexterm" data-primary="model inference" data-secondary="KFServing" data-tertiary="deployment strategies" id="idm45831167892264"></a>deployment strategies and version rollouts.</p>

<p>By leveraging Istio’s virtual services and the simplicity of an abstracted CRD,<a data-type="indexterm" data-primary="blue-green deployment" data-secondary="KFServing endpoints" id="idm45831167862504"></a><a data-type="indexterm" data-primary="canary deployments" data-secondary="KFServing endpoints" id="idm45831167861592"></a><a data-type="indexterm" data-primary="pinned deployments" data-secondary="KFServing endpoints" id="idm45831167860648"></a> KFServing makes the toggling of deployment strategies
simple. It makes the flow from blue-green → pinned → canary as simple as changing a few lines of YAML. Furthermore, with the diverse and ever-expanding features of its underlying stack, KFServing is easily extensible to support more-complicated deployment strategies like <a data-type="indexterm" data-primary="multi-armed bandits" id="idm45831167859224"></a>multi-armed bandits.<sup><a data-type="noteref" id="idm45831167858424-marker" href="#idm45831167858424">[26]</a></sup></p>

<p>By using Knative Serving, KFServing adopts revision management<a data-type="indexterm" data-primary="Knative" data-secondary="Serving" data-tertiary="KFServing" id="idm45831167856648"></a><a data-type="indexterm" data-primary="KFServing" data-secondary="infrastructure stack" data-tertiary="Knative Serving" id="idm45831167855320"></a> that makes Kubernetes deployment immutable. This ensures safe rollout by health checking the new revisions pods before moving over the traffic. A revision enables:</p>

<ul>
<li>
<p>Automated and safe rollouts</p>
</li>
<li>
<p>Bookkeeping for all revisions previously created</p>
</li>
<li>
<p>Rollbacks to known, good configurations</p>
</li>
</ul>

<p>This sufficiently satisfies your versioning requirements for models in development, in flight, and in production.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Summary"><div class="sect3" id="idm45831167850008" title2="Summary" no2="8.8.5.4">
<h3>8.8.5.4. Summary</h3>

<p>KFServing has developed a sophisticated inference solution that abstracts its complexity for day-one users while
also enabling power users to take advantage of its diverse feature set.
Building cloud native, KFServing seamlessly sits atop Kubeflow and finalizes the MDLC with its inference solution.<a data-type="indexterm" data-startref="ch08-kfs" id="idm45831167848360"></a><a data-type="indexterm" data-startref="ch08-kfs2" id="idm45831167847656"></a></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45831167846728" title2="Conclusion" no2="8.9">
<h1>8.9. Conclusion</h1>

<p>In this chapter we investigated various inference solutions that can be used within Kubeflow.</p>

<p>Based on what inference requirements you wish to prioritize and how deep you want your infrastructure stack to be, each of the solutions described has distinctive advantages.
Having reviewed each of the offerings in detail, it might be worthwhile to reconsider <a data-type="xref" href="#comparing_different_model_inference_approaches">TABLE 8-2</a> and see which inference solution is appropriate for your use case:</p>

<ul>
<li>
<p>TFServing provides extremely performant and sophisticated out-of-the-box integration for TensorFlow models.</p>
</li>
<li>
<p>Seldon Core provides extensibility and sophisticated out-of-the-box support for complex inference graphs and model insight.</p>
</li>
<li>
<p>KFServing provides a simpler opinionated deployment definition with serverless capabilities.</p>
</li>
</ul>

<p>However, technology and development are shared between all these projects, and looking to the future, Seldon Core will even support the
new KFServing data plane with the goal to provide easy interoperability and conversion.
Other exciting features to expect from KFServing include multi-model serving, progressive rollouts, and more advanced graph
inferencing techniques like pipelines and multi-armed bandit.</p>

<p>Now that you have completed the final step in your MDLC story, we will see how you can further customize Kubeflow to enable more advanced features in the next 
chapter.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45831170778040"><sup><a href="#idm45831170778040-marker">[1]</a></sup> If you are interested in learning more about model embedding, we suggest reading <a class="orm:hideurl" href=""><em>Serving Machine Learning Models</em></a> by Boris Lublinsky (O’Reilly).</p><p data-type="footnote" id="idm45831170715736"><sup><a href="#idm45831170715736-marker">[2]</a></sup> Some references include: <a href="">“Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift”</a>, <a href="">“Detecting and Correcting for Label Shift with Black Box Predictors”</a>, <a href="">“A Kernel Two-Sample Test”</a>, and <a href="">“Monitoring and Explainability of Models in Production”</a>.</p><p data-type="footnote" id="idm45831170617576"><sup><a href="#idm45831170617576-marker">[3]</a></sup> Refer to the <a href="">TensorFlow documentation</a> for details on using TFServing locally.</p><p data-type="footnote" id="idm45831170616056"><sup><a href="#idm45831170616056-marker">[4]</a></sup> Refer to the <a href="">TensorFlow documentation</a> for details on using TFServing on Kubernetes.</p><p data-type="footnote" id="idm45831170410888"><sup><a href="#idm45831170410888-marker">[5]</a></sup> If you are using Istio as a service mesh, follow these <a href="">instructions</a> to add a <a href="">virtual service</a>.</p><p data-type="footnote" id="idm45831170375912"><sup><a href="#idm45831170375912-marker">[6]</a></sup> You can, of course, scale it manually by changing the amount of deployed instances.</p><p data-type="footnote" id="idm45831170326744"><sup><a href="#idm45831170326744-marker">[7]</a></sup> See TFServing’s deployment strategy <a href="">configuration</a> for more information.</p><p data-type="footnote" id="idm45831170291704"><sup><a href="#idm45831170291704-marker">[8]</a></sup> Refer to the Seldon documentation for integration with <a href="">Prometheus</a>, <a href="">ELK</a>, and <a href="">Jaeger</a>.</p><p data-type="footnote" id="idm45831170170360"><sup><a href="#idm45831170170360-marker">[9]</a></sup> Currently supported prepackaged servers include MLflow server, SKLearn server, TensorFlow serving, and XGBoost server.</p><p data-type="footnote" id="idm45831170169576"><sup><a href="#idm45831170169576-marker">[10]</a></sup> Currently supported is a language server for Python. Incubating are Java, R, NodeJS, and Go.</p><p data-type="footnote" id="idm45831170150904"><sup><a href="#idm45831170150904-marker">[11]</a></sup> Because Seldon implements the computational structure as a tree, the combiner executes in reverse order to combine output from all children.</p><p data-type="footnote" id="idm45831169891080"><sup><a href="#idm45831169891080-marker">[12]</a></sup> You can also send requests using the Python client.</p><p data-type="footnote" id="idm45831169736392"><sup><a href="#idm45831169736392-marker">[13]</a></sup> A SeldonMessage can be defined as both an <a href="">OpenAPI specification</a> and a <a href="">protobuffer definition</a>.</p><p data-type="footnote" id="idm45831169631800"><sup><a href="#idm45831169631800-marker">[14]</a></sup> For more on how to enable this, see this <a href="">Seldon documentation page</a>.</p><p data-type="footnote" id="idm45831169343736"><sup><a href="#idm45831169343736-marker">[15]</a></sup> See <a data-type="xref" href="#model_building_income">SECTION 7.4</a> for more information on this model and how it is built.</p><p data-type="footnote" id="idm45831169035576"><sup><a href="#idm45831169035576-marker">[16]</a></sup> See the <a href="">Seldon Core documentation</a> for further details.</p><p data-type="footnote" id="idm45831168939448"><sup><a href="#idm45831168939448-marker">[17]</a></sup> KFServing is continuously evolving, as is its protocol. You can preview the V2 protocol <a href="">on this Kubeflow GitHub site</a>. <a data-type="indexterm" data-primary="KFServing" data-secondary="data plane" data-tertiary="prediction protocol in" id="idm45831168938008" href=""></a>The second version of the data plane protocol addresses several issues found in the V1 data plane protocol, including performance and generality across a large number of model frameworks and servers.</p><p data-type="footnote" id="idm45831168910168"><sup><a href="#idm45831168910168-marker">[18]</a></sup> KFServing also supports standalone installation without Kubeflow. In fact, most production users of KFServing run it as a standalone installation.</p><p data-type="footnote" id="idm45831168432584"><sup><a href="#idm45831168432584-marker">[19]</a></sup> You can still predict against a certain version by passing in a Host-Header in your request. For more information on rollouts, see <a href="">this GitHub repo</a>.</p><p data-type="footnote" id="idm45831168348568"><sup><a href="#idm45831168348568-marker">[20]</a></sup> You can install the SDK by running <code>pip install kfserving</code>. You can get the KFServing SDK documentation<a data-type="indexterm" data-primary="KFServing" data-secondary="SDK documentation" id="idm45831168295176" href=""></a><a data-type="indexterm" data-primary="KFServing" data-secondary="InferenceService" data-tertiary="examples" id="idm45831168294200" href=""></a> <a href="">on this GitHub site</a> and examples <a href="">on this GitHub site</a> for creating, rolling out, promoting, and deleting an <code>InferenceService</code>.</p><p data-type="footnote" id="idm45831168193512"><sup><a href="#idm45831168193512-marker">[21]</a></sup> You can further explore load testing on <a href="">this Kubeflow GitHub site</a>. Two great load-testing frameworks are <a href="">Hey</a> and <a href="">Vegeta</a>.</p><p data-type="footnote" id="idm45831168056504"><sup><a href="#idm45831168056504-marker">[22]</a></sup> A detailed debugging guide can be found on  <a href="">this Kubeflow GitHub site</a>.</p><p data-type="footnote" id="idm45831168015528"><sup><a href="#idm45831168015528-marker">[23]</a></sup> To learn more about Knative Eventing, see <a href="">the documentation</a>.</p><p data-type="footnote" id="idm45831167990536"><sup><a href="#idm45831167990536-marker">[24]</a></sup> Knative has a <a href="">nonexhaustive list of event sources</a>.</p><p data-type="footnote" id="idm45831167989016"><sup><a href="#idm45831167989016-marker">[25]</a></sup> To learn more about KafkaSource, see <a href="">the documentation</a>.</p><p data-type="footnote" id="idm45831167858424"><sup><a href="#idm45831167858424-marker">[26]</a></sup> Check out examples of how ML Graph can be used to build complex graphs of ML components on <a href="">this Seldon GitHub site</a>.</p></div></div></section>
<section data-type="chapter" data-pdf-bookmark="Chapter 9. Case Study Using Multiple Tools"><div class="chapter" id="beyond_tf" data-type="chapter" title2="Case Study Using Multiple Tools" no2="9">
<h1>Chapter 9. Case Study Using Multiple Tools</h1>


<p>In this chapter we’re going to discuss what to do if you need to use “other” tools for your particular data science pipeline. Python has a plethora of tools for handling a wide array of data formats. RStats has a large repository of advanced math functions. Scala is the default language of big data processing engines such as Apache Spark and Apache Flink. Legacy programs that would be costly to reproduce exist in any number of languages.</p>

<p>A very important benefit of Kubeflow is that users no longer need to choose which <a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="language capabilities" id="idm45831167835560"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="language capabilities" data-tertiary="denoising CT scan case study" id="ch09-ct"></a><a data-type="indexterm" data-primary="language capabilities of pipelines" id="idm45831167833080"></a><a data-type="indexterm" data-primary="denoising data" data-secondary="CT scan case study" id="ch09-ct8"></a><a data-type="indexterm" data-primary="language capabilities of pipelines" data-secondary="denoising CT scan case study" id="ch09-ct2"></a>language is best for their entire pipeline
but can instead use the best language for each job (as long as the language and code are containerizable).</p>

<p>We will demonstrate these concepts through a comprehensive example denoising CT scans.<a data-type="indexterm" data-primary="denoising data" data-secondary="about CT scan data" id="idm45831167829256"></a><a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="about data" id="idm45831167828280"></a> Low-dose CT scans allow clinicians to use the scans as a diagnostic tool by delivering a fraction of the radiation dose—however, these scans often
suffer from an increase in white noise. CT scans come in a format known as DICOM, and we’ll use a container with a specialized library called <code>pydicom</code> to load and process the data into a <code>numpy</code> matrix.</p>

<p>Several methods for denoising CT scans exist; however, they often focus on the mathematical justification, not the implementation.
We will present an <a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="singular value decomposition" id="idm45831167825272"></a><a data-type="indexterm" data-primary="denoising data" data-secondary="singular value decomposition" id="idm45831167824280"></a><a data-type="indexterm" data-primary="singular value decomposition (SVD)" id="idm45831167823320"></a><a data-type="indexterm" data-primary="denoising data" data-secondary="open source method" id="idm45831167822632"></a><a data-type="indexterm" data-primary="data cleaning" data-secondary="open source method" id="idm45831167821688"></a><a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="open source method" id="idm45831167820744"></a><a data-type="indexterm" data-primary="distributed stochastic singular value decomposition (DS-SVD)" id="idm45831167819800"></a>open source method that uses a <em>singular value decomposition</em> (SVD) to break the image into components, the “least important” of which are often the noise. We use Apache Spark with the Apache Mahout library to do a singular value decomposition. Finally, we use Python again to denoise the CT scans and visualize the results.</p>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="The Denoising CT Scans Example"><div class="sect1" id="idm45831167818232" title2="The Denoising CT Scans Example" no2="9.1">
<h1>9.1. The Denoising CT Scans Example</h1>

<p>Computed tomography (CT) scans are used for a wide array of medical purposes. The scans work by taking X-rays from multiple angles and forming image “slices” that can then be stacked to create a 3D image of a person’s insides. In the United States, health experts recommend a person receive no more than 100 milliSieverts (mSv) of radiation throughout their lives, which is equivalent to about 25 chest CT scans (at ~7 mSv each).<a data-type="indexterm" data-primary="Apache Spark" data-secondary="data denoising example id=ch09-dd5" id="idm45831167815880"></a></p>

<p>In the late twentieth and early twenty-first century, much research was done on what are known as “low-dose” CT scans.<a data-type="indexterm" data-primary="denoising data" data-secondary="about CT scan data" id="idm45831167814312"></a><a data-type="indexterm" data-primary="data cleaning" data-secondary="about CT scan data" id="idm45831167813336"></a><a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="about data" id="idm45831167812392"></a> A low-dose chest CT scan only delivers 1 to 2 mSv of radiation, but at a cost of a much noisier image, which can be harder to read. These scans are popular tools for screening for lung cancer among habitual smokers.</p>

<p>The cost of this low-dose CT scan is that the resultant image is lower quality, or noisier. In the 2000s, much research was done on denoising these low-dose CT scans.  Most of the papers present methods and results only (no code). Further, the FDA restricts what methods can be used for denoising CT scans, which has led to almost all solutions being proprietary and expensive. Denoising seeks to improve image quality by removing the white noise that is often present in these low-dose CT scans.</p>

<p>At the time of the writing of this book, the <a data-type="indexterm" data-primary="COVID-19 pandemic" id="idm45831167809800"></a>novel coronavirus more popularly known as COVID-19 has escalated into a global pandemic. It has been shown that chest CT scans are a more sensitive early-detection test than the reverse transcription polymerase chain reaction (RT-PCR) test, especially at early stages of infection.</p>

<p>As multiple repositories of CT scans are coming online and asking AI researchers to assist in fighting the pandemic, we have sought to add a <a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="open source method" id="idm45831167808184"></a><a data-type="indexterm" data-primary="denoising data" data-secondary="open source method" id="idm45831167807208"></a><a data-type="indexterm" data-primary="data cleaning" data-secondary="open source method" id="idm45831167806264"></a>method for denoising CT scans based entirely on off-the-shelf open source components. Namely we will use Python, Apache Spark, <a data-type="indexterm" data-primary="Apache Mahout" id="idm45831167805048"></a>Apache Mahout (a Spark library specializing in distributed linear algebra), and Kubeflow.</p>

<p>We will not delve into the math of what we are doing here, but we strongly encourage you to consult this <a data-type="indexterm" data-primary="denoising data" data-secondary="resource on math" id="idm45831167803624"></a><a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="resource on math" id="idm45831167802648"></a>paper.<sup><a data-type="noteref" id="idm45831167801576-marker" href="#idm45831167801576">[1]</a></sup></p>

<p>In this example, we will instead focus on the “how” of doing this technique with Kubeflow, and encourage readers to add their own steps at the end of this pipeline, which can then be freely shared with other researchers.</p>








<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Data Prep with Python"><div class="sect2" id="idm45831167799352" title2="Data Prep with Python" no2="9.1.1">
<h2>9.1.1. Data Prep with Python</h2>

<p>CT scan images are commonly stored in the DICOM format. In this format<a data-type="indexterm" data-primary="denoising data" data-secondary="data preparation" id="idm45831167797512"></a><a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="data preparation" id="idm45831167796456"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="CT scan data denoised" id="idm45831167795512"></a><a data-type="indexterm" data-primary="DICOM file format" id="idm45831167794568"></a><a data-type="indexterm" data-primary="data" data-secondary="DICOM file format" id="idm45831167793896"></a><a data-type="indexterm" data-primary="data" data-secondary="sources for datasets" id="idm45831167792952"></a> each “slice” of the image is stored in its own file, along with some metadata about the image, such as space between pixels, and space between slices. We want to read all of these files and create a 3D tensor of the pixel values. Then we want to “flatten” that tensor into a two-dimensional matrix, on which we can then perform a singular value decomposition.</p>

<p>There are several places where you can get DICOM file sets.<a data-type="indexterm" data-primary="DICOM file format" data-secondary="sources for datasets" id="idm45831167790888"></a><a data-type="indexterm" data-primary="repositories" data-secondary="COVID-19 CT scans" id="idm45831167789832"></a> For the paper, we retrieved some from <a href="https://coronacases.org"><em class="hyperlink">https://coronacases.org</em></a> (though downloading the
DICOMs can be a bit tricky). Other places you can find DICOM files are CT scans from the <a href="https://oreil.ly/fDXRn">Public Lung Image Database</a>, a CD you may have received from the doctor if you’ve ever had a CT scan, and other places online.<sup><a data-type="noteref" id="idm45831167786776-marker" href="#idm45831167786776">[2]</a></sup> The important thing is, we need one directory of DICOM files that comprise a single CT scan. We will assume there exists <em>some</em> DICOM file set comprising a single CT scan in the directory <code>/data/dicom</code>.</p>

<p>Converting a DICOM image into a tensor is shockingly easy, if you have the right dependencies in place. We will use <code>pydicom</code>, which is a well-supported Python interface for working with DICOM images. Unfortunately, the <code>pydicom</code> Docker images do not include Grassroots DICOM (GDCM), which is required for converting the DICOM into a pixel array. Our solution to this problem was to use the <code>pydicom</code> Docker container as a base image, then build a compatible GDCM version. The resulting image we’ve named <code>rawkintrevo/covid-prep-dicom</code>. With <code>pydicom</code> and GDCM it’s easy to convert DICOM images into tensors; we will use a Lightweight Python Function to do the rest (see <a data-type="xref" href="#Ltwt_Pythonf_converts_DICOMs">EXAMPLE 9-1</a>).</p>
<div id="Ltwt_Pythonf_converts_DICOMs" data-type="example" title2="Lightweight Python function converts DICOMs to tensors" no2="9-1">
<h5>Example 9-1. Lightweight Python function converts DICOMs to tensors</h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code><code> </code><code class="nf">dicom_to_matrix</code><code class="p">(</code><code class="n">input_dir</code><code class="p">:</code><code> </code><code class="nb">str</code><code class="p">,</code><code> </code><code class="n">output_file</code><code class="p">:</code><code> </code><code class="nb">str</code><code class="p">)</code><code> </code><code class="o">-</code><code class="o">&gt;</code><code> </code><code class="n">output_type</code><code class="p">:</code><code>
</code><code>    </code><code class="kn">import</code><code> </code><code class="nn">pydicom</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO1-1" href="#callout_case_study_using_multiple_tools_CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"></a><code>
</code><code>    </code><code class="kn">import</code><code> </code><code class="nn">numpy</code><code> </code><code class="kn">as</code><code> </code><code class="nn">np</code><code>
</code><code>
</code><code>    </code><code class="k">def</code><code> </code><code class="nf">dicom_to_tensor</code><code class="p">(</code><code class="n">path</code><code class="p">)</code><code class="p">:</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO1-2" href="#callout_case_study_using_multiple_tools_CO1-2"><img src="assets/2.png" alt="2" width="12" height="12"></a><code>
</code><code>        </code><code class="n">dicoms</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="n">pydicom</code><code class="o">.</code><code class="n">dcmread</code><code class="p">(</code><code class="n">f</code><code class="s2">"</code><code class="s2">{path}/{f}</code><code class="s2">"</code><code class="p">)</code><code> </code><code class="k">for</code><code> </code><code class="n">f</code><code> </code><code class="ow">in</code><code> </code><code class="n">listdir</code><code class="p">(</code><code class="n">path</code><code class="p">)</code><code class="p">]</code><code>
</code><code>        </code><code class="n">slices</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="n">d</code><code> </code><code class="k">for</code><code> </code><code class="n">d</code><code> </code><code class="ow">in</code><code> </code><code class="n">dicoms</code><code> </code><code class="k">if</code><code> </code><code class="nb">hasattr</code><code class="p">(</code><code class="n">d</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">SliceLocation</code><code class="s2">"</code><code class="p">)</code><code class="p">]</code><code>
</code><code>        </code><code class="n">slices</code><code> </code><code class="o">=</code><code> </code><code class="nb">sorted</code><code class="p">(</code><code class="n">slices</code><code class="p">,</code><code> </code><code class="n">key</code><code class="o">=</code><code class="k">lambda</code><code> </code><code class="n">s</code><code class="p">:</code><code> </code><code class="n">s</code><code class="o">.</code><code class="n">SliceLocation</code><code class="p">)</code><code>
</code><code>
</code><code>        </code><code class="n">img_shape</code><code> </code><code class="o">=</code><code> </code><code class="nb">list</code><code class="p">(</code><code class="n">slices</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">pixel_array</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code><code>
</code><code>        </code><code class="n">img_shape</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">slices</code><code class="p">)</code><code class="p">)</code><code>
</code><code>        </code><code class="n">img3d</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">zeros</code><code class="p">(</code><code class="n">img_shape</code><code class="p">)</code><code>
</code><code>
</code><code>        </code><code class="k">for</code><code> </code><code class="n">i</code><code class="p">,</code><code> </code><code class="n">s</code><code> </code><code class="ow">in</code><code> </code><code class="nb">enumerate</code><code class="p">(</code><code class="n">slices</code><code class="p">)</code><code class="p">:</code><code>
</code><code>            </code><code class="n">img2d</code><code> </code><code class="o">=</code><code> </code><code class="n">s</code><code class="o">.</code><code class="n">pixel_array</code><code>
</code><code>            </code><code class="n">img3d</code><code class="p">[</code><code class="p">:</code><code class="p">,</code><code> </code><code class="p">:</code><code class="p">,</code><code> </code><code class="n">i</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">img2d</code><code>
</code><code>
</code><code>        </code><code class="k">return</code><code> </code><code class="p">{</code><code class="s2">"</code><code class="s2">img3d</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="n">img3d</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">img_shape</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="n">img_shape</code><code class="p">}</code><code>
</code><code>
</code><code>    </code><code class="n">m</code><code> </code><code class="o">=</code><code> </code><code class="n">dicom_to_tensor</code><code class="p">(</code><code class="n">f</code><code class="s2">"</code><code class="s2">{input_dir}</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">np</code><code class="o">.</code><code class="n">savetxt</code><code class="p">(</code><code class="n">output_file</code><code class="p">,</code><code> </code><code class="n">m</code><code class="p">[</code><code class="s1">'</code><code class="s1">img3d</code><code class="s1">'</code><code class="p">]</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code><code class="n">m</code><code class="p">[</code><code class="s1">'</code><code class="s1">img_shape</code><code class="s1">'</code><code class="p">]</code><code class="p">[</code><code class="mi">2</code><code class="p">]</code><code class="p">)</code><code class="p">)</code><code class="p">,</code><code> </code><code class="n">delimiter</code><code class="o">=</code><code class="s2">"</code><code class="s2">,</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO1-3" href="#callout_case_study_using_multiple_tools_CO1-3"><img src="assets/3.png" alt="3" width="12" height="12"></a><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="bp">None</code><code>
</code><code>
</code><code>
</code><code class="n">dicom_to_matrix_op</code><code> </code><code class="o">=</code><code> </code><code class="n">comp</code><code class="o">.</code><code class="n">func_to_container_op</code><code class="p">(</code><code>
</code><code>        </code><code class="n">dicom_to_matrix</code><code class="p">,</code><code>
</code><code>        </code><code class="n">base_image</code><code class="o">=</code><code class="s1">'</code><code class="s1">rawkintrevo/covid-prep-dicom:0.8.0.0</code><code class="s1">'</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO1-1" href="#co_case_study_using_multiple_tools_CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>Our imports must occur within the function (not globally).</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO1-2" href="#co_case_study_using_multiple_tools_CO1-2"><img src="assets/2.png" alt="2" width="12" height="12"></a></dt>
<dd><p>This function reads the list of “slices,” which themselves are 2D images, and stacks them into a 3D tensor.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO1-3" href="#co_case_study_using_multiple_tools_CO1-3"><img src="assets/3.png" alt="3" width="12" height="12"></a></dt>
<dd><p>We use <code>numpy</code> to reshape the 3D tensor into a 2D matrix.</p></dd>
</dl></div>

<p>Next, let’s consider denoising our CT scan using Apache Spark and Apache Mahout.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="DS-SVD with Apache Spark"><div class="sect2" id="idm45831167238376" title2="DS-SVD with Apache Spark" no2="9.1.2">
<h2>9.1.2. DS-SVD with Apache Spark</h2>

<p>The mathematics behind distributed stochastic singular value decomposition (DS-SVD)<a data-type="indexterm" data-primary="denoising data" data-secondary="singular value decomposition" id="idm45831167237032"></a><a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="singular value decomposition" id="idm45831167236088"></a><a data-type="indexterm" data-primary="singular value decomposition (SVD)" id="idm45831167724008"></a><a data-type="indexterm" data-primary="distributed stochastic singular value decomposition (DS-SVD)" id="idm45831167723368"></a> are well beyond the scope of this book; however, we direct you to learn more in <em>Apache Mahout: Beyond MapReduce</em>, on the <a href="https://oreil.ly/T3VUE">Apache Mahout website</a>, or in the aforementioned paper.</p>

<p>We seek to decompose our CT scan into a set of features, and then<a data-type="indexterm" data-primary="denoising data" data-secondary="decomposing CT scan" id="idm45831167196808"></a><a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="decomposing CT scan" id="idm45831167195832"></a> drop the least important features, as these are probably noise. So let’s jump into decomposing a CT scan with Apache Spark and Apache Mahout.</p>

<p>A significant feature of Apache Mahout is its “R-Like” domain-specific language,<a data-type="indexterm" data-primary="Apache Mahout" id="idm45831167200984"></a> which makes math code written in Scala easy to read. In <a data-type="xref" href="#decompose_scala_mahout">EXAMPLE 9-2</a> we load our data into a Spark RDD, wrap that RDD in a Mahout distributed row matrix (DRM), and perform the DS-SVD on the matrix, which yields three matrices that we will then save.</p>
<div id="decompose_scala_mahout" data-type="example" title2="Decomposing a CT scan with Spark and Mahout" no2="9-2">
<h5>Example 9-2. Decomposing a CT scan with Spark and Mahout</h5>

<pre data-type="programlisting" data-code-language="scala"><code class="k">val</code><code> </code><code class="n">pathToMatrix</code><code> </code><code class="k">=</code><code> </code><code class="s">"gs://covid-dicoms/s.csv"</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO2-1" href="#callout_case_study_using_multiple_tools_CO2-1"><img src="assets/1.png" alt="1" width="12" height="12"></a><code>
</code><code>
</code><code class="k">val</code><code> </code><code class="n">voxelRDD</code><code class="k">:</code><code class="kt">DrmRdd</code><code class="o">[</code><code class="kt">Int</code><code class="o">]</code><code>  </code><code class="k">=</code><code> </code><code class="n">sc</code><code class="o">.</code><code class="n">textFile</code><code class="o">(</code><code class="n">pathToMatrix</code><code class="o">)</code><code>
</code><code>  </code><code class="o">.</code><code class="n">map</code><code class="o">(</code><code class="n">s</code><code> </code><code class="k">=&gt;</code><code> </code><code class="n">dvec</code><code class="o">(</code><code> </code><code class="n">s</code><code class="o">.</code><code class="n">split</code><code class="o">(</code><code class="s">","</code><code class="o">)</code><code>
</code><code>  </code><code class="o">.</code><code class="n">map</code><code class="o">(</code><code class="n">f</code><code> </code><code class="k">=&gt;</code><code> </code><code class="n">f</code><code class="o">.</code><code class="n">toDouble</code><code class="o">)</code><code class="o">)</code><code class="o">)</code><code>
</code><code>  </code><code class="o">.</code><code class="n">zipWithIndex</code><code>
</code><code>  </code><code class="o">.</code><code class="n">map</code><code class="o">(</code><code class="n">o</code><code> </code><code class="k">=&gt;</code><code> </code><code class="o">(</code><code class="n">o</code><code class="o">.</code><code class="n">_2</code><code class="o">.</code><code class="n">toInt</code><code class="o">,</code><code> </code><code class="n">o</code><code class="o">.</code><code class="n">_1</code><code class="o">)</code><code class="o">)</code><code>
</code><code>
</code><code class="k">val</code><code> </code><code class="n">voxelDRM</code><code> </code><code class="k">=</code><code> </code><code class="n">drmWrap</code><code class="o">(</code><code class="n">voxelRDD</code><code class="o">)</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO2-2" href="#callout_case_study_using_multiple_tools_CO2-2"><img src="assets/2.png" alt="2" width="12" height="12"></a><code>
</code><code>
</code><code class="c1">// k, p, q should all be cli parameters
</code><code class="c1">// k is rank of the output, e.g., the number of eigenfaces we want out.
</code><code class="c1">// p is oversampling parameter,
</code><code class="c1">// and q is the number of additional power iterations
</code><code class="c1">// Read https://mahout.apache.org/users/dim-reduction/ssvd.html
</code><code class="k">val</code><code> </code><code class="n">k</code><code> </code><code class="k">=</code><code> </code><code class="n">args</code><code class="o">(</code><code class="mi">0</code><code class="o">)</code><code class="o">.</code><code class="n">toInt</code><code>
</code><code class="k">val</code><code> </code><code class="n">p</code><code> </code><code class="k">=</code><code> </code><code class="n">args</code><code class="o">(</code><code class="mi">1</code><code class="o">)</code><code class="o">.</code><code class="n">toInt</code><code>
</code><code class="k">val</code><code> </code><code class="n">q</code><code> </code><code class="k">=</code><code> </code><code class="n">args</code><code class="o">(</code><code class="mi">2</code><code class="o">)</code><code class="o">.</code><code class="n">toInt</code><code>
</code><code>
</code><code class="k">val</code><code class="o">(</code><code class="n">drmU</code><code class="o">,</code><code> </code><code class="n">drmV</code><code class="o">,</code><code> </code><code class="n">s</code><code class="o">)</code><code> </code><code class="k">=</code><code> </code><code class="n">dssvd</code><code class="o">(</code><code class="n">voxelDRM</code><code class="o">.</code><code class="n">t</code><code class="o">,</code><code> </code><code class="n">k</code><code class="o">,</code><code> </code><code class="n">p</code><code class="o">,</code><code> </code><code class="n">q</code><code class="o">)</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO2-3" href="#callout_case_study_using_multiple_tools_CO2-3"><img src="assets/3.png" alt="3" width="12" height="12"></a><code>
</code><code>
</code><code class="k">val</code><code> </code><code class="n">V</code><code> </code><code class="k">=</code><code> </code><code class="n">drmV</code><code class="o">.</code><code class="n">checkpoint</code><code class="o">(</code><code class="o">)</code><code class="o">.</code><code class="n">rdd</code><code class="o">.</code><code class="n">saveAsTextFile</code><code class="o">(</code><code class="s">"gs://covid-dicoms/drmV"</code><code class="o">)</code><code>
</code><code class="k">val</code><code> </code><code class="n">U</code><code> </code><code class="k">=</code><code> </code><code class="n">drmU</code><code class="o">.</code><code class="n">t</code><code class="o">.</code><code class="n">checkpoint</code><code class="o">(</code><code class="o">)</code><code class="o">.</code><code class="n">rdd</code><code class="o">.</code><code class="n">saveAsTextFile</code><code class="o">(</code><code class="s">"gs://covid-dicoms/drmU"</code><code class="o">)</code><code>
</code><code>
</code><code class="n">sc</code><code class="o">.</code><code class="n">parallelize</code><code class="o">(</code><code class="n">s</code><code class="o">.</code><code class="n">toArray</code><code class="o">,</code><code class="mi">1</code><code class="o">)</code><code class="o">.</code><code class="n">saveAsTextFile</code><code class="o">(</code><code class="s">"gs://covid-dicoms/s"</code><code class="o">)</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO2-4" href="#callout_case_study_using_multiple_tools_CO2-4"><img src="assets/4.png" alt="4" width="12" height="12"></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO2-1" href="#co_case_study_using_multiple_tools_CO2-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>Load the data.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO2-2" href="#co_case_study_using_multiple_tools_CO2-2"><img src="assets/2.png" alt="2" width="12" height="12"></a></dt>
<dd><p>Wrap the RDD in a DRM.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO2-3" href="#co_case_study_using_multiple_tools_CO2-3"><img src="assets/3.png" alt="3" width="12" height="12"></a></dt>
<dd><p>Perform the DS-SVD.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO2-4" href="#co_case_study_using_multiple_tools_CO2-4"><img src="assets/4.png" alt="4" width="12" height="12"></a></dt>
<dd><p>Save the output.</p></dd>
</dl></div>

<p>And so in just a few lines of Scala we are able to execute an out-of-core singular value decomposition.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Visualization"><div class="sect2" id="idm45831166298424" title2="Visualization" no2="9.1.3">
<h2>9.1.3. Visualization</h2>

<p>There are lots of good libraries for visualization in R and Python, and we want to use one of these for <a data-type="indexterm" data-primary="denoising data" data-secondary="visualizing denoised DICOMs" id="ch09-viz"></a><a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="visualizing denoised DICOMs" id="ch09-viz2"></a>visualizing our denoised DICOMs.  We also want to save our final images to somewhere more persistent than a persistent volume container (PVC), so that we can come back later to view our images.</p>

<p>This phase of the pipeline will have three steps:</p>
<ol>
<li>
<p>Download the DRMs that resulted from the DS-SVD.</p>
</li>
<li>
<p>Recombine the matrices into a DICOM, denoised by setting some of the diagonal values of the matrix <em>s</em> to zero.</p>
</li>
<li>
<p>Render a slice of the resulting DICOM visually.</p>
</li>

</ol>
<div data-type="note"><h6>Note</h6>
<p>Visualization could be easily accomplished in R or Python. We will proceed in Python, but using the <code>oro.dicom</code> package in R.  We have chosen Python because Google officially supports a Python API for interacting with Cloud Storage.</p>
</div>










<section data-type="sect3" data-pdf-bookmark="Downloading DRMs"><div class="sect3" id="idm45831166262136" title2="Downloading DRMs" no2="9.1.3.1">
<h3>9.1.3.1. Downloading DRMs</h3>

<p>Recall the DRM is really just a wrapper around an RDD. In the cloud storage bucket, it will be represented as a directory
full of “parts” of the matrix.  To download these files we use the helper function shown in <a data-type="xref" href="#download_dir_helper">EXAMPLE 9-3</a>.</p>
<div id="download_dir_helper" data-type="example" title2="Helper function to download a directory from GCS" no2="9-3">
<h5>Example 9-3. Helper function to download a directory from GCS</h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">download_folder</code><code class="p">(</code><code class="n">bucket_name</code> <code class="o">=</code> <code class="s1">'your-bucket-name'</code><code class="p">,</code>
                    <code class="n">bucket_dir</code> <code class="o">=</code> <code class="s1">'your-bucket-directory/'</code><code class="p">,</code>
                    <code class="n">dl_dir</code><code class="o">=</code> <code class="s2">"local-dir/"</code><code class="p">):</code>
    <code class="n">storage_client</code> <code class="o">=</code> <code class="n">storage</code><code class="o">.</code><code class="n">Client</code><code class="p">()</code>
    <code class="n">bucket</code> <code class="o">=</code> <code class="n">storage_client</code><code class="o">.</code><code class="n">get_bucket</code><code class="p">(</code><code class="n">bucket_name</code><code class="p">)</code>
    <code class="n">blobs</code> <code class="o">=</code> <code class="n">bucket</code><code class="o">.</code><code class="n">list_blobs</code><code class="p">(</code><code class="n">prefix</code><code class="o">=</code><code class="n">bucket_dir</code><code class="p">)</code>  <code class="c1"># Get list of files</code>
    <code class="k">for</code> <code class="n">blob</code> <code class="ow">in</code> <code class="n">blobs</code><code class="p">:</code>
        <code class="n">filename</code> <code class="o">=</code> <code class="n">blob</code><code class="o">.</code><code class="n">name</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="s1">'/'</code><code class="p">,</code> <code class="s1">'_'</code><code class="p">)</code>
        <code class="n">blob</code><code class="o">.</code><code class="n">download_to_filename</code><code class="p">(</code><code class="n">dl_dir</code> <code class="o">+</code> <code class="n">filename</code><code class="p">)</code>  <code class="c1"># Download</code></pre></div>

<p>At the time of writing, Mahout’s integration with Python is sparse (there is no PySpark equivalent to this code).</p>

<p>Also, there are no helper functions for reading Mahout DRMs into Python NumPy arrays, so we must write another helper function to assist us with that (shown in <a data-type="xref" href="#mahout_helper">EXAMPLE 9-4</a>).</p>
<div id="mahout_helper" data-type="example" title2="Helper function to read Mahout DRMs into NumPy matrices" no2="9-4">
<h5>Example 9-4. Helper function to read Mahout DRMs into NumPy matrices</h5>

<pre data-type="programlisting" data-code-language="python"><code class="k">def</code><code> </code><code class="nf">read_mahout_drm</code><code class="p">(</code><code class="n">path</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">data</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code class="p">}</code><code>
</code><code>    </code><code class="n">counter</code><code> </code><code class="o">=</code><code> </code><code class="mi">0</code><code>
</code><code>    </code><code class="n">parts</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="n">p</code><code> </code><code class="k">for</code><code> </code><code class="n">p</code><code> </code><code class="ow">in</code><code> </code><code class="n">os</code><code class="o">.</code><code class="n">listdir</code><code class="p">(</code><code class="n">path</code><code class="p">)</code><code> </code><code class="k">if</code><code> </code><code class="s2">"</code><code class="s2">part</code><code class="s2">"</code><code class="p">]</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO3-1" href="#callout_case_study_using_multiple_tools_CO3-1"><img src="assets/1.png" alt="1" width="12" height="12"></a><code>
</code><code>    </code><code class="k">for</code><code> </code><code class="n">p</code><code> </code><code class="ow">in</code><code> </code><code class="n">parts</code><code class="p">:</code><code>
</code><code>        </code><code class="k">with</code><code> </code><code class="nb">open</code><code class="p">(</code><code class="n">f</code><code class="s2">"</code><code class="s2">{path}/{p}</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">r</code><code class="s1">'</code><code class="p">)</code><code> </code><code class="k">as</code><code> </code><code class="n">f</code><code class="p">:</code><code>
</code><code>            </code><code class="n">lines</code><code> </code><code class="o">=</code><code> </code><code class="n">f</code><code class="o">.</code><code class="n">read</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>            </code><code class="k">for</code><code> </code><code class="n">l</code><code> </code><code class="ow">in</code><code> </code><code class="n">lines</code><code class="p">[</code><code class="p">:</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code><code class="p">:</code><code>
</code><code>                </code><code class="n">counter</code><code> </code><code class="o">+</code><code class="o">=</code><code class="mi">1</code><code>
</code><code>                </code><code class="n">t</code><code> </code><code class="o">=</code><code> </code><code class="n">literal_eval</code><code class="p">(</code><code class="n">l</code><code class="p">)</code><code>
</code><code>                </code><code class="n">arr</code><code> </code><code class="o">=</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="p">[</code><code class="n">t</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code> </code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">t</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">keys</code><code class="p">(</code><code class="p">)</code><code class="p">)</code><code class="p">)</code><code class="p">]</code><code class="p">)</code><code>
</code><code>                </code><code class="n">data</code><code class="p">[</code><code class="n">t</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="p">]</code><code> </code><code class="o">=</code><code> </code><code class="n">arr</code><code>
</code><code>    </code><code class="k">print</code><code class="p">(</code><code class="n">f</code><code class="s2">"</code><code class="s2">read {counter} lines from {path}</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="k">return</code><code> </code><code class="n">data</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO3-1" href="#co_case_study_using_multiple_tools_CO3-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>Remember, most Mahout DRMs will be in “parts” of files, so we must iterate through the parts to reconstruct the matrix.</p></dd>
</dl></div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Recomposing the matrix into denoised images"><div class="sect3" id="idm45831166143656" title2="Recomposing the matrix into denoised images" no2="9.1.3.2">
<h3>9.1.3.2. Recomposing the matrix into denoised images</h3>

<p>In a singular value decomposition, the diagonal matrix of singular values are typically denoted with a sigma. In our code,
however, we use the letter <code>s</code>. By convention, these values are typically ordered from most important to least important,
and happily, this convention is followed in the Mahout implementation. To denoise the images, we simply set the last few values of the diagonals to zero. The idea is that the least important basis vectors probably represent noise which we seek to get rid of (see <a data-type="xref" href="#image_writer">EXAMPLE 9-5</a>).</p>
<div id="image_writer" data-type="example" title2="A loop to write several images" no2="9-5">
<h5>Example 9-5. A loop to write several images</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">percs</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="mf">0.001</code><code class="p">,</code><code> </code><code class="mf">0.01</code><code class="p">,</code><code> </code><code class="mf">0.05</code><code class="p">,</code><code> </code><code class="mf">0.1</code><code class="p">,</code><code> </code><code class="mf">0.3</code><code class="p">]</code><code>
</code><code>
</code><code class="k">for</code><code> </code><code class="n">p</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">percs</code><code class="p">)</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">perc</code><code> </code><code class="o">=</code><code> </code><code class="n">percs</code><code class="p">[</code><code class="n">p</code><code class="p">]</code><code>
</code><code>    </code><code class="n">diags</code><code> </code><code class="o">=</code><code> </code><code class="p">[</code><code class="n">diags_orig</code><code class="p">[</code><code class="n">i</code><code class="p">]</code><code>
</code><code>             </code><code class="k">if</code><code> </code><code class="n">i</code><code> </code><code class="o">&lt;</code><code> </code><code class="nb">round</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">diags</code><code class="p">)</code><code> </code><code class="o">-</code><code> </code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">diags</code><code class="p">)</code><code> </code><code class="o">*</code><code> </code><code class="n">perc</code><code class="p">)</code><code class="p">)</code><code>
</code><code>             </code><code class="k">else</code><code> </code><code class="mi">0</code><code>
</code><code>             </code><code class="k">for</code><code> </code><code class="n">i</code><code> </code><code class="ow">in</code><code> </code><code class="nb">range</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">diags</code><code class="p">)</code><code class="p">)</code><code class="p">]</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO4-1" href="#callout_case_study_using_multiple_tools_CO4-1"><img src="assets/1.png" alt="1" width="12" height="12"></a><code>
</code><code>    </code><code class="n">recon</code><code> </code><code class="o">=</code><code> </code><code class="n">drmU_p5</code><code> </code><code class="err">@</code><code> </code><code class="n">np</code><code class="o">.</code><code class="n">diag</code><code class="p">(</code><code class="n">diags</code><code class="p">)</code><code> </code><code class="err">@</code><code> </code><code class="n">drmV_p5</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO4-2" href="#callout_case_study_using_multiple_tools_CO4-2"><img src="assets/2.png" alt="2" width="12" height="12"></a><code>
</code><code>    </code><code class="n">composite_img</code><code> </code><code class="o">=</code><code> </code><code class="n">recon</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="p">(</code><code class="mi">512</code><code class="p">,</code><code class="mi">512</code><code class="p">,</code><code class="mi">301</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO4-3" href="#callout_case_study_using_multiple_tools_CO4-3"><img src="assets/3.png" alt="3" width="12" height="12"></a><code>
</code><code>    </code><code class="n">a1</code><code> </code><code class="o">=</code><code> </code><code class="n">plt</code><code class="o">.</code><code class="n">subplot</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">,</code><code class="mi">1</code><code class="p">)</code><code>
</code><code>    </code><code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">composite_img</code><code class="p">[</code><code class="p">:</code><code class="p">,</code><code> </code><code class="p">:</code><code class="p">,</code><code> </code><code class="mi">150</code><code class="p">]</code><code class="p">,</code><code> </code><code class="n">cmap</code><code class="o">=</code><code class="n">plt</code><code class="o">.</code><code class="n">cm</code><code class="o">.</code><code class="n">bone</code><code class="p">)</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO4-4" href="#callout_case_study_using_multiple_tools_CO4-4"><img src="assets/4.png" alt="4" width="12" height="12"></a><code>
</code><code>    </code><code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="n">f</code><code class="s2">"</code><code class="s2">{perc*100}</code><code class="si">% d</code><code class="s2">enoised.  (k={len(diags)}, oversample=15, power_iters=2)</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">a1</code><code class="o">.</code><code class="n">set_aspect</code><code class="p">(</code><code class="mf">1.0</code><code class="p">)</code><code>
</code><code>    </code><code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s1">'</code><code class="s1">off</code><code class="s1">'</code><code class="p">)</code><code>
</code><code>    </code><code class="n">fname</code><code> </code><code class="o">=</code><code> </code><code class="n">f</code><code class="s2">"</code><code class="s2">{100-(perc*100)}</code><code class="si">%-d</code><code class="s2">enoised-img.png</code><code class="s2">"</code><code>
</code><code>    </code><code class="n">plt</code><code class="o">.</code><code class="n">savefig</code><code class="p">(</code><code class="n">f</code><code class="s2">"</code><code class="s2">/tmp/{fname}</code><code class="s2">"</code><code class="p">)</code><code>
</code><code>    </code><code class="n">upload_blob</code><code class="p">(</code><code class="n">bucket_name</code><code class="p">,</code><code> </code><code class="n">f</code><code class="s2">"</code><code class="s2">/tmp/{fname}</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="n">f</code><code class="s2">"</code><code class="s2">/output/{fname}</code><code class="s2">"</code><code class="p">)</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO4-5" href="#callout_case_study_using_multiple_tools_CO4-5"><img src="assets/5.png" alt="5" width="12" height="12"></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO4-1" href="#co_case_study_using_multiple_tools_CO4-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>Set the last <code>p</code>% of the singular values to equal zero.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO4-2" href="#co_case_study_using_multiple_tools_CO4-2"><img src="assets/2.png" alt="2" width="12" height="12"></a></dt>
<dd><p><code>@</code> is the “matrix multiplication” operator.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO4-3" href="#co_case_study_using_multiple_tools_CO4-3"><img src="assets/3.png" alt="3" width="12" height="12"></a></dt>
<dd><p>We’re presuming our original image was 512 x 512 x 301 slices, which may or may not be correct for your case.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO4-4" href="#co_case_study_using_multiple_tools_CO4-4"><img src="assets/4.png" alt="4" width="12" height="12"></a></dt>
<dd><p>Take the 150th slice.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO4-5" href="#co_case_study_using_multiple_tools_CO4-5"><img src="assets/5.png" alt="5" width="12" height="12"></a></dt>
<dd><p>We’ll talk about this function in the next section.</p></dd>
</dl></div>

<p>Now in our bucket, we will have several images in the <code>/output/</code> folder, named for what percentage of denoising they have been through.</p>

<p>Our output was an image of one slice of the DICOM. Instead, we could have output several full DICOM files (one for each level of denoising) that could then be viewed in a DICOM viewer, though the full example is a bit involved and out of scope for this text.
We encourage you to read <a href="https://oreil.ly/_1-sT"><code>pydicom</code>’s documentation</a> if you are interested in this output.<a data-type="indexterm" data-startref="ch09-viz" id="idm45831165632440"></a><a data-type="indexterm" data-startref="ch09-viz2" id="idm45831165631736"></a></p>
</div></section>



</div></section>













<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="The CT Scan Denoising Pipeline"><div class="sect2" id="idm45831165818536" title2="The CT Scan Denoising Pipeline" no2="9.1.4">
<h2>9.1.4. The CT Scan Denoising Pipeline</h2>

<p>To create our pipeline, we will first create a manifest for our Spark job,<a data-type="indexterm" data-primary="denoising data" data-secondary="denoising pipeline" id="ch09-dnz2"></a><a data-type="indexterm" data-primary="data cleaning" data-secondary="denoising pipeline" id="ch09-dns2"></a><a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="denoising pipeline" id="ch09-dnz3"></a><a data-type="indexterm" data-primary="Apache Spark" data-secondary="data denoising pipeline" id="ch09-dnz"></a><a data-type="indexterm" data-primary="denoising data" data-secondary="Apache Spark" id="ch09-dnz4"></a><a data-type="indexterm" data-primary="data cleaning" data-secondary="Apache Spark" id="ch09-dns4"></a><a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="Apache Spark" id="ch09-dnz5"></a> which will specify what image to use, what secrets
to use to mount what buckets, and a wide array of other information. Then we will create a pipeline using our containers from earlier steps and the manifest we define, which will output a PNG of one slice of the DICOM image with varying levels of noise removed.</p>










<section data-type="sect3" data-pdf-bookmark="Spark operation manifest"><div class="sect3" id="idm45831165807720" title2="Spark operation manifest" no2="9.1.4.1">
<h3>9.1.4.1. Spark operation manifest</h3>

<p>Spark read/wrote the files from GCS because it has issues with ReadWriteOnce (RWO) PVCs. We’ll need to download output from GCS, then upload.</p>

<p>The Apache Spark operator does not like to read from ReadWriteOnce PVCs. If your
Kubernetes is using these operators, and you can’t request ReadWriteMany (as, for example, is the case on GCP), then you
will need to use some other storage for the original matrix which is to be decomposed.</p>

<p>Most of our containers to this point have used <code>ContainerOp</code>.  As a Spark job may actually consist of several containers,
we will use a more generic <code>ResourceOp</code>. Defining <code>ResourceOp</code>s gives us much more power and control, but this comes at
the cost of the pretty Python API. To define a <code>ResourceOp</code> we must define a manifest (see <a data-type="xref" href="#spark_manifest">EXAMPLE 9-6</a>) and pass that to the <code>ResourceOp</code>
creation (see the next section).</p>
<div id="spark_manifest" data-type="example" title2="Spark operation manifest" no2="9-6">
<h5>Example 9-6. Spark operation manifest</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">container_manifest</code><code> </code><code class="o">=</code><code> </code><code class="p">{</code><code>
</code><code>    </code><code class="s2">"</code><code class="s2">apiVersion</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">sparkoperator.k8s.io/v1beta2</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>    </code><code class="s2">"</code><code class="s2">kind</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">SparkApplication</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>    </code><code class="s2">"</code><code class="s2">metadata</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="p">{</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">name</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">spark-app</code><code class="s2">"</code><code class="p">,</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO5-1" href="#callout_case_study_using_multiple_tools_CO5-1"><img src="assets/1.png" alt="1" width="12" height="12"></a><code>
</code><code>        </code><code class="s2">"</code><code class="s2">namespace</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">kubeflow</code><code class="s2">"</code><code>
</code><code>    </code><code class="p">}</code><code class="p">,</code><code>
</code><code>    </code><code class="s2">"</code><code class="s2">spec</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="p">{</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">type</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">Scala</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">mode</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">cluster</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">image</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">docker.io/rawkintrevo/covid-basis-vectors:0.2.0</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">imagePullPolicy</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">Always</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">hadoopConf</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="p">{</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO5-2" href="#callout_case_study_using_multiple_tools_CO5-2"><img src="assets/2.png" alt="2" width="12" height="12"></a><code>
</code><code>            </code><code class="s2">"</code><code class="s2">fs.gs.project.id</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">kubeflow-hacky-hacky</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">fs.gs.system.bucket</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">covid-dicoms</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">fs.gs.impl</code><code class="s2">"</code><code> </code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">google.cloud.auth.service.account.enable</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">true</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">google.cloud.auth.service.account.json.keyfile</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">/mnt/secrets/user-gcp-sa.json</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="p">}</code><code class="p">,</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">mainClass</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">org.rawkintrevo.covid.App</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">mainApplicationFile</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">local:///covid-0.1-jar-with-dependencies.jar</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="c1"># See the Dockerfile</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">arguments</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="p">[</code><code class="s2">"</code><code class="s2">245</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">15</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">1</code><code class="s2">"</code><code class="p">]</code><code class="p">,</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">sparkVersion</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">2.4.5</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">restartPolicy</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="p">{</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">type</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">Never</code><code class="s2">"</code><code>
</code><code>        </code><code class="p">}</code><code class="p">,</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">driver</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="p">{</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">cores</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="mi">1</code><code class="p">,</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">secrets</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="p">[</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO5-3" href="#callout_case_study_using_multiple_tools_CO5-2"><img src="assets/2.png" alt="2" width="12" height="12"></a><code>
</code><code>                </code><code class="p">{</code><code class="s2">"</code><code class="s2">name</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">user-gcp-sa</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>                 </code><code class="s2">"</code><code class="s2">path</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">/mnt/secrets</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>                 </code><code class="s2">"</code><code class="s2">secretType</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">GCPServiceAccount</code><code class="s2">"</code><code>
</code><code>                 </code><code class="p">}</code><code>
</code><code>            </code><code class="p">]</code><code class="p">,</code><code>
</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">coreLimit</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">1200m</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">memory</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">512m</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">labels</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="p">{</code><code>
</code><code>                </code><code class="s2">"</code><code class="s2">version</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">2.4.5</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>            </code><code class="p">}</code><code class="p">,</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">serviceAccount</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">spark-operatoroperator-sa</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="c1"># also try spark-operatoroperator-sa</code><code>
</code><code>        </code><code class="p">}</code><code class="p">,</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">executor</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="p">{</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">cores</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="mi">1</code><code class="p">,</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">secrets</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="p">[</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO5-4" href="#callout_case_study_using_multiple_tools_CO5-2"><img src="assets/2.png" alt="2" width="12" height="12"></a><code>
</code><code>                </code><code class="p">{</code><code class="s2">"</code><code class="s2">name</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">user-gcp-sa</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>                 </code><code class="s2">"</code><code class="s2">path</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">/mnt/secrets</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>                 </code><code class="s2">"</code><code class="s2">secretType</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">GCPServiceAccount</code><code class="s2">"</code><code>
</code><code>                 </code><code class="p">}</code><code>
</code><code>            </code><code class="p">]</code><code class="p">,</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">instances</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="mi">4</code><code class="p">,</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO5-5" href="#callout_case_study_using_multiple_tools_CO5-3"><img src="assets/3.png" alt="3" width="12" height="12"></a><code>
</code><code>            </code><code class="s2">"</code><code class="s2">memory</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">4084m</code><code class="s2">"</code><code>
</code><code>        </code><code class="p">}</code><code class="p">,</code><code>
</code><code>        </code><code class="s2">"</code><code class="s2">labels</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="p">{</code><code>
</code><code>            </code><code class="s2">"</code><code class="s2">version</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="s2">"</code><code class="s2">2.4.5</code><code class="s2">"</code><code>
</code><code>        </code><code class="p">}</code><code class="p">,</code><code>
</code><code>
</code><code>    </code><code class="p">}</code><code>
</code><code class="p">}</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO5-1" href="#co_case_study_using_multiple_tools_CO5-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>Name of the app: you can check on progress in the console with <code>kubectl logs spark-app-driver</code>.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO5-2" href="#co_case_study_using_multiple_tools_CO5-2"><img src="assets/2.png" alt="2" width="12" height="12"></a></dt>
<dd><p>Different cloud providers use slightly different configurations here.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO5-3" href="#co_case_study_using_multiple_tools_CO5-5"><img src="assets/3.png" alt="3" width="12" height="12"></a></dt>
<dd><p>We’re doing a decomposition on a very large matrix—you may want to give even more resources than this if you can spare them.</p></dd>
</dl></div>
<div data-type="note"><h6>Note</h6>
<p>Because we are accessing GCP, we need to base our image from <code>gcr.io/spark-operator/spark:v2.4.5-gcs-prometheus</code>,
which has additional included JARs for accessing GCP (otherwise we would use <code>gcr.io/spark-operator/spark:v2.4.5</code>).</p>
</div>

<p>While this is tuned for GCP, with a very minimal change in configuration, specifically around the secrets, this could easily be ported to AWS or Azure.</p>

<p>If you are familiar with Kubernetes, you are probably used to seeing manifests represented as YAML files. Here we have
created a manifest with a Python dictionary. Next we will use this dictionary in our pipeline definition to create a <code>ResourceOp</code>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="The pipeline"><div class="sect3" id="idm45831165249480" title2="The pipeline" no2="9.1.4.2">
<h3>9.1.4.2. The pipeline</h3>

<p>Finally, we have all of our necessary components. We will create a pipeline that strings them together into a repeatable operation for us.</p>

<p>To review, <a data-type="xref" href="#ct_scan_denoise">EXAMPLE 9-7</a> does the following:</p>

<ul>
<li>
<p>Downloads CT scans from GCP to a local PVC.</p>
</li>
<li>
<p>Converts the CT scans (DICOM files) into a matrix (<em>s.csv</em>).</p>
</li>
<li>
<p>A Spark job does a distributed stochastic singular value decomposition and writes the output to GCP.</p>
</li>
<li>
<p>The decomposed matrix is recomposed with some of the singular values set to zero—thus denoising the image.</p>
</li>
</ul>
<div id="ct_scan_denoise" data-type="example" title2="CT scan denoising pipeline" no2="9-7">
<h5>Example 9-7. CT scan denoising pipeline</h5>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code><code> </code><code class="nn">kfp.gcp</code><code> </code><code class="kn">import</code><code> </code><code class="n">use_gcp_secret</code><code>
</code><code class="nd">@kfp.dsl.pipeline</code><code class="p">(</code><code>
</code><code>    </code><code class="n">name</code><code class="o">=</code><code class="s2">"</code><code class="s2">Covid DICOM Pipe v2</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>    </code><code class="n">description</code><code class="o">=</code><code class="s2">"</code><code class="s2">Visualize Denoised CT Scans</code><code class="s2">"</code><code>
</code><code class="p">)</code><code>
</code><code class="k">def</code><code> </code><code class="nf">covid_dicom_pipeline</code><code class="p">(</code><code class="p">)</code><code class="p">:</code><code>
</code><code>    </code><code class="n">vop</code><code> </code><code class="o">=</code><code> </code><code class="n">kfp</code><code class="o">.</code><code class="n">dsl</code><code class="o">.</code><code class="n">VolumeOp</code><code class="p">(</code><code>
</code><code>        </code><code class="n">name</code><code class="o">=</code><code class="s2">"</code><code class="s2">requisition-PVC</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="n">resource_name</code><code class="o">=</code><code class="s2">"</code><code class="s2">datapvc</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="n">size</code><code class="o">=</code><code class="s2">"</code><code class="s2">20Gi</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="c1">#10 Gi blows up...</code><code>
</code><code>        </code><code class="n">modes</code><code class="o">=</code><code class="n">kfp</code><code class="o">.</code><code class="n">dsl</code><code class="o">.</code><code class="n">VOLUME_MODE_RWO</code><code>
</code><code>    </code><code class="p">)</code><code>
</code><code>    </code><code class="n">step1</code><code> </code><code class="o">=</code><code> </code><code class="n">kfp</code><code class="o">.</code><code class="n">dsl</code><code class="o">.</code><code class="n">ContainerOp</code><code class="p">(</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO6-1" href="#callout_case_study_using_multiple_tools_CO6-1"><img src="assets/1.png" alt="1" width="12" height="12"></a><code>
</code><code>        </code><code class="n">name</code><code class="o">=</code><code class="s2">"</code><code class="s2">download-dicom</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="n">image</code><code class="o">=</code><code class="s2">"</code><code class="s2">rawkintrevo/download-dicom:0.0.0.4</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="n">command</code><code class="o">=</code><code class="p">[</code><code class="s2">"</code><code class="s2">/run.sh</code><code class="s2">"</code><code class="p">]</code><code class="p">,</code><code>
</code><code>        </code><code class="n">pvolumes</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">/data</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="n">vop</code><code class="o">.</code><code class="n">volume</code><code class="p">}</code><code>
</code><code>    </code><code class="p">)</code><code>
</code><code>    </code><code class="n">step2</code><code> </code><code class="o">=</code><code> </code><code class="n">kfp</code><code class="o">.</code><code class="n">dsl</code><code class="o">.</code><code class="n">ContainerOp</code><code class="p">(</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO6-2" href="#callout_case_study_using_multiple_tools_CO6-2"><img src="assets/2.png" alt="2" width="12" height="12"></a><code>
</code><code>        </code><code class="n">name</code><code class="o">=</code><code class="s2">"</code><code class="s2">convert-dicoms-to-vectors</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="n">image</code><code class="o">=</code><code class="s2">"</code><code class="s2">rawkintrevo/covid-prep-dicom:0.9.5</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="n">arguments</code><code class="o">=</code><code class="p">[</code><code>
</code><code>            </code><code class="s1">'</code><code class="s1">--bucket_name</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">covid-dicoms</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="p">]</code><code class="p">,</code><code>
</code><code>        </code><code class="n">command</code><code class="o">=</code><code class="p">[</code><code class="s2">"</code><code class="s2">python</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">/program.py</code><code class="s2">"</code><code class="p">]</code><code class="p">,</code><code>
</code><code>        </code><code class="n">pvolumes</code><code class="o">=</code><code class="p">{</code><code class="s2">"</code><code class="s2">/mnt/data</code><code class="s2">"</code><code class="p">:</code><code> </code><code class="n">step1</code><code class="o">.</code><code class="n">pvolume</code><code class="p">}</code><code>
</code><code>    </code><code class="p">)</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">kfp</code><code class="o">.</code><code class="n">gcp</code><code class="o">.</code><code class="n">use_gcp_secret</code><code class="p">(</code><code class="n">secret_name</code><code class="o">=</code><code class="s1">'</code><code class="s1">user-gcp-sa</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO6-3" href="#callout_case_study_using_multiple_tools_CO6-5"><img src="assets/5.png" alt="5" width="12" height="12"></a><code>
</code><code>    </code><code class="n">rop</code><code> </code><code class="o">=</code><code> </code><code class="n">kfp</code><code class="o">.</code><code class="n">dsl</code><code class="o">.</code><code class="n">ResourceOp</code><code class="p">(</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO6-4" href="#callout_case_study_using_multiple_tools_CO6-3"><img src="assets/3.png" alt="3" width="12" height="12"></a><code>
</code><code>        </code><code class="n">name</code><code class="o">=</code><code class="s2">"</code><code class="s2">calculate-basis-vectors</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="n">k8s_resource</code><code class="o">=</code><code class="n">container_manifest</code><code class="p">,</code><code>
</code><code>        </code><code class="n">action</code><code class="o">=</code><code class="s2">"</code><code class="s2">create</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="n">success_condition</code><code class="o">=</code><code class="s2">"</code><code class="s2">status.applicationState.state == COMPLETED</code><code class="s2">"</code><code>
</code><code>    </code><code class="p">)</code><code class="o">.</code><code class="n">after</code><code class="p">(</code><code class="n">step2</code><code class="p">)</code><code>
</code><code>    </code><code class="n">pyviz</code><code> </code><code class="o">=</code><code> </code><code class="n">kfp</code><code class="o">.</code><code class="n">dsl</code><code class="o">.</code><code class="n">ContainerOp</code><code class="p">(</code><code> </code><a class="co" id="co_case_study_using_multiple_tools_CO6-5" href="#callout_case_study_using_multiple_tools_CO6-4"><img src="assets/4.png" alt="4" width="12" height="12"></a><code>
</code><code>        </code><code class="n">name</code><code class="o">=</code><code class="s2">"</code><code class="s2">visualize-slice-of-dicom</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="n">image</code><code class="o">=</code><code class="s2">"</code><code class="s2">rawkintrevo/visualize-dicom-output:0.0.11</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="n">command</code><code class="o">=</code><code class="p">[</code><code class="s2">"</code><code class="s2">python</code><code class="s2">"</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">/program.py</code><code class="s2">"</code><code class="p">]</code><code class="p">,</code><code>
</code><code>        </code><code class="n">arguments</code><code class="o">=</code><code class="p">[</code><code>
</code><code>            </code><code class="s1">'</code><code class="s1">--bucket_name</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s2">"</code><code class="s2">covid-dicoms</code><code class="s2">"</code><code class="p">,</code><code>
</code><code>        </code><code class="p">]</code><code class="p">,</code><code>
</code><code>    </code><code class="p">)</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code><code class="n">kfp</code><code class="o">.</code><code class="n">gcp</code><code class="o">.</code><code class="n">use_gcp_secret</code><code class="p">(</code><code class="n">secret_name</code><code class="o">=</code><code class="s1">'</code><code class="s1">user-gcp-sa</code><code class="s1">'</code><code class="p">)</code><code class="p">)</code><code class="o">.</code><code class="n">after</code><code class="p">(</code><code class="n">rop</code><code class="p">)</code><code>
</code><code>
</code><code>
</code><code class="n">kfp</code><code class="o">.</code><code class="n">compiler</code><code class="o">.</code><code class="n">Compiler</code><code class="p">(</code><code class="p">)</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="n">covid_dicom_pipeline</code><code class="p">,</code><code class="s2">"</code><code class="s2">dicom-pipeline-2.zip</code><code class="s2">"</code><code class="p">)</code><code>
</code><code class="n">client</code><code> </code><code class="o">=</code><code> </code><code class="n">kfp</code><code class="o">.</code><code class="n">Client</code><code class="p">(</code><code class="p">)</code><code>
</code><code>
</code><code class="n">my_experiment</code><code> </code><code class="o">=</code><code> </code><code class="n">client</code><code class="o">.</code><code class="n">create_experiment</code><code class="p">(</code><code class="n">name</code><code class="o">=</code><code class="s1">'</code><code class="s1">my-experiments</code><code class="s1">'</code><code class="p">)</code><code>
</code><code class="n">my_run</code><code> </code><code class="o">=</code><code> </code><code class="n">client</code><code class="o">.</code><code class="n">run_pipeline</code><code class="p">(</code><code class="n">my_experiment</code><code class="o">.</code><code class="n">id</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">my-run1</code><code class="s1">'</code><code class="p">,</code><code> </code><code class="s1">'</code><code class="s1">dicom-pipeline-2.zip</code><code class="s1">'</code><code class="p">)</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO6-1" href="#co_case_study_using_multiple_tools_CO6-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>This container was not discussed, but it simply downloads images from a GCP bucket to our local PVC.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO6-2" href="#co_case_study_using_multiple_tools_CO6-2"><img src="assets/2.png" alt="2" width="12" height="12"></a></dt>
<dd><p>Here we convert our DICOM into a matrix and upload it to a specified GCP bucket.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO6-3" href="#co_case_study_using_multiple_tools_CO6-4"><img src="assets/3.png" alt="3" width="12" height="12"></a></dt>
<dd><p>This is the Spark job that calculates the singular value decomposition.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO6-4" href="#co_case_study_using_multiple_tools_CO6-5"><img src="assets/4.png" alt="4" width="12" height="12"></a></dt>
<dd><p>This is where DICOM images are reconstructed.</p></dd>
<dt><a class="co" id="callout_case_study_using_multiple_tools_CO6-5" href="#co_case_study_using_multiple_tools_CO6-3"><img src="assets/5.png" alt="5" width="12" height="12"></a></dt>
<dd><p>For GCP we <code>use_gcp_secret</code>, but similar functions exist for Azure and AWS.</p></dd>
</dl></div>

<p>For illustration, Figures <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ct_original">FIGURE 9-1</a> through <a data-type="xref" data-xrefstyle="select:labelnumber" href="#ct_1030">FIGURE 9-3</a> are slices of the DICOM image at various levels of denoising. As we are not radiology experts, we won’t try to make any points about changes in quality or what is optimal, other than to point out that at 10% denoising we’ve probably gone too far, and at 30% we unquestionably have.</p>

<figure><div id="ct_original" class="figure" data-type="figure" title2="Original slice of DICOM" no2="9-1">
<img src="assets/kfml_0901.png" alt="Original DICOM Slice" width="547" height="450">
<h6>Figure 9-1. Original slice of DICOM</h6>
</div></figure>

<figure><div id="ct_15" class="figure" data-type="figure" title2="1% denoised DICOM slice (left); 5% denoised DICOM slice (right)" no2="9-2">
<img src="assets/kfml_0902.png" alt="1% and 5% Denoised" width="1223" height="516">
<h6>Figure 9-2. 1% denoised DICOM slice (left); 5% denoised DICOM slice (right)</h6>
</div></figure>

<figure><div id="ct_1030" class="figure" data-type="figure" title2="10% denoised DICOM slice (left); .5% denoised DICOM slice (right)" no2="9-3">
<img src="assets/kfml_0903.png" alt="10% and 30% Denoised" width="1223" height="518">
<h6>Figure 9-3. 10% denoised DICOM slice (left); .5% denoised DICOM slice (right)</h6>
</div></figure>

<p>Again we see that while this pipeline is now hardcoded for GCP, it can with only a few lines of updates be changed to work with AWS or Azure; specifically, how we mount secrets to the container. A significant advantage of this is that we are able to safely decouple passcodes from code.</p>
<aside data-type="sidebar"><div class="sidebar" id="Use_rstats">
<h5>Using RStats</h5>
<p>Our examples have all been Python- or Scala-based, but remember—a container is just an OS that is going to run a program. As such, you can use any language that can exist in a container. To use an RStats script as a pipeline step:</p>
<ol>
<li>
<p>Create a Docker container (probably from a preexisting images such as <code>r-base:latest</code>).</p>
</li>
<li>
<p>Create a program that takes command-line arguments.</p>
</li>
<li>
<p>Output the results to a mounted PVC or save to a cloud storage provider.<a data-type="indexterm" data-startref="ch09-dnz" id="idm45831165407368"></a><a data-type="indexterm" data-startref="ch09-dnz2" id="idm45831165406664"></a><a data-type="indexterm" data-startref="ch09-dnz3" id="idm45831165405992"></a><a data-type="indexterm" data-startref="ch09-dnz4" id="idm45831165405320"></a><a data-type="indexterm" data-startref="ch09-dnz5" id="idm45831165191880"></a><a data-type="indexterm" data-startref="ch09-dns4" id="idm45831165191208"></a><a data-type="indexterm" data-startref="ch09-dns2" id="idm45831165190536"></a></p>
</li>

</ol>
</div></aside>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Sharing the Pipeline"><div class="sect1" id="idm45831165248760" title2="Sharing the Pipeline" no2="9.2">
<h1>9.2. Sharing the Pipeline</h1>

<p>A final important benefit of Kubeflow is the reproducibility of experiments.<a data-type="indexterm" data-primary="CT scan data denoised" data-secondary="sharing the pipeline" id="idm45831165188456"></a><a data-type="indexterm" data-primary="denoising data" data-secondary="sharing the pipeline" id="idm45831165162264"></a><a data-type="indexterm" data-primary="experiments" data-secondary="reproducibility by sharing pipeline" id="idm45831165161320"></a><a data-type="indexterm" data-primary="Kubeflow Pipelines" data-secondary="experiments" data-tertiary="reproducibility by sharing pipeline" id="idm45831165160360"></a>  While often underscored in academia, reproducibiltiy
is an important concept in business settings as well. By containerizing pipeline steps, we can remove hidden dependencies
that allow a program to only run on one device—or, to put it another way, reproducibility prevents you from developing an algorithm that only runs
on one person’s machine.</p>

<p>The pipeline we present here should run on any Kubeflow deployment.<sup><a data-type="noteref" id="idm45831165586696-marker" href="#idm45831165586696">[3]</a></sup>
This also allows for rapid iteration. Any reader can use this pipeline as a basis and, for instance, could create a final step where some <a data-type="indexterm" data-primary="deep learning" data-secondary="sharing a pipeline" id="idm45831165585848"></a><a data-type="indexterm" data-primary="training" data-secondary="deep learning" data-tertiary="sharing a pipeline" id="idm45831165584904"></a>deep learning is performed on the denoised images and the original images to compare the effects of denoising.<a data-type="indexterm" data-startref="ch09-ct" id="idm45831165583432"></a><a data-type="indexterm" data-startref="ch09-ct2" id="idm45831165399896"></a><a data-type="indexterm" data-startref="ch09-ct8" id="idm45831165399224"></a><a data-type="indexterm" data-startref="ch09-dd5" id="idm45831165398552"></a></p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45831165397624" title2="Conclusion" no2="9.3">
<h1>9.3. Conclusion</h1>

<p>We have now seen how to create very maintainable pipelines by leveraging containers that have most, if not all, of the required dependencies to make our program run. This not only removes the technical debt of having to maintain a system with all of
these dependencies, but makes the program much more transferable, and our research much more easily transferable and reproducible.</p>

<p>There exists a large and exciting galaxy of Docker containers, and odds are you already have some steps Dockerized in preexisting
containers. Being able to leverage these containers for Kubeflow Pipeline steps is certainly one of Kubeflow’s biggest
strengths.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45831167801576"><sup><a href="#idm45831167801576-marker">[1]</a></sup> The full paper can be found  <a href="">here</a>.</p><p data-type="footnote" id="idm45831167786776"><sup><a href="#idm45831167786776-marker">[2]</a></sup> The <a href="">Radiological Society of North America</a> hopes to publish a repository of COVID-19 CT scans soon.</p><p data-type="footnote" id="idm45831165586696"><sup><a href="#idm45831165586696-marker">[3]</a></sup> With minor tuning for no GCE deployments.</p></div></div></section>
<section data-type="chapter" data-pdf-bookmark="Chapter 10. Hyperparameter Tuning and Automated 
Machine Learning"><div class="chapter" id="hyperparameter_tuning" data-type="chapter" title2="Hyperparameter Tuning and Automated 
Machine Learning" no2="10">
<h1>Chapter 10. Hyperparameter Tuning and Automated 
Machine Learning</h1>


<p>In the previous chapters, we have seen how Kubeflow helps with the various phases of machine learning.<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="no single model works best" id="idm45831164938744"></a> But knowing what to do in each phase—whether it’s feature preparation or training or deploying models—requires some amount of expert knowledge and experimentation. According to the <a href="https://oreil.ly/H_IHi">“no free lunch” theorem</a>, no single model works best for every machine learning problem, therefore each model must be constructed carefully. It can be very time-consuming and expensive to fully build a highly performing model if each phase requires significant human input.</p>

<p>Naturally, one might wonder: is it possible to automate parts—or even the entirety—of the machine learning process? Can we reduce the amount of overhead for data scientists while still sustaining high model quality?</p>

<p>In machine learning, the umbrella term for solving these type of problems<a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="AutoML" data-tertiary="about" id="idm45831165229432"></a><a data-type="indexterm" data-primary="AutoML (automated machine learning)" data-secondary="about" id="idm45831165228184"></a> is <em>automated machine learning</em> (AutoML). It is a constantly evolving field of research, and has found its way to the industry with practical applications. AutoML seeks to simplify machine learning for experts and nonexperts alike by reducing the need for manual interaction in the more time-consuming and iterative phases of machine learning: feature engineering, model construction, and hyperparameter configuration.</p>

<p>In this chapter we will see how Kubeflow can be used to automate hyperparameter search and neural architecture search, two important subfields of AutoML.</p>






<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="AutoML: An Overview"><div class="sect1" id="idm45831165224824" title2="AutoML: An Overview" no2="10.1">
<h1>10.1. AutoML: An Overview</h1>

<p>AutoML refers to the various processes and tools that automate parts of the machine learning process. At a high level, AutoML refers to any algorithms and methodologies that seek to solve one or more of the following problems:</p>
<dl>
<dt>Data preprocessing</dt>
<dd>
<p>Machine learning requires data, and raw data can come from various<a data-type="indexterm" data-primary="data preparation" data-secondary="AutoML for" id="idm45831165185720"></a> sources and in different formats. To make raw data useful, human experts typically have to comb over the data, normalize values, remove erroneous or corrupted data, and ensure data consistency.</p>
</dd>
<dt>Feature engineering</dt>
<dd>
<p>Training models with too few input variables (or “features”) can lead<a data-type="indexterm" data-primary="feature preparation" data-secondary="AutoML for" id="idm45831165183128"></a><a data-type="indexterm" data-primary="data preparation" data-secondary="feature preparation" data-tertiary="AutoML for" id="idm45831164735912"></a> to inaccurate models. However, having too many features can also be problematic; the learning process would be slower and more resource-consuming, and overfitting problems can occur. Coming up with the right set of features can be the most time-consuming part of building a machine learning model. Automated feature engineering can speed up the process of feature extraction, selection, and 
transformation.</p>
</dd>
<dt>Model selection</dt>
<dd>
<p>Once you have all the training data, you need to pick the right training model<a data-type="indexterm" data-primary="training" data-secondary="model selection" data-tertiary="AutoML for" id="idm45831164732312"></a> for your dataset. The ideal model should be as simple as possible while still providing a good measure of prediction accuracy.</p>
</dd>
<dt>Hyperparameter tuning</dt>
<dd>
<p>Most learning models have a number of parameters that are external<a data-type="indexterm" data-primary="hyperparameters" data-secondary="AutoML for tuning" id="idm45831164671064"></a> to the model, such as the learning rate, the batch size, and the number of layers in the neural network. We call these <em>hyperparameters</em> to distinguish them from model parameters that are adjusted by the learning process. Hyperparameter tuning is the process of automating the search process for these parameters in order to improve the accuracy of the model.</p>
</dd>
<dt>Neural architecture search</dt>
<dd>
<p>A related field to hyperparameter tuning is <em>neural architecture search</em> (NAS).<a data-type="indexterm" data-primary="neural architecture search (NAS)" data-secondary="about" id="idm45831164907512"></a><a data-type="indexterm" data-primary="neural architecture search (NAS)" data-secondary="AutoML" id="idm45831164906472"></a><a data-type="indexterm" data-primary="hyperparameters" data-secondary="neural architecture search" id="idm45831164905512"></a> Instead of choosing between a fixed range of values for each hyperparameter value, NAS seeks to take automation one step further and generates an entire neural network that outperforms handcrafted architectures. Common methodologies for NAS include reinforcement learning and evolutionary algorithms.</p>
</dd>
</dl>

<p>The focus of this chapter will be on the latter two problems—hyperparameter tuning and neural architecture search. As they are related, they can be solved using similar methodologies.</p>
</div></section>













<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Hyperparameter Tuning with Kubeflow Katib"><div class="sect1" id="idm45831164903512" title2="Hyperparameter Tuning with Kubeflow Katib" no2="10.2">
<h1>10.2. Hyperparameter Tuning with Kubeflow Katib</h1>

<p>In <a data-type="xref" href="#tf_ch">CHAPTER 7</a>, it was mentioned that we needed to set a few hyperparameters.<a data-type="indexterm" data-primary="hyperparameters" data-secondary="Kubeflow Katib for tuning" data-seealso="Kubeflow Katib" id="idm45831164846760"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="about" id="idm45831164845528"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="about" id="idm45831164844584"></a><a data-type="indexterm" data-primary="hyperparameters" data-secondary="definition" id="idm45831164843640"></a><a data-type="indexterm" data-primary="AutoML (automated machine learning)" data-secondary="Kubeflow Katib" id="idm45831164803464"></a><a data-type="indexterm" data-primary="machine learning (ML)" data-secondary="AutoML" data-tertiary="Kubeflow Katib" id="idm45831164802504"></a> In machine learning, hyperparameters refer to parameters that are set before the training process begins (as opposed to model parameters which are learned from the training process). Examples of hyperparameters include the learning rate, number of decision trees, number of layers in a neural network, etc.</p>

<p>The concept of hyperparameter optimization is very simple: select the set of hyperparameter values that lead to optimal model performance. A hyperparameter tuning framework is a tool that does exactly that. Typically, the user of such a tool would define a few things:</p>

<ul>
<li>
<p>The list of hyperparameters and their valid range of values (called the <em>search space</em>)</p>
</li>
<li>
<p>The metrics used to measure model performance</p>
</li>
<li>
<p>The methodology to use for the searching process</p>
</li>
</ul>

<p>Kubeflow comes packaged with <a href="https://oreil.ly/BW4TM">Katib</a>, a general framework for hyperparameter tuning. Among similar open source tools, Katib has a few distinguishing features:</p>
<dl>
<dt>It is Kubernetes native</dt>
<dd>
<p>This means that Katib experiments can be ported wherever Kubernetes runs.</p>
</dd>
<dt>It has multiframework support</dt>
<dd>
<p>Katib supports many popular learning frameworks, with first-class support for TensorFlow and PyTorch distributed training.</p>
</dd>
<dt>It is language-agnostic</dt>
<dd>
<p>Training code can be written in any language, as long as it is built as a Docker image.</p>
</dd>
</dl>
<div data-type="note"><h6>Note</h6>
<p>The name <em>katib</em> means “secretary” or “scribe” in Arabic, and is<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="about katib meaning" id="idm45831164953928"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="about katib meaning" id="idm45831164952952"></a><a data-type="indexterm" data-primary="Google Vizier" id="idm45831164952008"></a> an homage to the Vizier framework that inspired its initial version (“vizier” being Arabic for a minister or high official).</p>
</div>

<p>In this chapter, we’ll take a look at how Katib simplifies hyperparameter optimization.</p>
</div></section>













<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Katib Concepts"><div class="sect1" id="idm45831164950312" title2="Katib Concepts" no2="10.3">
<h1>10.3. Katib Concepts</h1>

<p>Let’s begin by defining a few terms that are central to the workflow of Katib (as illustrated in <a data-type="xref" href="#katib-workflow">FIGURE 10-1</a>):</p>
<dl>
<dt>Experiment</dt>
<dd>
<p>An experiment is an end-to-end process that takes a problem<a data-type="indexterm" data-primary="experiments" data-secondary="Kubeflow Katib" id="idm45831164656744"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="experiments" id="idm45831164655768"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="experiments" id="idm45831164654824"></a> (e.g., tuning a training model for handwriting recognition), an objective metric (maximize the prediction accuracy), and a search space (range for hyperparameters), and produces a final set of optimal hyperparameter values.</p>
</dd>
<dt>Suggestion</dt>
<dd>
<p>A suggestion is one possible solution to the problem we are trying to solve.<a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="suggestions" id="idm45831164652232"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="suggestions" id="idm45831164649448"></a><a data-type="indexterm" data-primary="suggestions (Kubeflow Katib)" id="idm45831164648504"></a> Since we are trying to find the combination of hyperparameter values that lead to optimal model performance, a suggestion would be one set of hyperparameter values from the specified search space.</p>
</dd>
<dt>Trial</dt>
<dd>
<p>A trial is one iteration of the experiment. Each trial takes a suggestion<a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="trials" id="idm45831164646248"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="trials" id="idm45831164645272"></a><a data-type="indexterm" data-primary="trials (Kubeflow Katib)" id="idm45831164644328"></a> and executes a worker process (packaged through Docker) that produces evaluation metrics. Katib’s controller then computes the next suggestion based on previous metrics and spawns new trials.</p>
</dd>
</dl>

<figure><div id="katib-workflow" class="figure" data-type="figure" title2="Katib system workflow" no2="10-1">
<img src="assets/kfml_1001.png" alt="kfml 1001" width="1334" height="707">
<h6>Figure 10-1. Katib <a href="https://oreil.ly/BW4TM">system workflow</a></h6>
</div></figure>
<div data-type="note"><h6>Note</h6>
<p>In Katib, experiments, suggestions, and trials are all custom resources.<a data-type="indexterm" data-primary="custom resources on Kubernetes" data-secondary="Kubeflow Katib" id="idm45831165200424"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="custom resources" data-tertiary="Kubeflow Katib" id="idm45831165199480"></a><a data-type="indexterm" data-primary="Kubernetes custom resources" data-secondary="APIs" id="idm45831165198264"></a> This means they are stored in Kubernetes and can be manipulated using standard Kubernetes APIs.</p>
</div>

<p>Another important aspect of hyperparameter tuning is how to find the next set of parameters. As of the time of this writing, Katib supports the following search 
algorithms:</p>
<dl>
<dt>Grid search</dt>
<dd>
<p>Also known as a parameter sweep, grid search is the simplest approach—exhaustively<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="search algorithms" data-tertiary="grid search" id="idm45831165193976"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="search algorithms" data-tertiary="grid search" id="idm45831165192728"></a><a data-type="indexterm" data-primary="grid search in Katib" id="idm45831164524840"></a> search through possible parameter values in the specified search space. Although resource-intensive, grid search has the advantage of having high parallelism since the tasks are completely independent.</p>
</dd>
<dt>Random search</dt>
<dd>
<p>Similar to grid search, the tasks in random search are completely independent.<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="search algorithms" data-tertiary="random search" id="idm45831164522536"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="search algorithms" data-tertiary="random search" id="idm45831164521288"></a><a data-type="indexterm" data-primary="Bayesian optimization" data-secondary="in Katib" id="idm45831164520072"></a><a data-type="indexterm" data-primary="random search in Katib" id="idm45831164519128"></a> Instead of enumerating every possible value, random search attempts to generate parameter values through random selection. When there are many hyperparameters to tune (but only a few have significant impact on model performance), random search can vastly outperform grid search. Random search can also be useful when the number of discrete parameters is high, which makes grid search 
infeasible.</p>
</dd>
<dt>Bayesian optimization</dt>
<dd>
<p>This is a powerful approach that uses probability and statistics to seek better parameters.<a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="search algorithms" data-tertiary="Bayesian optimization" id="idm45831164515976"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="search algorithms" data-tertiary="Bayesian optimization" id="idm45831164514728"></a> Bayesian optimization builds a probabilistic model for the objective function, finds parameter values that perform well on the model, and then iteratively updates the model based on metrics collected during trial runs. Intuitively speaking, Bayesian optimization seeks to improve upon a model by making informed guesses. This optimization method relies on previous iterations to find new parameters, and can be parallelized. While trials are not as independent as grid or random search, Bayesian optimization can find results with fewer trials overall.</p>
</dd>
<dt>Hyperband</dt>
<dd>
<p>This is a relatively new approach that selects configuration values randomly.<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="search algorithms" data-tertiary="hyperbrand" id="idm45831164511528"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="search algorithms" data-tertiary="hyperbrand" id="idm45831164510280"></a> But unlike traditional random search, hyperband only evaluates each trial for a small number of iterations. Then it takes the best-performing configurations and runs them longer, repeating this process until a desired result is reached. Due to its similarity to random search, tasks can be highly parallelized.</p>
</dd>
<dt>Other experimental algorithms</dt>
<dd>
<p>These include the tree of Parzen estimators (TPE) and covariance matrix adaptation evolution strategy (CMA-ES), both implemented by using the <a href="https://oreil.ly/PDGOg">Goptuna</a> optimization framework.</p>
</dd>
</dl>

<p>One final piece of the puzzle in Katib is the metrics collector.<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="metrics collector" id="idm45831164506136"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="metrics collector" id="idm45831164505160"></a> This is the process that collects and parses evaluation metrics after each trial and pushes them into the 
persistent database. Katib implements metrics collection through a sidecar container, which runs alongside the main container in a pod.</p>

<p>Overall, Katib’s design makes it highly scalable, portable, and extensible.<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="about" id="idm45831164502744"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="about" id="idm45831164501768"></a> Since it is part of the Kubeflow platform, Katib natively supports integration with many of Kubeflow’s other training components, like the TFJob and PyTorch operators. Katib is also the first hyperparameter tuning framework that supports multitenancy, making it ideal for a cloud hosted environment.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Installing Katib"><div class="sect1" id="idm45831164949912" title2="Installing Katib" no2="10.4">
<h1>10.4. Installing Katib</h1>

<p>Katib is installed by default. To install Katib as a standalone service,<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="installing" id="idm45831164499000"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="installing" id="idm45831164498024"></a><a data-type="indexterm" data-primary="getting started" data-secondary="installing Kubeflow Katib" id="idm45831164497080"></a> you can use the following script in the Kubeflow GitHub repo:</p>

<pre data-type="programlisting" data-code-language="shell" id="untitled_programlisting_28" title2="(no caption)" no2="">git clone https://github.com/kubeflow/katib
bash ./katib/scripts/v1beta1/deploy.sh</pre>

<p>If your Kubernetes cluster doesn’t support dynamic volume provisioning, you would also create a persistent volume:</p>

<pre data-type="programlisting" data-code-language="shell" id="untitled_programlisting_29" title2="(no caption)" no2=""><code class="nv">pv_path</code><code class="o">=</code>https://raw.githubusercontent.com/kubeflow/katib/master/manifests<code class="se">\</code>
/v1beta1/pv/pv.yaml
kubectl apply -f pv_path</pre>

<p>After installing Katib components, you can navigate to the Katib dashboard to verify that it is running. If you installed Katib through Kubeflow and have an endpoint, simply navigate to the Kubeflow dashboard and select “Katib” in the menu. Otherwise, you can set up port forwarding to test your deployment:</p>

<pre data-type="programlisting" data-code-language="shell" id="untitled_programlisting_30" title2="(no caption)" no2="">kubectl port-forward svc/katib-ui -n kubeflow 8080:80</pre>

<p>Then navigate to:</p>

<pre data-type="programlisting" data-code-language="shell" id="untitled_programlisting_31" title2="(no caption)" no2="">http://localhost:8080/katib/</pre>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Running Your First Katib Experiment"><div class="sect1" id="idm45831164484120" title2="Running Your First Katib Experiment" no2="10.5">
<h1>10.5. Running Your First Katib Experiment</h1>

<p>Now that Katib is up and running in your cluster, let’s take a look at how<a data-type="indexterm" data-primary="getting started" data-secondary="installing Kubeflow Katib" data-tertiary="first experiment" id="ch10-fstexx"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="experiments" data-tertiary="first experiment" id="ch10-fstex"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="experiments" data-tertiary="first experiment" id="ch10-fstex2"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="first experiment" data-tertiary="about" id="idm45831164487080"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="first experiment" data-tertiary="about" id="idm45831164464456"></a><a data-type="indexterm" data-primary="MNIST (Modified National Institute of Standards and Technology)" data-secondary="Kubeflow Katib first experiment" data-tertiary="about" id="idm45831164463240"></a> to run an actual experiment. In this section we will use Katib to tune a simple MNist model. You can find the source code and all configuration files on <a href="https://oreil.ly/tdSM_">Katib’s GitHub page</a>.</p>








<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Prepping Your Training Code"><div class="sect2" id="idm45831164461000" title2="Prepping Your Training Code" no2="10.5.1">
<h2>10.5.1. Prepping Your Training Code</h2>

<p>The first step is to prepare your training code. Since Katib runs training<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="first experiment" data-tertiary="prepping training code" id="idm45831164459192"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="first experiment" data-tertiary="prepping training code" id="idm45831164457944"></a><a data-type="indexterm" data-primary="MNIST (Modified National Institute of Standards and Technology)" data-secondary="Kubeflow Katib first experiment" data-tertiary="prepping training code" id="idm45831164440088"></a> jobs for trial evaluation, each training job needs to be packaged as a Docker container. Katib is language-agnostic, so it does not matter how you write the training code. However, to be compatible with Katib, the training code must satisfy a couple of requirements:</p>

<ul>
<li>
<p>Hyperparameters must be exposed as command-line arguments. For example:</p>
</li>
</ul>

<pre data-type="programlisting" id="untitled_programlisting_32" title2="(no caption)" no2="">python mnist.py --batch_size=100 --learning_rate=0.1</pre>

<ul>
<li>
<p>Metrics must be exposed in a format consistent with the metrics collector. Katib currently supports metrics collection through standard output, file, TensorFlow events, or custom. The simplest option is to use the standard metrics collector, which means the evaluation metrics must be written to stdout, in the following format:</p>
</li>
</ul>

<pre data-type="programlisting" data-code-language="shell" id="untitled_programlisting_33" title2="(no caption)" no2=""><code class="nv">metrics_name</code><code class="o">=</code>metrics_value</pre>

<p>The example training model code that we will use can be found <a href="https://oreil.ly/USb-Y">on this GitHub site</a>.</p>

<p>After preparing the training code, simply package it as a Docker image and it is ready to go.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Configuring an Experiment"><div class="sect2" id="idm45831164379624" title2="Configuring an Experiment" no2="10.5.2">
<h2>10.5.2. Configuring an Experiment</h2>

<p>Once you have the training container, the next step is to write a spec for your experiment.<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="first experiment" data-tertiary="configuring experiment" id="idm45831164377928"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="first experiment" data-tertiary="configuring experiment" id="idm45831164376680"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="experiments" data-tertiary="configuring" id="idm45831164375464"></a><a data-type="indexterm" data-primary="experiments" data-secondary="Kubeflow Katib" data-tertiary="configuring" id="idm45831164374248"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="experiments" data-tertiary="configuring" id="idm45831164373032"></a><a data-type="indexterm" data-primary="Kubernetes" data-secondary="custom resources" data-tertiary="Kubeflow Katib" id="idm45831164371272"></a><a data-type="indexterm" data-primary="custom resources on Kubernetes" data-secondary="Kubeflow Katib" id="idm45831164370056"></a><a data-type="indexterm" data-primary="MNIST (Modified National Institute of Standards and Technology)" data-secondary="Kubeflow Katib first experiment" data-tertiary="configuring experiment" id="idm45831164369144"></a> Katib uses Kubernetes custom resources to represent experiments. <a data-type="xref" href="#Examp_experimen_spec">EXAMPLE 10-1</a> can be downloaded from <a href="https://oreil.ly/nwbbJ">this GitHub page</a>.</p>
<div id="Examp_experimen_spec" data-type="example" title2="Example experiment spec" no2="10-1">
<h5>Example 10-1. Example experiment spec</h5>

<pre data-type="programlisting">apiVersion: "kubeflow.org/v1beta1"
kind: Experiment
metadata:
  namespace: kubeflow
  labels:
    controller-tools.k8s.io: "1.0"
  name: random-example
spec:
  objective:               <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-1" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"></a>
    type: maximize
    goal: 0.99
    objectiveMetricName: Validation-accuracy
    additionalMetricNames:
      - Train-accuracy
  algorithm:               <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-2" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-2"><img src="assets/2.png" alt="2" width="12" height="12"></a>
    algorithmName: random
  parallelTrialCount: 3    <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-3" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-3"><img src="assets/3.png" alt="3" width="12" height="12"></a>
  maxTrialCount: 12
  maxFailedTrialCount: 3
  parameters:              <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-4" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-4"><img src="assets/4.png" alt="4" width="12" height="12"></a>
    - name: --lr
      parameterType: double
      feasibleSpace:
        min: "0.01"
        max: "0.03"
    - name: --num-layers
      parameterType: int
      feasibleSpace:
        min: "2"
        max: "5"
    - name: --optimizer
      parameterType: categorical
      feasibleSpace:
        list:
        - sgd
        - adam
        - ftrl
  trialTemplate:           <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-5" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-5"><img src="assets/5.png" alt="5" width="12" height="12"></a>
    goTemplate:
        rawTemplate: |-
          apiVersion: batch/v1
          kind: Job
          metadata:
            name: {{.Trial}}
            namespace: {{.NameSpace}}
          spec:
            template:
              spec:
                containers:
                - name: {{.Trial}}
                  image: docker.io/kubeflowkatib/mxnet-mnist
                  command:
                  - "python3"
                  - "/opt/mxnet-mnist/mnist.py"
                  - "--batch-size=64"
                  {{- with .HyperParameters}}
                  {{- range .}}
                  - "{{.Name}}={{.Value}}"
                  {{- end}}
                  {{- end}}
                restartPolicy: Never</pre>

<p>That’s quite a lot to follow. Let’s take a closer look at each part of the <code>spec</code> section:</p>
<dl class="calloutlist">
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-1" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p><em>Objective.</em> This is where you configure how to measure the performance of your training model, and the goal of the experiment. In this experiment, we are trying to maximize the validation-accuracy metric. We are stopping our experiment if we reach the objective goal of 0.99 (99% accuracy). The <code>additionalMetricsNames</code> represents metrics that are collected from each trial, but aren’t used to evaluate the trial.</p></dd>
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-2" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-2"><img src="assets/2.png" alt="2" width="12" height="12"></a></dt>
<dd><p><em>Algorithm.</em> In this experiment we are using random search; some algorithms may require additional configurations.</p></dd>
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-3" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-3"><img src="assets/3.png" alt="3" width="12" height="12"></a></dt>
<dd><p><em>Budget configurations.</em> This is where we configure our experiment budget. In this experiment, we would run 3 trials in parallel, with a total of 12 trials. We would also stop our experiment if we have three failed trials. This last part is also called an <em>error budget</em>—an important concept in maintaining production-grade system uptime.</p></dd>
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-4" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-4"><img src="assets/4.png" alt="4" width="12" height="12"></a></dt>
<dd><p><em>Parameters.</em> Here we define which parameters we want to tune and the search space for each. For example, the <code>learning rate</code> parameter is exposed in the training code as <code>--lr</code>. It is a double, with a contiguous search space between 0.01 and 0.03.</p></dd>
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-5" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO1-5"><img src="assets/5.png" alt="5" width="12" height="12"></a></dt>
<dd><p><em>Trial template.</em> The last part of the experiment spec is the template from which each trial is configured. For the purpose of this example, the only important parts are:</p></dd>
</dl>

<pre data-type="programlisting">    image: docker.io/kubeflowkatib/mxnet-mnist
    command:
      - "python3"
      - "/opt/mxnet-mnist/mnist.py"
      - "--batch-size=64"</pre>

<p>This should point to the Docker image that you built in the previous step, with the command-line entry point to run the code.</p></div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Running the Experiment"><div class="sect2" id="idm45831164317832" title2="Running the Experiment" no2="10.5.3">
<h2>10.5.3. Running the Experiment</h2>

<p>After everything is configured, apply the resource to start the experiment:<a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="first experiment" data-tertiary="running" id="ch10-fexr2"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="first experiment" data-tertiary="running" id="ch10-fexr"></a><a data-type="indexterm" data-primary="MNIST (Modified National Institute of Standards and Technology)" data-secondary="Kubeflow Katib first experiment" data-tertiary="running experiment" id="ch10-fexr3"></a></p>

<pre data-type="programlisting" data-code-language="bash" id="untitled_programlisting_34" title2="(no caption)" no2="">kubectl apply -f random-example.yaml</pre>

<p>You can check the status of the experiment by running the following:</p>

<pre data-type="programlisting" data-code-language="bash" id="untitled_programlisting_35" title2="(no caption)" no2="">kubectl -n kubeflow describe experiment random-example</pre>

<p>In the output, you should see something like <a data-type="xref" href="#Ex_exper_output">EXAMPLE 10-2</a>.</p>
<div id="Ex_exper_output" data-type="example" title2="Example experiment output" no2="10-2">
<h5>Example 10-2. Example experiment output</h5>

<pre data-type="programlisting">Name:         random-example
Namespace:    kubeflow
Labels:       controller-tools.k8s.io=1.0
Annotations:  &lt;none&gt;
API Version:  kubeflow.org/v1beta1
Kind:         Experiment
Metadata:
  Creation Timestamp:  2019-12-22T22:53:25Z
  Finalizers:
    update-prometheus-metrics
  Generation:        2
  Resource Version:  720692
  Self Link:         /apis/kubeflow.org/v1beta1/namespaces/kubeflow/experiments/random-example
  UID:               dc6bc15a-250d-11ea-8cae-42010a80010f
Spec:
  Algorithm:
    Algorithm Name:        random
    Algorithm Settings:    &lt;nil&gt;
  Max Failed Trial Count:  3
  Max Trial Count:         12
  Metrics Collector Spec:
    Collector:
      Kind:  StdOut
  Objective:
    Additional Metric Names:
      accuracy
    Goal:                   0.99
    Objective Metric Name:  Validation-accuracy
    Type:                   maximize
  Parallel Trial Count:     3
  Parameters:
    Feasible Space:
      Max:           0.03
      Min:           0.01
    Name:            --lr
    Parameter Type:  double
    Feasible Space:
      Max:           5
      Min:           2
    Name:            --num-layers
    Parameter Type:  int
    Feasible Space:
      List:
        sgd
        adam
        ftrl
    Name:            --optimizer
    Parameter Type:  categorical
  Trial Template:
    Go Template:
      Raw Template:  apiVersion: batch/v1
kind: Job
metadata:
  name: {{.Trial}}
  namespace: {{.NameSpace}}
spec:
  template:
    spec:
      containers:
      - name: {{.Trial}}
        image: docker.io/kubeflowkatib/mxnet-mnist-example
        command:
        - "python"
        - "/mxnet/example/image-classification/train_mnist.py"
        - "--batch-size=64"
        {{- with .HyperParameters}}
        {{- range .}}
        - "{{.Name}}={{.Value}}"
        {{- end}}
        {{- end}}
      restartPolicy: Never
Status:                                       <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO2-1" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO2-1"><img src="assets/1.png" alt="1" width="12" height="12"></a>
  Conditions:
    Last Transition Time:  2019-12-22T22:53:25Z
    Last Update Time:      2019-12-22T22:53:25Z
    Message:               Experiment is created
    Reason:                ExperimentCreated
    Status:                True
    Type:                  Created
    Last Transition Time:  2019-12-22T22:55:10Z
    Last Update Time:      2019-12-22T22:55:10Z
    Message:               Experiment is running
    Reason:                ExperimentRunning
    Status:                True
    Type:                  Running
  Current Optimal Trial:                      <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO2-2" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO2-2"><img src="assets/2.png" alt="2" width="12" height="12"></a>
    Observation:
      Metrics:
        Name:   Validation-accuracy
        Value:  0.981091
    Parameter Assignments:
      Name:          --lr
      Value:         0.025139701133432946
      Name:          --num-layers
      Value:         4
      Name:          --optimizer
      Value:         sgd
  Start Time:        2019-12-22T22:53:25Z
  Trials:            12                       <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO2-3" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO2-3"><img src="assets/3.png" alt="3" width="12" height="12"></a>
  Trials Running:    2
  Trials Succeeded:  10
Events:              &lt;none&gt;</pre>

<p>Some of the interesting parts of the output are:</p>
<dl class="calloutlist">
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO2-1" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO2-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p><em>Status.</em> Here you can see the current state of the experiment, as well as its previous states.</p></dd>
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO2-2" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO2-2"><img src="assets/2.png" alt="2" width="12" height="12"></a></dt>
<dd><p><em>Current Optimal Trial.</em> This is the “best” trial so far, i.e., the trial that produced the best outcome as determined by our predefined metrics. You can also see this trial’s parameters and metrics.</p></dd>
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO2-3" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO2-3"><img src="assets/3.png" alt="3" width="12" height="12"></a></dt>
<dd><p><em>Trials Succeeded/Running/Failed.</em> In this section, you can see how your experiment is progressing.<a data-type="indexterm" data-startref="ch10-fstex" id="idm45831164291592"></a><a data-type="indexterm" data-startref="ch10-fstex2" id="idm45831164290920"></a><a data-type="indexterm" data-startref="ch10-fstexx" id="idm45831164290248"></a><a data-type="indexterm" data-startref="ch10-fexr" id="idm45831164289576"></a><a data-type="indexterm" data-startref="ch10-fexr2" id="idm45831164250968"></a><a data-type="indexterm" data-startref="ch10-fexr3" id="idm45831164250360"></a></p></dd>
</dl></div>
</div></section>













<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Katib User Interface"><div class="sect2" id="idm45831164249624" title2="Katib User Interface" no2="10.5.4">
<h2>10.5.4. Katib User Interface</h2>

<p>Alternatively, you can use Katib’s user interface (UI) to submit and monitor<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="UI" id="idm45831164247928"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="UI" id="idm45831164247080"></a><a data-type="indexterm" data-primary="user interfaces (UI)" data-secondary="Katib" id="idm45831164246232"></a> your experiments. If you have a Kubeflow deployment, you can navigate to the Katib UI by clicking “Katib” in the navigation panel and then “Hyperparameter Tuning” on the main page, shown in <a data-type="xref" href="#katib-main">FIGURE 10-2</a>.</p>

<figure><div id="katib-main" class="figure" data-type="figure" title2="Katib UI main page" no2="10-2">
<img src="assets/kfml_1002.png" alt="kfml 1002" width="1096" height="820">
<h6>Figure 10-2. Katib UI main page</h6>
</div></figure>

<p>Let’s submit our random search experiment (see <a data-type="xref" href="#katib-new-experiment">FIGURE 10-3</a>). You can simply paste a YAML in the textbox here, or have one generated for you by following the UI. To do this, click the <code>Parameters</code> tab.</p>

<figure><div id="katib-new-experiment" class="figure" data-type="figure" title2="Configuring a new experiment, part 1" no2="10-3">
<img src="assets/kfml_1003.png" alt="kfml 1003" width="1680" height="809">
<h6>Figure 10-3. Configuring a new experiment, part 1</h6>
</div></figure>

<p>You should see a panel like <a data-type="xref" href="#katib-new-experiment-2">FIGURE 10-4</a>. Enter the necessary configuration parameters on this page; define a run budget and the validation metrics.</p>

<figure><div id="katib-new-experiment-2" class="figure" data-type="figure" title2="Configuring a new experiment, part 2" no2="10-4">
<img src="assets/kfml_1004.png" alt="kfml 1004" width="1680" height="875">
<h6>Figure 10-4. Configuring a new experiment, part 2</h6>
</div></figure>

<p>Then scroll down the page and finish up the rest of the experiment by configuring the search space and the trial template. For the latter, you can just leave it on the default template. When you are done, click “Deploy.”</p>

<p class="less_space pagebreak-before">Now that the experiment is running, you can monitor its status and see a visual graph of the progress (see <a data-type="xref" href="#katib-random-example-graph">FIGURE 10-5</a>). You can see your running and completed experiments by navigating to the drop-down menu in the Katib dashboard, and then selecting “UI” and then “Monitor.”</p>

<figure><div id="katib-random-example-graph" class="figure" data-type="figure" title2="Katib UI for an experiment" no2="10-5">
<img src="assets/kfml_1005.png" alt="kfml 1005" width="940" height="698">
<h6>Figure 10-5. Katib UI for an experiment</h6>
</div></figure>

<p class="less_space pagebreak-before">Below this graph, you will see a detailed breakdown of each trial (shown in <a data-type="xref" href="#katib-random-example-table">FIGURE 10-6</a>), the values of the hyperparameters for each of the trials, and the final metric values. This is very useful for comparing the effects of certain hyperparameters on the model’s performance.</p>

<figure><div id="katib-random-example-table" class="figure" data-type="figure" title2="Katib metrics for an experiment" no2="10-6">
<img src="assets/kfml_1006.png" alt="kfml 1006" width="1679" height="1045">
<h6>Figure 10-6. Katib metrics for an experiment</h6>
</div></figure>

<p class="less_space pagebreak-before">Since we are also collecting validation metrics along the way, we can actually plot the graph for each trial. Click a row to see how the model performs with the given hyperparameter values across time (as in <a data-type="xref" href="#katib-trial">FIGURE 10-7</a>).</p>

<figure><div id="katib-trial" class="figure" data-type="figure" title2="Metrics for each trial" no2="10-7">
<img src="assets/kfml_1007.png" alt="kfml 1007" width="1501" height="971">
<h6>Figure 10-7. Metrics for each trial</h6>
</div></figure>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Tuning Distributed Training Jobs"><div class="sect1" id="idm45831164249128" title2="Tuning Distributed Training Jobs" no2="10.6">
<h1>10.6. Tuning Distributed Training Jobs</h1>

<p>In <a data-type="xref" href="#tf_ch">CHAPTER 7</a> we saw an example of using Kubeflow to orchestrate distributed training.<a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="distributed training jobs" id="idm45831164223528"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="distributed training jobs" id="idm45831164222680"></a><a data-type="indexterm" data-primary="training" data-secondary="distributed training" data-tertiary="Kubeflow Katib" id="idm45831164221832"></a> What if we want to use Katib to tune parameters for a distributed training job?</p>

<p>The good news is that Katib natively supports integration with TensorFlow and PyTorch distributed training. An MNIST example with TensorFlow can be found at this <a href="https://oreil.ly/I3q4x">Katib GitHub page</a>. This example uses the same MNIST distributed training example we saw in <a data-type="xref" href="#tf_ch">CHAPTER 7</a>, and directly integrates it into the Katib framework. In <a data-type="xref" href="#distr_train_example">EXAMPLE 10-3</a>, we will launch an experiment to tune hyperparameters (learning rate and batch size) for a distributed TensorFlow job.</p>
<div id="distr_train_example" data-type="example" title2="Distributed training example" no2="10-3">
<h5>Example 10-3. Distributed training example</h5>

<pre data-type="programlisting">apiVersion: "kubeflow.org/v1beta1"
kind: Experiment
metadata:
  namespace: kubeflow
  name: tfjob-example
spec:
  parallelTrialCount: 3             <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-1" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-1"><img src="assets/1.png" alt="1" width="12" height="12"></a>
  maxTrialCount: 12
  maxFailedTrialCount: 3
  objective:                        <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-2" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-2"><img src="assets/2.png" alt="2" width="12" height="12"></a>
    type: maximize
    goal: 0.99
    objectiveMetricName: accuracy_1
  algorithm:
    algorithmName: random
  metricsCollectorSpec:             <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-3" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-3"><img src="assets/3.png" alt="3" width="12" height="12"></a>
    source:
      fileSystemPath:
        path: /train
        kind: Directory
    collector:
      kind: TensorFlowEvent
  parameters:                       <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-4" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-4"><img src="assets/4.png" alt="4" width="12" height="12"></a>
    - name: learning_rate
      parameterType: double
      feasibleSpace:
        min: "0.01"
        max: "0.05"
    - name: batch_size
      parameterType: int
      feasibleSpace:
        min: "100"
        max: "200"
  trialTemplate:
    trialParameters:
      - name: learningRate
        description: Learning rate for the training model
        reference: learning_rate
      - name: batchSize
        description: Batch Size
        reference: batch_size
    trialSpec:
      apiVersion: "kubeflow.org/v1"
      kind: TFJob
      spec:
        tfReplicaSpecs:             <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-5" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-5"><img src="assets/5.png" alt="5" width="12" height="12"></a>
          Worker:
            replicas: 2
            restartPolicy: OnFailure
            template:
              spec:
                containers:
                  - name: tensorflow
                    image: gcr.io/kubeflow-ci/tf-mnist-with-summaries:1.0
                    imagePullPolicy: Always
                    command:
                      - "python"
                      - "/var/tf_mnist/mnist_with_summaries.py"
                      - "--log_dir=/train/metrics"
                      - "--learning_rate=${trialParameters.learningRate}"
                      - "--batch_size=${trialParameters.batchSize}"</pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-1" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>The total and parallel trial counts are similar to the previous experiment. In this case they refer to the total and parallel number of distributed training jobs to run.</p></dd>
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-2" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-2"><img src="assets/2.png" alt="2" width="12" height="12"></a></dt>
<dd><p>The objective specification is also similar—in this case we want to maximize the <code>accuracy</code> measurement.</p></dd>
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-3" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-3"><img src="assets/3.png" alt="3" width="12" height="12"></a></dt>
<dd><p>The metrics collector specification looks slightly different. This is because this is a TensorFlow job, and we can use TFEvents outputted by TensorFlow directly. Using the built-in <code>TensorFlowEvent</code> collector type, Katib can automatically parse TensorFlow events and populate the metrics database.</p></dd>
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-4" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-4"><img src="assets/4.png" alt="4" width="12" height="12"></a></dt>
<dd><p>The parameter configurations are exactly the same—in this case we are tuning the learning rate and batch size of the model.</p></dd>
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-5" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO3-5"><img src="assets/5.png" alt="5" width="12" height="12"></a></dt>
<dd><p>The trial template should look familiar to you if you read <a data-type="xref" href="#tf_ch">CHAPTER 7</a>—it’s the same distributed training example spec that we ran before. The imporant difference here is that we’ve parameterized the input to <code>learning_rate</code> and <code>batch_size</code>.</p></dd>
</dl></div>

<p>So now you have learned how to use Katib to tune hyperparameters. But notice that you still have to select the model yourself. Can we reduce the amount of human work even further? What about other subfields in AutoML? In the next section we will look at how Katib supports the generation of entire artificial neural networks.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Neural Architecture Search"><div class="sect1" id="idm45831164188376" title2="Neural Architecture Search" no2="10.7">
<h1>10.7. Neural Architecture Search</h1>

<p>Neural architecture search (NAS) is a growing subfield in automated machine learning.<a data-type="indexterm" data-primary="neural architecture search (NAS)" data-secondary="about" id="idm45831164187000"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="neural architecture search support" data-tertiary="about NAS" id="idm45831164186152"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="neural architecture search support" data-tertiary="about NAS" id="idm45831164185064"></a><a data-type="indexterm" data-primary="NAS" data-see="neural architecture search" id="idm45831164183976"></a> Unlike hyperparameter tuning, where the model is already chosen and our goal is to optimize its performance by turning a few knobs, in NAS we are trying to generate the network architecture itself. Recent research has shown that NAS can outperform handcrafted neural networks on tasks like image classification, object detection, and semantic segmentation.<sup><a data-type="noteref" id="idm45831164182632-marker" href="#idm45831164182632">[1]</a></sup></p>

<p>Most the methodologies for NAS can be categorized as either<a data-type="indexterm" data-primary="neural architecture search (NAS)" data-secondary="generation versus mutation methods" id="idm45831164180152"></a> <em>generation</em> methods or <em>mutation</em> methods. In <em>generation</em> methods, the algorithm will propose one or more candidate architectures in each iteration. These proposed architectures are then evaluated and then refined in the next iteration. In <em>mutation</em> methods, an overly complex architecture is proposed first, and subsequent iterations will attempt to prune the model.</p>

<p>Katib currently supports two implementations of NAS:<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="neural architecture search support" id="idm45831164176840"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="neural architecture search support" id="idm45831164175992"></a><a data-type="indexterm" data-primary="neural architecture search (NAS)" data-secondary="Kubeflow Katib supporting" id="idm45831164175144"></a><a data-type="indexterm" data-primary="neural architecture search (NAS)" data-secondary="Differentiable Architecture Search" id="idm45831164174296"></a><a data-type="indexterm" data-primary="Differentiable Architecture Search (DARTS)" id="idm45831164173448"></a><a data-type="indexterm" data-primary="Efficient Neural Architecture Search (ENAS)" id="idm45831164172840"></a><a data-type="indexterm" data-primary="neural architecture search (NAS)" data-secondary="Efficient Neural Architecture Search" id="idm45831164172232"></a><a data-type="indexterm" data-primary="architecture" data-secondary="Efficient Neural Architecture Search (ENAS)" id="idm45831164171384"></a><a data-type="indexterm" data-primary="architecture" data-secondary="Differentiable Architecture Search (DARTS)" id="idm45831164170536"></a><a data-type="indexterm" data-primary="architecture" data-secondary="neural architecture search support and Katb" id="idm45831164169688"></a> <em>Differentiable Architecture Search (DARTS)</em>,<sup><a data-type="noteref" id="idm45831164168328-marker" href="#idm45831164168328">[2]</a></sup> and <em>Efficient Neural Architecture Search (ENAS)</em>.<sup><a data-type="noteref" id="idm45831164166328-marker" href="#idm45831164166328">[3]</a></sup> DARTS achieves scalability of NAS by relaxing the search space to be continuous instead of discrete and utilizes gradient descent to optimize the architecture. ENAS takes a different approach, by observing that in most NAS algorithms the bottleneck occurs during the training of each child model. ENAS forces each child model to share parameters, thus improving the overall efficiency.</p>

<p>The general workflow of NAS in Katib is similar to hyperparameter search, with an additional step for constructing the model architecture. <a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="neural architecture search support" data-tertiary="model manager" id="idm45831164163896"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="neural architecture search support" data-tertiary="model manager" id="idm45831164162808"></a><a data-type="indexterm" data-primary="neural architecture search (NAS)" data-secondary="Kubeflow Katib supporting" data-tertiary="model manager" id="idm45831164161720"></a>An internal module of Katib, called the <em>model manager</em>, is responsible for taking topological configurations and mutation parameters, and constructing new models. Katib then uses the same concepts of trials and metrics to evaluate the model’s performance.</p>

<p>As an example, see the spec of a NAS experiment using DARTS in <a data-type="xref" href="#Example_NAS_experiment_spec">EXAMPLE 10-4</a>.<a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="neural architecture search support" data-tertiary="example DARTS experiment" id="ch10-mm"></a><a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="neural architecture search support" data-tertiary="example DARTS experiment" id="ch10-mm2"></a><a data-type="indexterm" data-primary="neural architecture search (NAS)" data-secondary="Kubeflow Katib supporting" data-tertiary="example DARTS experiment" id="ch10-mm3"></a></p>
<div id="Example_NAS_experiment_spec" data-type="example" title2="Example NAS experiment spec" no2="10-4">
<h5>Example 10-4. Example NAS experiment spec</h5>

<pre data-type="programlisting">apiVersion: "kubeflow.org/v1beta1"
kind: Experiment
metadata:
  namespace: kubeflow
  name: darts-example-gpu
spec:
  parallelTrialCount: 1
  maxTrialCount: 1
  maxFailedTrialCount: 1
  objective:
    type: maximize
    objectiveMetricName: Best-Genotype
  metricsCollectorSpec:
    collector:
      kind: StdOut
    source:
      filter:
        metricsFormat:
          - "([\\w-]+)=(Genotype.*)"
  algorithm:
    algorithmName: darts
    algorithmSettings:
      - name: num_epochs
        value: "3"
  nasConfig:                     <a class="co" id="co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO4-1" href="#callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO4-1"><img src="assets/1.png" alt="1" width="12" height="12"></a>
    graphConfig:
      numLayers: 3
    operations:
      - operationType: separable_convolution
        parameters:
          - name: filter_size
            parameterType: categorical
            feasibleSpace:
              list:
                - "3"
      - operationType: dilated_convolution
        parameters:
          - name: filter_size
            parameterType: categorical
            feasibleSpace:
              list:
                - "3"
                - "5"
      - operationType: avg_pooling
        parameters:
          - name: filter_size
            parameterType: categorical
            feasibleSpace:
              list:
                - "3"
      - operationType: max_pooling
        parameters:
          - name: filter_size
            parameterType: categorical
            feasibleSpace:
              list:
                - "3"
      - operationType: skip_connection
  trialTemplate:
    trialParameters:
      - name: algorithmSettings
        description: Algorithm settings of DARTS Experiment
        reference: algorithm-settings
      - name: searchSpace
        description: Search Space of DARTS Experiment
        reference: search-space
      - name: numberLayers
        description: Number of Neural Network layers
        reference: num-layers
    trialSpec:
      apiVersion: batch/v1
      kind: Job
      spec:
        template:
          spec:
            containers:
              - name: training-container
                image: docker.io/kubeflowkatib/darts-cnn-cifar10
                imagePullPolicy: Always
                command:
                  - python3
                  - run_trial.py
                  - --algorithm-settings="${trialParameters.algorithmSettings}"
                  - --search-space="${trialParameters.searchSpace}"
                  - --num-layers="${trialParameters.numberLayers}"
                resources:
                  limits:
                    nvidia.com/gpu: 1
            restartPolicy: Never</pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO4-1" href="#co_hyperparameter_tuning_and_automated___span_class__keep_together__machine_learning__span__CO4-1"><img src="assets/1.png" alt="1" width="12" height="12"></a></dt>
<dd><p>The general structure of a NAS experiment is similar to that of a hyperparameter search experiment. The majority of the specification should look very familiar; the most important difference is the addition of the <code>nasConfig</code>. This is where you can configure the specifications of the neural network that you want to create, such as the number of layers, the inputs and outputs at each layer, and the types of operations.<a data-type="indexterm" data-startref="ch10-mm" id="idm45831164144904"></a><a data-type="indexterm" data-startref="ch10-mm2" id="idm45831164144296"></a><a data-type="indexterm" data-startref="ch10-mm3" id="idm45831164143688"></a></p></dd>
</dl></div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Advantages of Katib over Other Frameworks"><div class="sect1" id="idm45831164142824" title2="Advantages of Katib over Other Frameworks" no2="10.8">
<h1>10.8. Advantages of Katib over Other Frameworks</h1>

<p>There are many similar open source systems for hyperparameter search, among them<a data-type="indexterm" data-primary="Kubeflow Katib" data-secondary="advantages of" id="idm45831164141464"></a><a data-type="indexterm" data-primary="Katib (Kubeflow)" data-secondary="advantages of" id="idm45831164140616"></a><a data-type="indexterm" data-primary="hyperparameters" data-secondary="Kubeflow Katib for tuning" id="idm45831164139768"></a>  <a href="https://oreil.ly/lDwpN">NNI</a>, <a href="https://oreil.ly/gAHgZ">Optuna</a>, <a href="https://oreil.ly/CnNFZ">Ray Tune</a>, and <a href="https://oreil.ly/jlfoP">Hyperopt</a>. In addition, the original design of Katib was inspired by <a href="https://oreil.ly/q1xiz">Google Vizier</a>. While these frameworks offer many capabilities similar to Katib’s, namely the ability to configure parallel hyperparameter sweeps using a variety of algorithms, there are a few features of Katib that make it unique:</p>
<dl>
<dt>Design catering to both user and admin</dt>
<dd>
<p>Most tuning frameworks are designed to cater to the <em>user</em>—the data scientist performing the tuning experiment. Katib is also designed to make life easier for the <em>system admin</em>, who is responsible for maintaining the infrastructure, allocating compute resources, and monitoring system health.</p>
</dd>
<dt>Cloud native design</dt>
<dd>
<p>Other frameworks (such as Ray Tune) may support integration with Kubernetes, but often require additional effort to set up a cluster. By contrast, Katib is the first hyperparameter search framework to base its design entirely on Kubernetes; every one of its resources can be accessed and manipulated by Kubernetes APIs.</p>
</dd>
<dt>Scalable and portable</dt>
<dd>
<p>Because Katib uses Kubernetes as its orchestration engine, it is very easy to scale up an experiment. You can run the same experiments on a laptop for prototyping and deploy the job to a production cluster with minimal changes to the spec. By contrast, other frameworks require additional effort to install and configure depending on the hardware availability.</p>
</dd>
<dt>Extensible</dt>
<dd>
<p>Katib offers flexible and pluggable interfaces for its search algorithms and storage systems. Most other frameworks come with a preset list of algorithms and have hardcoded mechanisms for metrics collection. In Katib, the user can easily implement a custom search algorithm and integrate it with the framework.</p>
</dd>
<dt>Native support</dt>
<dd>
<p>Katib natively supports advanced features like distributed training and neural architecture search.</p>
</dd>
</dl>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45831164126824" title2="Conclusion" no2="10.9">
<h1>10.9. Conclusion</h1>

<p>In this chapter we’ve taken a quick overview of AutoML and learned how it can accelerate the development of machine learning models by automating time-consuming tasks like hyperparameter search. With techniques like automated hyperparameter tuning, you can scale up the development of your models while sustaining high model quality.</p>

<p>We have then used Katib—a Kubernetes-native tuning service from the Kubeflow platform—to configure and execute a hyperparameter search experiment. We have also shown how you can use Katib’s dashboard to submit, track, and visualize your 
experiments.</p>

<p>We’ve also explored how Katib handles neural architecture search (NAS). Katib currently supports two methods of NAS—DARTS and ENAS, with more development to follow.</p>

<p>Hopefully, this has given you some insights into how Katib can be leveraged to reduce the amount of work in your machine learning workflows. Katib is still an evolving project, and you can follow the latest developments on this <a href="https://oreil.ly/OHFAL">Katib GitHub page</a>.</p>
<p>&nbsp;</p>

<p>Thank you for joining us on your adventures in learning Kubeflow.
We hope that Kubeflow meets your needs and helps you deliver on machine learning’s ability to bring value to your organization. To keep up to date on the latest changes with Kubeflow, we encourage you to join the<a data-type="indexterm" data-primary="Kubeflow" data-secondary="online community" id="idm45831164121064"></a><a data-type="indexterm" data-primary="resources for Kubeflow" data-secondary="online community" id="idm45831164120216"></a><a data-type="indexterm" data-primary="online community for Kubeflow" id="idm45831164119368"></a><a data-type="indexterm" data-primary="Kubeflow Slack workspace" id="idm45831164118760"></a> <a href="https://oreil.ly/4fT2i">Kubeflow Slack workspace and mailing lists</a>.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45831164182632"><sup><a href="#idm45831164182632-marker">[1]</a></sup> T. Elsken, J. H. Metzen, F. Hutter, “Neural Architecture Search: A Survey,” <em>Journal of Machine Learning Research</em> 20 (2019), <a href=""><em class="hyperlink">https://oreil.ly/eO-CV</em></a>, pp. 1-21.</p><p data-type="footnote" id="idm45831164168328"><sup><a href="#idm45831164168328-marker">[2]</a></sup> H. Liu, K. Simonyan, and Y. Tang, “Differentiable Architecture Search (DARTS),” <a href=""><em class="hyperlink">https://oreil.ly/JSAIX</em></a>.</p><p data-type="footnote" id="idm45831164166328"><sup><a href="#idm45831164166328-marker">[3]</a></sup> H. Pham et al., “Efficient Neural Architecture Search via Parameter Sharing,” <a href=""><em class="hyperlink">https://oreil.ly/SQPxn</em></a>.</p></div></div></section>
<section data-type="appendix" data-pdf-bookmark="Appendix A. Argo Executor Configurations and Trade-Offs"><div class="appendix" id="appendix_executors" data-type="appendix" title2="Argo Executor Configurations and Trade-Offs" no2="A">
<h1>Appendix A. Argo Executor Configurations and Trade-Offs</h1>


<p>Until recently, all Kubernetes implementations supported Docker APIs. The initial Argo implementation depended on them.<a data-type="indexterm" data-primary="Argo executors APIs" id="idm45831164115176"></a><a data-type="indexterm" data-primary="Argo Workflows" data-secondary="executors" id="idm45831164114568"></a><a data-type="indexterm" data-primary="executors for Argo Workflows" id="idm45831164113720"></a><a data-type="indexterm" data-primary="Argo Workflows" data-secondary="Docker as executor" id="idm45831164113112"></a><a data-type="indexterm" data-primary="Docker" data-secondary="Argo Workflows executor" id="idm45831164112264"></a>
With the introduction of <a href="https://oreil.ly/bIoqk">OpenShift 4</a>, which doesn’t support the Docker APIs, the situation changed. To support the absence of Docker APIs, Argo introduced several new executors: Docker, Kubelet, and Kubernetes APIs. The <code>containerRuntimeExecutor</code> config value in the Argo parameters file controls which executor is used.
The pros and cons of each executor (based on the information here) are summarized in <a data-type="xref" href="#argo_k8s_table">TABLE A-1</a>. This table should help you pick the correct value of the Argo executor.</p>
<table id="argo_k8s_table" data-type="table" title2="Argo and Kubernetes APIs" no2="A-1">
<caption>Table A-1. Argo and Kubernetes APIs</caption>
<thead>
<tr>
<th>Executor</th>
<th>Docker</th>
<th>Kubelet</th>
<th>Kubernetes API</th>
<th>PNC</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Pros</p></td>
<td><p>Supports all workflow examples. Most reliable, well tested, very scalable. Communicates with Docker daemon for heavy lifting.</p></td>
<td><p>Secure. Can’t escape pod’s service account privileges. Medium scalability. Log retrieval and container polling are done against Kubelet.</p></td>
<td><p>Secure. Can’t escape privileges of pod’s service account. No extra configuration.</p></td>
<td><p>Secure. Can’t escape service account privileges. Artifact collection can be done from base image layer. Scalable: process polling is done over procfs, not kubelet/k8s API.</p></td>
</tr>
<tr>
<td><p>Cons</p></td>
<td><p>Least secure. Requires <code>docker.sock</code> of host to be mounted (often rejected by OPA).</p></td>
<td><p>Additional kubelet configuration may be required. Can only save params/artifacts in volumes (e.g., <code>emptyDir</code>), and not the base image layer (e.g., <code>/tmp</code>).</p></td>
<td><p>Least scalable. Log retrieval and container polling are done against k8s API server. Can only save params/artifacts in volumes (e.g., <code>emptyDir</code>), and not the base image layer (e.g., <code>/tmp</code>).</p></td>
<td><p>Processes no longer run with pid 1. Artifact collection may fail for containers completing too fast. Can’t capture artifact directories from base image layer with volume mounted under it. Immature.</p></td>
</tr>
<tr>
<td><p>Argo Config</p></td>
<td><p>docker</p></td>
<td><p>kubelet</p></td>
<td><p>k8sapi</p></td>
<td><p>pns</p></td>
</tr>
</tbody>
</table>
</div></section>
<section data-type="appendix" data-pdf-bookmark="Appendix B. Cloud-Specific Tools and Configuration"><div class="appendix" id="appendix_cloud_specific" data-type="appendix" title2="Cloud-Specific Tools and Configuration" no2="B">
<h1>Appendix B. Cloud-Specific Tools and Configuration</h1>


<p>Cloud-specific tools can accelerate your development, but they can also cause vendor lock-in.</p>






<section data-type="sect1" data-pdf-bookmark="Google Cloud"><div class="sect1" id="google_cloud_specific" title2="Google Cloud" no2="B.1">
<h1>B.1. Google Cloud</h1>

<p>Since Kubeflow originates from Google, it is no surprise that there are some extra features available when running on Google Cloud. We’ll quickly point out how to use TPUs and Dataflow to accelerate your machine learning pipelines, and more Google-specific components are available in the<a data-type="indexterm" data-primary="Google Cloud Platform (GCP)" data-secondary="Google-specific components" id="idm45831164089384"></a><a data-type="indexterm" data-primary="components" data-secondary="Google-specific" id="idm45831164088536"></a> <a href="https://oreil.ly/F7c9l">Kubeflow GitHub repo</a>.</p>








<section data-type="sect2" data-pdf-bookmark="TPU-Accelerated Instances"><div class="sect2" id="idm45831164086808" title2="TPU-Accelerated Instances" no2="B.1.1">
<h2>B.1.1. TPU-Accelerated Instances</h2>

<p>Different parts of the machine learning process can benefit from not only different <a data-type="indexterm" data-primary="Google Cloud Platform (GCP)" data-secondary="TPU-accelerated instances" id="idm45831164085672"></a><a data-type="indexterm" data-primary="TPU-accelerated instances" id="idm45831164084824"></a>numbers of machines, but also different types of machines.
The most common example is with model serving: often lots of low-memory machines can perform reasonably well, but for model training, high-memory or TPU accelerated machines can offer greater benefits. While there is a handy built-in shorthand for using GPUs, with TPUs you need to explicitly <code>import kfp.gcp as gcp</code>. Once you’ve imported kfp’s gcp you can add TPU resources to any container operation in a similar way to GPUs by adding <code>.apply(gcp.use_tpu(tpu_cores=cores, tpu_resource=version, tf_version=tf_version))</code> to your container operation.</p>
<div data-type="warning"><h6>Warning</h6>
<p>TPU nodes are only available in certain regions. Check <a href="https://oreil.ly/1HAzM">this Google Cloud page</a> for a list of supported regions.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Dataflow for TFX"><div class="sect2" id="idm45831164080920" title2="Dataflow for TFX" no2="B.1.2">
<h2>B.1.2. Dataflow for TFX</h2>

<p>On Google Cloud you can configure Kubeflow’s TFX components<a data-type="indexterm" data-primary="Google Dataflow" data-secondary="TensorFlow Extended configured for" id="idm45831164079544"></a><a data-type="indexterm" data-primary="TensorFlow Extended (TFX)" data-secondary="Google Dataflow" id="idm45831164078696"></a><a data-type="indexterm" data-primary="TFX" data-see="TensorFlow Extended" id="idm45831164077848"></a> to use Google’s Dataflow for distributed processing.
To do this, you will need to specify a distributed output location (since there is not a shared persistent volume between the workers), and configure TFX to use the Dataflow runner.
The simplest way to show this is by revisiting <a data-type="xref" href="#use_csv_examples">EXAMPLE 5-8</a>; to use Dataflow we would change it to <a data-type="xref" href="#use_csv_examples_dataflow">EXAMPLE B-1</a>.</p>
<div id="use_csv_examples_dataflow" data-type="example" title2="Changing the pipeline to use Dataflow" no2="B-1">
<h5>Example B-1. Changing the pipeline to use Dataflow</h5>

<pre data-type="programlisting" data-code-language="python"><code class="n">generated_output_uri</code> <code class="o">=</code> <code class="n">root_output_uri</code> <code class="o">+</code> <code class="n">kfp</code><code class="o">.</code><code class="n">dsl</code><code class="o">.</code><code class="n">EXECUTION_ID_PLACEHOLDER</code>
<code class="n">beam_pipeline_args</code> <code class="o">=</code> <code class="p">[</code>
    <code class="s1">'--runner=DataflowRunner'</code><code class="p">,</code>
    <code class="s1">'--project='</code> <code class="o">+</code> <code class="n">project_id</code><code class="p">,</code>
    <code class="s1">'--temp_location='</code> <code class="o">+</code> <code class="n">root_output_uri</code> <code class="o">+</code> <code class="s1">'/tmp'</code><code class="p">),</code>
    <code class="s1">'--region='</code> <code class="o">+</code> <code class="n">gcp_region</code><code class="p">,</code>
    <code class="s1">'--disk_size_gb=50'</code><code class="p">,</code> <code class="c1"># Adjust as needed</code>
<code class="p">]</code>

<code class="n">records_example</code> <code class="o">=</code> <code class="n">tfx_csv_gen</code><code class="p">(</code>
    <code class="n">input_uri</code><code class="o">=</code><code class="n">fetch</code><code class="o">.</code><code class="n">output</code><code class="p">,</code> <code class="c1"># Must be on distributed storage</code>
    <code class="n">beam_pipeline_args</code><code class="o">=</code><code class="n">beam_pipeline_args</code><code class="p">,</code>
    <code class="n">output_examples_uri</code><code class="o">=</code><code class="n">generated_output_uri</code><code class="p">)</code></pre></div>

<p>As you can see, changing the pipeline to use Dataflow is relatively simple and opens up a larger scale of data for processing.</p>

<p>While cloud-specific accelerations can be beneficial, be careful that the trade-off is worth the additional future headache if you ever need to change providers.</p>
</div></section>





</div></section>







</div></section>
<section data-type="appendix" data-pdf-bookmark="Appendix C. Using Model Serving in Applications"><div class="appendix" id="Model_inference_appendix" data-type="appendix" title2="Using Model Serving in Applications" no2="C">
<h1>Appendix C. Using Model Serving in Applications</h1>


<p>In <a data-type="xref" href="#inference_ch">CHAPTER 8</a> you learned different approaches for exposing <a data-type="indexterm" data-primary="model serving" data-secondary="custom applications" data-tertiary="about" id="idm45831164013288"></a>model servers provided by Kubeflow. As described there, Kubeflow provides several ways of deploying trained models and providing both REST and gRPC interfaces for running model inference. However, it falls short in providing support for using these models in custom applications. Here we will present some of the approaches to building applications by leveraging model servers exposed by Kubeflow.</p>

<p>When it comes to applications leveraging model inference, they can be broadly classified into two categories: real time and batch applications. In the real time/stream applications model, inference is done on data directly as it is produced or received. In this case, typically only one request is available at a time and it can be used for inferencing as it arrives. In the batch scenarios all of the data is available up front and can be used for inference either sequentially or in parallel. We will start from the streaming use case and then take a look at possible batch implementations.</p>






<section data-type="sect1" data-pdf-bookmark="Building Streaming Applications Leveraging 
Model Serving"><div class="sect1" id="idm45831164010408" title2="Building Streaming Applications Leveraging 
Model Serving" no2="C.1">
<h1>C.1. Building Streaming Applications Leveraging <br>Model Serving</h1>

<p>The majority of today’s streaming applications leverage <a href="https://kafka.apache.org">Apache Kafka</a> as<a data-type="indexterm" data-primary="model serving" data-secondary="custom applications" data-tertiary="streaming applications" id="app-ms"></a><a data-type="indexterm" data-primary="streaming applications" data-secondary="about" id="idm45831164006344"></a> the data backbone of a system. The two possible options for implementing streaming applications themselves are: usage of stream processing engines and usage of stream processing libraries.</p>








<section data-type="sect2" data-pdf-bookmark="Stream Processing Engines and Libraries"><div class="sect2" id="idm45831164004936" title2="Stream Processing Engines and Libraries" no2="C.1.1">
<h2>C.1.1. Stream Processing Engines and Libraries</h2>

<p>As defined in the article “Defining the Execution Semantics of Stream Processing Engines,”<sup><a data-type="noteref" id="idm45831164003160-marker" href="#idm45831164003160">[1]</a></sup> modern <a data-type="indexterm" data-primary="libraries" data-secondary="stream processing" id="idm45831164000792"></a><a data-type="indexterm" data-primary="streaming applications" data-secondary="processing engines versus libraries" id="idm45831163999816"></a>stream processing engines are based on organizing computations into blocks and leveraging cluster architectures.<sup><a data-type="noteref" id="idm45831163998552-marker" href="#idm45831163998552">[2]</a></sup>
Splitting computations in blocks enables execution parallelism, where different blocks run on different threads on the same machine, or on different machines. It also enables failover by moving execution blocks from failed machines to healthy ones. Additionally, checkpointing supported by modern engines further improves the reliability of cluster-based execution.</p>

<p>Stream processing libraries, on the other hand, are libraries with a domain-specific language providing a set of constructs that simplify building streaming applications. Such libraries typically do not support distribution and/or clustering—this is typically left as an exercise for developers.</p>

<p>Because these options sound similar, they are often used interchangeably. In reality, as Jay Kreps has <a href="https://oreil.ly/hzK4d">outlined in his blog</a>, stream processing engines and stream processing libraries are two very different approaches to building streaming applications and choosing one of them is a trade-off between power and simplicity. As described previously, stream processing engines provide more functionality, but require a developer to  adhere to their programming model and deployment. They also often require a steeper learning curve for mastering their functionality.  Stream processing libraries, on another hand, are typically easier to use, providing more flexibility, but require specific implementation of deployment, scalability, and load 
balancing.</p>

<p>Today’s most popular <a href="https://oreil.ly/h7bKa">stream processing engines</a> include the following:<a data-type="indexterm" data-primary="Apache Flink" id="idm45831163993208"></a></p>

<ul>
<li>
<p><a href="https://spark.apache.org">Apache Spark</a></p>
</li>
<li>
<p><a href="https://flink.apache.org">Apache Flink</a></p>
</li>
<li>
<p><a href="https://beam.apache.org">Apache Beam</a></p>
</li>
</ul>

<p>The most popular stream libraries are:</p>

<ul>
<li>
<p><a href="https://oreil.ly/phyB-">Apache Kafka streams</a></p>
</li>
<li>
<p><a href="https://oreil.ly/-qlfT">Akka streams</a></p>
</li>
</ul>

<p>All of these can be used as a platform for building streaming applications including model
serving.<sup><a data-type="noteref" id="idm45831163984136-marker" href="#idm45831163984136">[3]</a></sup></p>

<p>A side-by-side <a href="https://oreil.ly/LehcG">comparison</a> of stream processing engines (Flink) and stream processing libraries (Kafka streams), done jointly by data Artisans (currently Vervetica) and Confluent teams, also emphasizes yet another difference between stream processing engines and libraries: enterprise ownership. Stream processing engines are typically owned and managed centrally by enterprise-wide units, while stream processing libraries are typically under the purview of individual development teams, which often makes their adoption much simpler.
A stream processing engine is a good fit for applications that require features provided out of the box by such engines, including cluster scalability and high throughput through parallelism across a cluster, event-time semantics, checkpointing, built-in support for monitoring and management, and mixing of stream and batch processing. The drawback of using engines is that you are constrained by the programming and deployment models they provide.</p>

<p>In contrast, the stream processing libraries provide a programming model that allows developers to build the applications or microservices the way that fits their precise needs and deploy them as simple standalone Java applications. But in this case they need to roll out their own scalability, high availability, and monitoring solutions (Kafka-based implementations support some of them by leveraging Kafka).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Introducing Cloudflow"><div class="sect2" id="idm45831164004344" title2="Introducing Cloudflow" no2="C.1.2">
<h2>C.1.2. Introducing Cloudflow</h2>

<p>In reality, most of the streaming application implementations require <a data-type="indexterm" data-primary="streaming applications" data-secondary="Cloudflow" id="app-cl2"></a><a data-type="indexterm" data-primary="Cloudflow" id="app-cl"></a>usage of multiple engines and libraries for building individual applications, which creates additional integration and maintenance complexities. Many of these can be alleviated by using an open source project, like <a href="https://cloudflow.io">Cloudflow</a>, which allows you to quickly develop, orchestrate, and operate distributed streaming applications on Kubernetes. Cloudflow supports building streaming applications as a set of small, composable components communicating over Kafka and wired together with schema-based contracts. This approach can significantly improve reuse and allows you to dramatically accelerate streaming application development. At the time of this writing, such components can be implemented using Akka Streams; Flink and Spark streaming with Kafka Streams support is coming soon. The overall architecture of Cloudflow is presented in <a data-type="xref" href="#Cloudflow_arch">FIGURE C-1</a>.</p>

<figure><div id="Cloudflow_arch" class="figure" data-type="figure" title2="Cloudflow architecture" no2="C-1">
<img src="assets/kfml_ad01a.png" alt="Cloudflow architecture" width="1212" height="815">
<h6>Figure C-1. Cloudflow architecture</h6>
</div></figure>

<p>In the heart of Cloudflow is a Cloudflow operator, which is responsible for deploying/undeploying, management, and scaling of pipelines and individual streamlets. The operator also leverages existing <a href="https://oreil.ly/pg2JL">Flink</a> and <a href="https://oreil.ly/J2umN">Spark</a> operators to manage Flink and Spark streamlets. A set of provided Helm charts allows for simple installation of the operator and supporting components.</p>

<p>A common challenge when building streaming applications is wiring all of the components together and testing them end-to-end before going into production. Cloudflow addresses this by allowing you to validate the connections between components and to run your application locally during development to avoid surprises during deployment.</p>

<p>Everything in Cloudflow is done in the context of an application, which represents a self-contained distributed system (graph) of data processing services connected together by data streams over Kafka.</p>

<p>Cloudflow supports:</p>
<dl>
<dt>Development</dt>
<dd>
<p>By generating a lot of boilerplate code, it allows developers to focus on business logic.</p>
</dd>
<dt>Build</dt>
<dd>
<p>It provides all the tooling for going from business logic to a deployable Docker image.</p>
</dd>
<dt>Deploy</dt>
<dd>
<p>It provides Kubernetes tooling to deploy your distributed application with a single command.</p>
</dd>
<dt>Operate</dt>
<dd>
<p>It provides all the tools you need to get insights, observability, and life cycle management for your distributed streaming application. Another important operational concern directly supported by Cloudflow is an ability to scale individual components of the stream.</p>
</dd>
</dl>

<p>When using Cloudflow for implementing streaming applications, model server invocation is typically implemented by a separate streamlet<sup><a data-type="noteref" id="idm45831163932264-marker" href="#idm45831163932264">[4]</a></sup> based on a <a href="https://oreil.ly/Wijie">dynamically controlled stream</a> pattern.</p>

<p>In <a data-type="xref" href="#dynamically_controlled_figure">FIGURE C-2</a> an implementation contains a state, where a state is a URL to the model serving server, in the case when a model server is used for inference.<sup><a data-type="noteref" id="idm45831163927944-marker" href="#idm45831163927944">[5]</a></sup> The actual data processing in this case is done by invoking a model server to get an inference result. This call can be done using either REST or gRPC (or any other interface supported by the model server).</p>

<figure><div id="dynamically_controlled_figure" class="figure" data-type="figure" title2="Dynamically controlled stream pattern" no2="C-2">
<img src="assets/kfml_ad01.png" alt="Dynamically controlled stream pattern" width="1398" height="445">
<h6>Figure C-2. Dynamically controlled stream pattern</h6>
</div></figure>

<p>This state can be updated through an additional Kafka topic, which allows for switching the URL (in the case when model server deployment is moved) without redeployment of the applications. The state is used by a data processor for processing incoming data.</p>

<p>Additional streamlets (with the same architecture) can be introduced into the application to get model serving insights, such as explanation and drift detection (see <a data-type="xref" href="#Model_Monitor">SECTION 8.2</a> for more details).<a data-type="indexterm" data-startref="app-cl" id="idm45831163922984"></a><a data-type="indexterm" data-startref="app-cl2" id="idm45831163922280"></a><a data-type="indexterm" data-startref="app-ms" id="idm45831163921608"></a></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Building Batch Applications Leveraging Model Serving"><div class="sect1" id="idm45831163920808" title2="Building Batch Applications Leveraging Model Serving" no2="C.2">
<h1>C.2. Building Batch Applications Leveraging Model Serving</h1>

<p>A typical batch application is implemented by reading a dataset containing <a data-type="indexterm" data-primary="model serving" data-secondary="custom applications" data-tertiary="batch applications" id="app-bat2"></a><a data-type="indexterm" data-primary="batch applications via model serving" id="app-bat"></a>all the samples and then processing them, invoking the model server for every one of them. The simplest batch application implementation is doing this sequentially, one data element at a time. Although such implementation will work, it is not very performant, due to the network overhead for processing every element.</p>

<p>One popular way to speed up processing is to use batching. TFServing, for example, <a data-type="indexterm" data-primary="TensorFlow Serving (TFServing)" data-secondary="batch applications" id="idm45831163915832"></a>supports <a href="https://oreil.ly/v7LFl">two batching approaches</a>: server-side batching and client-side batching.</p>

<p>Server-side batching is supported out of the box by TFServing.<sup><a data-type="noteref" id="idm45831163913320-marker" href="#idm45831163913320">[6]</a></sup> To enable batching, set <code>--enable_batching</code> and <code>--batching_parameters_file</code> flags. To achieve the best trade-offs between latency and throughput, pick appropriate batching parameters.<sup><a data-type="noteref" id="idm45831163910920-marker" href="#idm45831163910920">[7]</a></sup> Some of the recommendations for the parameters values for both CPU and GPU usage can be found in <a href="https://oreil.ly/TecPs">this TFServing GitHub repo</a>.</p>

<p>Upon reaching full batch on the server side, inference requests are merged internally into a single large request (tensor) and a Tensorflow Session is run on the merged request. You need to use asynchronous client requests to populate server-side batches. Running a batch of requests on a single session is where CPU/GPU parallelism can really be leveraged.</p>

<p>Client-side batching is just grouping multiple inputs together on the client to make a single request.</p>

<p>Although batching can significantly improve performance of the batch inference, it’s often not sufficient for reaching performance goals. Another popular approach for performance improvement is multithreading.<sup><a data-type="noteref" id="idm45831163906808-marker" href="#idm45831163906808">[8]</a></sup>
The idea behind this approach is to deploy multiple instances of a model server, split data processing into multiple threads, and allow each thread to do inference for part of the data it is responsible for.</p>

<p>One of the ways to implement multithreading is through a batch implementation via streaming. This can be done by implementing software component<sup><a data-type="noteref" id="idm45831163904600-marker" href="#idm45831163904600">[9]</a></sup> reading source data and writing each record to Kafka for processing. This approach effectively turns batch processing into a streaming one to allow for better scalability through an architecture as shown in <a data-type="xref" href="#batch_processing_figure">FIGURE C-3</a>.</p>

<figure><div id="batch_processing_figure" class="figure" data-type="figure" title2="Using stream processing for batch serving implementation" no2="C-3">
<img src="assets/kfml_ad02.png" alt="Using Stream processing for batch serving implementation" width="1204" height="1007">
<h6>Figure C-3. Using stream processing for batch serving implementation</h6>
</div></figure>

<p>This deployment includes three layers:</p>

<ul>
<li>
<p>Cloudflow-based stream processing that invokes model serving for every element of the stream. Every streamlet of this solution can be scaled appropriately to provide required throughput.</p>
</li>
<li>
<p>A model server that does the actual model inference. This layer can be independently scaled by changing the amount of model servers.</p>
</li>
<li>
<p>Load balancers, for example Istio or Ambassador, that provide load balancing for inference REST/gRPC requests.</p>
</li>
</ul>

<p>Because every layer in this architecture can scale independently, such an architecture can provide a model serving solution that is quite scalable for both streaming and batch use cases.<a data-type="indexterm" data-startref="app-bat" id="idm45831163896136"></a><a data-type="indexterm" data-startref="app-bat2" id="idm45831163895432"></a></p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm45831164003160"><sup><a href="#idm45831164003160-marker">[1]</a></sup> L. Affetti et al., “Defining the Execution Semantics of Stream Processing Engines,” <em>Journal of Big Data</em> 4 (2017), <a href=""><em class="hyperlink">https://oreil.ly/TcI39</em></a>.</p><p data-type="footnote" id="idm45831163998552"><sup><a href="#idm45831163998552-marker">[2]</a></sup> Compare to MapReduce architecture.</p><p data-type="footnote" id="idm45831163984136"><sup><a href="#idm45831163984136-marker">[3]</a></sup> For implementation details, see the report, <a href=""><em>Serving Machine Learning Models</em></a>, and <a href="">Kai Waehner’s project on GitHub</a>.</p><p data-type="footnote" id="idm45831163932264"><sup><a href="#idm45831163932264-marker">[4]</a></sup> Some of the examples of such implementations for TFServing integration can be found <a href="">in this GitHub repo</a>, and for Seldon integration, <a href="">in this GitHub repo</a>.</p><p data-type="footnote" id="idm45831163927944"><sup><a href="#idm45831163927944-marker">[5]</a></sup> In the case of embedded model usage, the state is a model itself.</p><p data-type="footnote" id="idm45831163913320"><sup><a href="#idm45831163913320-marker">[6]</a></sup> See this <a href="">TFServing document</a> for more details.</p><p data-type="footnote" id="idm45831163910920"><sup><a href="#idm45831163910920-marker">[7]</a></sup> For the complete definitions of available parameters, see <a href="">this TFServing GitHub repo</a>.</p><p data-type="footnote" id="idm45831163906808"><sup><a href="#idm45831163906808-marker">[8]</a></sup> Compare to the <a href="">MapReduce</a> programming model.</p><p data-type="footnote" id="idm45831163904600"><sup><a href="#idm45831163904600-marker">[9]</a></sup> Streamlet, in the case of Cloudflow-based implementation.</p></div></div></section>
<section data-type="index"><div class="index" id="idm45831163894392" data-type="index" title2="Index" no2=""><h1>Index</h1><div data-type="index"><div data-type="indexdiv"><h3>A</h3><ul><li><span data-type="index-term">A/B testing</span>, <a data-type="index:locator" href="#idm45831170690008">Model Updating</a></li><li><span data-type="index-term">administrator of Kubeflow cluster</span>, <a data-type="index:locator" href="#idm45831179762120">Kubeflow Multiuser Isolation</a></li><li><span data-type="index-term">Amazon EMR</span>, <a data-type="index:locator" href="#idm45831176351976">Spark operators in Kubeflow</a></li><li><span data-type="index-term">Ambassador</span>, <a data-type="index:locator" href="#idm45831169774776">Serving Requests</a></li><li><span data-type="index-term">annotations on pipelines</span>, <a data-type="index:locator" href="#idm45831179361288">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">Apache Beam</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831177057448">Distributed Tooling</a></li><li><span data-type="index-term">Python support</span>, <a data-type="index:locator" href="#idm45831177015992">TensorFlow Extended</a></li><li><span data-type="index-term">TensorFlow Extended on top of</span>, <a data-type="index:locator" href="#idm45831177049208">TensorFlow Extended</a></li></ul></li><li><span data-type="index-term">Apache Flink</span>, <a data-type="index:locator" href="#idm45831163993208">Stream Processing Engines and Libraries</a></li><li><span data-type="index-term">Apache Mahout</span>, <a data-type="index:locator" href="#idm45831167805048">The Denoising CT Scans Example</a>, <a data-type="index:locator" href="#idm45831167200984">DS-SVD with Apache Spark</a></li><li><span data-type="index-term">Apache Software Foundation</span> (<span data-gentext="see">see</span> mailing list data preparation)</li><li><span data-type="index-term">Apache SpamAssassin</span>, <a data-type="index:locator" href="#idm45831177193208">Data Cleaning: Filtering Out the Junk</a>, <a data-type="index:locator" href="#idm45831175028424">Putting It Together in a Pipeline</a></li><li><span data-type="index-term">Apache Spark</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831179772632">Apache Spark</a>, <a data-type="index:locator" href="#idm45831177058424">Distributed Tooling</a>, <a data-type="index:locator" href="#idm45831176529976">Distributed Data Using Apache Spark</a></li><li><span data-type="index-term">basics</span>, <a data-type="index:locator" href="#idm45831176090648">Spark operators in Kubeflow</a></li><li><span data-type="index-term">cloud-specific options for running</span>, <a data-type="index:locator" href="#idm45831176368280">Spark operators in Kubeflow</a></li><li><span data-type="index-term">configuring</span>, <a data-type="index:locator" href="#idm45831176060520">Spark operators in Kubeflow</a></li><li><span data-type="index-term">data denoising example id=ch09-dd5</span>, <a data-type="index:locator" href="#idm45831167815880">The Denoising CT Scans Example</a></li><li><span data-type="index-term">data denoising pipeline</span>, <a data-type="index:locator" href="#ch09-dnz">The CT Scan Denoising Pipeline</a>-<a data-type="index:locator" href="#idm45831165407368">The pipeline</a></li><li><span data-type="index-term">feature preparation example</span>, <a data-type="index:locator" href="#idm45831175392520">Distributed Feature Preparation Using Apache Spark</a></li><li><span data-type="index-term">Jupyter notebooks</span>, <a data-type="index:locator" href="#idm45831176518936">Distributed Data Using Apache Spark</a></li><li><span data-type="index-term">Kubeflow native operator</span>, <a data-type="index:locator" href="#ch05-emr">Spark operators in Kubeflow</a>-<a data-type="index:locator" href="#idm45831175816568">Spark operators in Kubeflow</a></li><li><span data-type="index-term">mailing list example</span>, <a data-type="index:locator" href="#idm45831175389224">Distributed Feature Preparation Using Apache Spark</a></li><li><span data-type="index-term">MinIO configuration</span>, <a data-type="index:locator" href="#idm45831175853784">Spark operators in Kubeflow</a></li><li><span data-type="index-term">ResourceOp request validation</span>, <a data-type="index:locator" href="#idm45831176345288">Spark operators in Kubeflow</a></li><li><span data-type="index-term">resources on</span>, <a data-type="index:locator" href="#idm45831176526104">Distributed Data Using Apache Spark</a></li><li><span data-type="index-term">schema validation</span>, <a data-type="index:locator" href="#idm45831175746440">Validating the schema</a></li><li><span data-type="index-term">SQL</span>, <a data-type="index:locator" href="#idm45831175561768">Filtering out bad data</a></li><li><span data-type="index-term">using with Kubeflow</span>, <a data-type="index:locator" href="#idm45831190333736">Data/Feature Preparation</a>, <a data-type="index:locator" href="#idm45831176529000">Distributed Data Using Apache Spark</a></li></ul></li><li><span data-type="index-term">Apache Spark in Kubeflow</span><ul><li><span data-type="index-term">components</span>, <a data-type="index:locator" href="#idm45831179771656">Apache Spark</a></li></ul></li><li><span data-type="index-term">architecture</span><ul><li><span data-type="index-term">Differentiable Architecture Search (DARTS)</span>, <a data-type="index:locator" href="#idm45831164170536">Neural Architecture Search</a></li><li><span data-type="index-term">Efficient Neural Architecture Search (ENAS)</span>, <a data-type="index:locator" href="#idm45831164171384">Neural Architecture Search</a></li><li><span data-type="index-term">neural architecture search support and Katb</span>, <a data-type="index:locator" href="#idm45831164169688">Neural Architecture Search</a></li></ul></li><li><span data-type="index-term">Argo executors APIs</span>, <a data-type="index:locator" href="#idm45831164115176">Argo Executor Configurations and Trade-Offs</a></li><li><span data-type="index-term">Argo Workflows</span><ul><li><span data-type="index-term">deleting a workflow</span>, <a data-type="index:locator" href="#idm45831178592296">Argo: the Foundation of Pipelines</a></li><li><span data-type="index-term">Docker as executor</span>, <a data-type="index:locator" href="#idm45831178828472">Argo: the Foundation of Pipelines</a>, <a data-type="index:locator" href="#idm45831164113112">Argo Executor Configurations and Trade-Offs</a></li><li><span data-type="index-term">executing pipeline YAML files</span>, <a data-type="index:locator" href="#idm45831179688344">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">execution information</span>, <a data-type="index:locator" href="#idm45831178810552">Argo: the Foundation of Pipelines</a></li><li><span data-type="index-term">execution logs</span>, <a data-type="index:locator" href="#idm45831178603352">Argo: the Foundation of Pipelines</a></li><li><span data-type="index-term">executors</span>, <a data-type="index:locator" href="#idm45831178825864">Argo: the Foundation of Pipelines</a>, <a data-type="index:locator" href="#idm45831164114568">Argo Executor Configurations and Trade-Offs</a></li><li><span data-type="index-term">flow execution graph details</span>, <a data-type="index:locator" href="#idm45831178693272">Argo: the Foundation of Pipelines</a></li><li><span data-type="index-term">installing</span>, <a data-type="index:locator" href="#idm45831178830840">Argo: the Foundation of Pipelines</a></li><li><span data-type="index-term">Kubeflow Pipelines built on</span>, <a data-type="index:locator" href="#idm45831180179400">Kubeflow Pipelines</a>, <a data-type="index:locator" href="#ch04-pip2">Introduction to Kubeflow Pipelines Components</a>-<a data-type="index:locator" href="#idm45831178662600">Argo: the Foundation of Pipelines</a></li><li><span data-type="index-term">Kubeflow Pipelines enhancing</span>, <a data-type="index:locator" href="#idm45831178653368">What Kubeflow Pipelines Adds to Argo Workflow</a></li><li><span data-type="index-term">orchestration controller</span>, <a data-type="index:locator" href="#idm45831180154488">Kubeflow Pipelines</a></li><li><span data-type="index-term">parameter passing</span>, <a data-type="index:locator" href="#idm45831178469208">Building a Pipeline Using Existing Images</a></li><li><span data-type="index-term">pipelines visible</span>, <a data-type="index:locator" href="#idm45831178813784">Argo: the Foundation of Pipelines</a>, <a data-type="index:locator" href="#idm45831178666360">Argo: the Foundation of Pipelines</a></li><li><span data-type="index-term">UI for pipeline execution</span>, <a data-type="index:locator" href="#idm45831178639432">Argo: the Foundation of Pipelines</a>, <a data-type="index:locator" href="#idm45831178665384">Argo: the Foundation of Pipelines</a></li><li><span data-type="index-term">UI installation</span>, <a data-type="index:locator" href="#idm45831178682008">Argo: the Foundation of Pipelines</a></li><li><span data-type="index-term">YAML files for pipelines</span>, <a data-type="index:locator" href="#idm45831179704856">Exploring the Prepackaged Sample Pipelines</a></li></ul></li><li><span data-type="index-term">artifact store, for logged metadata events</span>, <a data-type="index:locator" href="#idm45831174097912">Kubeflow Metadata UI</a></li><li><span data-type="index-term">attribution for code examples</span>, <a data-type="index:locator" href="#idm45831189712136">Using Code Examples</a></li><li><span data-type="index-term">author contact information</span>, <a data-type="index:locator" href="#idm45831190062504">How to Contact the Authors</a></li><li><span data-type="index-term">AutoML (automated machine learning)</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831165228184">Hyperparameter Tuning and Automated 
Machine Learning</a></li><li><span data-type="index-term">continuous learning as</span>, <a data-type="index:locator" href="#idm45831170671320">Summary of Inference Requirements</a></li><li><span data-type="index-term">Kubeflow Katib</span>, <a data-type="index:locator" href="#idm45831180121912">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164803464">Hyperparameter Tuning with Kubeflow Katib</a></li></ul></li></ul></div><div data-type="indexdiv"><h3>B</h3><ul><li><span data-type="index-term">batch applications via model serving</span>, <a data-type="index:locator" href="#app-bat">Building Batch Applications Leveraging Model Serving</a>-<a data-type="index:locator" href="#idm45831163896136">Building Batch Applications Leveraging Model Serving</a></li><li><span data-type="index-term">Bayesian optimization</span><ul><li><span data-type="index-term">in Katib</span>, <a data-type="index:locator" href="#idm45831180118776">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164520072">Katib Concepts</a></li></ul></li><li><span data-type="index-term">beginners’ resources for ML</span>, <a data-type="index:locator" href="#pref-rsrc">Our Assumption About You</a>-<a data-type="index:locator" href="#idm45831190138376">Our Assumption About You</a><ul><li>(<span data-gentext="see">see also</span> getting started)</li></ul></li><li><span data-type="index-term">biases of machine learning</span>, <a data-type="index:locator" href="#idm45831190151464">Your Responsibility as a Practitioner</a></li><li><span data-type="index-term">blue-green deployment</span>, <a data-type="index:locator" href="#idm45831170696584">Model Updating</a><ul><li><span data-type="index-term">KFServing endpoints</span>, <a data-type="index:locator" href="#idm45831168957352">Data Plane</a>, <a data-type="index:locator" href="#idm45831167862504">Model updating</a></li></ul></li></ul></div><div data-type="indexdiv"><h3>C</h3><ul><li><span data-type="index-term">Caffe2 for distributed training</span>, <a data-type="index:locator" href="#idm45831171935208">Using Other Frameworks for Distributed Training</a></li><li><span data-type="index-term">canary deployment</span>, <a data-type="index:locator" href="#idm45831170693880">Model Updating</a></li><li><span data-type="index-term">canary deployments</span><ul><li><span data-type="index-term">KFServing endpoints</span>, <a data-type="index:locator" href="#idm45831168959240">Data Plane</a>, <a data-type="index:locator" href="#idm45831167861592">Model updating</a></li></ul></li><li><span data-type="index-term">Canonical resources</span>, <a data-type="index:locator" href="#idm45831189722392">Code Examples</a></li><li><span data-type="index-term">central dashboard</span>, <a data-type="index:locator" href="#idm45831180254024">Getting Around the Central Dashboard</a></li><li><span data-type="index-term">central storage distributed training</span>, <a data-type="index:locator" href="#idm45831172027288">Distributed Training</a></li><li><span data-type="index-term">Clipper</span>, <a data-type="index:locator" href="#idm45831189904760">Clipper (RiseLabs)</a></li><li><span data-type="index-term">cloud native microservices</span>, <a data-type="index:locator" href="#idm45831172140808">Deploying a TensorFlow Training Job</a></li><li><span data-type="index-term">Cloudflow</span>, <a data-type="index:locator" href="#app-cl">Introducing Cloudflow</a>-<a data-type="index:locator" href="#idm45831163922984">Introducing Cloudflow</a></li><li><span data-type="index-term">code examples in book</span><ul><li><span data-type="index-term">download link</span>, <a data-type="index:locator" href="#idm45831189725048">Code Examples</a></li><li><span data-type="index-term">permission for use</span>, <a data-type="index:locator" href="#idm45831189715352">Using Code Examples</a></li></ul></li><li><span data-type="index-term">collaborative filtering</span>, <a data-type="index:locator" href="#idm45831173154856">Building a Recommender with TensorFlow</a></li><li><span data-type="index-term">competing models</span>, <a data-type="index:locator" href="#idm45831170700456">Model Updating</a></li><li><span data-type="index-term">components</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#ch01-comp2">Kubeflow’s Design and Core Components</a>-<a data-type="index:locator" href="#idm45831189912408">Component Overview</a>, <a data-type="index:locator" href="#idm45831178030952">Kubeflow Pipeline Components</a></li><li><span data-type="index-term">central dashboard</span>, <a data-type="index:locator" href="#idm45831180255000">Getting Around the Central Dashboard</a></li><li><span data-type="index-term">file-fetching component</span>, <a data-type="index:locator" href="#idm45831178071560">Kubeflow Pipeline Components</a></li><li><span data-type="index-term">Google-specific</span>, <a data-type="index:locator" href="#idm45831164088536">Google Cloud</a></li><li><span data-type="index-term">hyperparameter tuning</span>, <a data-type="index:locator" href="#ch03-cpy">Hyperparameter Tuning</a>-<a data-type="index:locator" href="#idm45831180093592">Hyperparameter Tuning</a></li><li><span data-type="index-term">load_component function warning</span>, <a data-type="index:locator" href="#idm45831178049240">Kubeflow Pipeline Components</a>, <a data-type="index:locator" href="#idm45831176985880">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">load_component_from_file function</span>, <a data-type="index:locator" href="#idm45831176982904">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">metadata management</span>, <a data-type="index:locator" href="#idm45831180050952">Metadata</a><ul><li>(<span data-gentext="see">see also</span> Kubeflow ML Metadata)</li></ul></li><li><span data-type="index-term">model inference</span>, <a data-type="index:locator" href="#idm45831180090088">Model Inference</a></li><li><span data-type="index-term">multiuser isolation</span>, <a data-type="index:locator" href="#idm45831179766120">Kubeflow Multiuser Isolation</a></li><li><span data-type="index-term">pipelines</span>, <a data-type="index:locator" href="#idm45831180182536">Kubeflow Pipelines</a>, <a data-type="index:locator" href="#idm45831179731576">Kubeflow Pipelines</a><ul><li>(<span data-gentext="see">see also</span> Kubeflow Pipelines)</li></ul></li><li><span data-type="index-term">repositories</span>, <a data-type="index:locator" href="#idm45831179749176">Kubeflow Multiuser Isolation</a>, <a data-type="index:locator" href="#idm45831178083432">Kubeflow Pipeline Components</a></li><li><span data-type="index-term">training operators</span>, <a data-type="index:locator" href="#idm45831180216136">Training Operators</a></li></ul></li><li><span data-type="index-term">composability of Kubeflow</span>, <a data-type="index:locator" href="#idm45831190354584">Kubeflow’s Design and Core Components</a></li><li><span data-type="index-term">concept drift</span>, <a data-type="index:locator" href="#idm45831170732728">Model Accuracy, Drift, and Explainability</a></li><li><span data-type="index-term">conditional execution of pipelines</span>, <a data-type="index:locator" href="#ch04-cond2">Conditional Execution of Pipeline Stages</a>-<a data-type="index:locator" href="#idm45831177532440">Conditional Execution of Pipeline Stages</a></li><li><span data-type="index-term">container registry</span>, <a data-type="index:locator" href="#idm45831180914360">Setting up Docker</a></li><li><span data-type="index-term">containers</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831188242120">Why Containerize?</a></li><li><span data-type="index-term">beginners’ resources</span>, <a data-type="index:locator" href="#idm45831190140536">Our Assumption About You</a></li><li><span data-type="index-term">building example pipeline</span>, <a data-type="index:locator" href="#idm45831179682888">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">container registry</span>, <a data-type="index:locator" href="#idm45831180915304">Setting up Docker</a></li><li><span data-type="index-term">custom containers</span>, <a data-type="index:locator" href="#idm45831177071416">Custom Containers</a></li><li><span data-type="index-term">overhead</span>, <a data-type="index:locator" href="#idm45831190376136">Why Containerize?</a></li><li><span data-type="index-term">pipeline custom code and tools</span>, <a data-type="index:locator" href="#idm45831178507944">Building a Pipeline Using Existing Images</a>, <a data-type="index:locator" href="#idm45831177073336">Custom Containers</a></li><li><span data-type="index-term">resource about</span>, <a data-type="index:locator" href="#idm45831188245464">Why Containerize?</a></li><li><span data-type="index-term">serverless</span>, <a data-type="index:locator" href="#idm45831180077960">Model Inference</a></li><li><span data-type="index-term">SpamAssassin package</span>, <a data-type="index:locator" href="#idm45831177187608">Data Cleaning: Filtering Out the Junk</a>, <a data-type="index:locator" href="#idm45831175027752">Putting It Together in a Pipeline</a></li><li><span data-type="index-term">tag for pushing</span>, <a data-type="index:locator" href="#idm45831180906648">Setting up Docker</a></li><li><span data-type="index-term">why containerize</span>, <a data-type="index:locator" href="#idm45831188246440">Why Containerize?</a></li></ul></li><li><span data-type="index-term">continuous learning (CL)</span>, <a data-type="index:locator" href="#idm45831170672952">Summary of Inference Requirements</a></li><li><span data-type="index-term">control plane</span>, <a data-type="index:locator" href="#idm45831168980680">Serverless and the Service Plane</a></li><li><span data-type="index-term">COVID-19 pandemic</span>, <a data-type="index:locator" href="#idm45831185480664">CT Scans</a>, <a data-type="index:locator" href="#idm45831167809800">The Denoising CT Scans Example</a></li><li><span data-type="index-term">CRDs</span> (<span data-gentext="see">see</span> custom resource definitions)</li><li><span data-type="index-term">credentials for MinIO</span>, <a data-type="index:locator" href="#idm45831179909128">MinIO</a></li><li><span data-type="index-term">CSV component in recommender system</span>, <a data-type="index:locator" href="#idm45831176825352">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">CT scan data denoised</span><ul><li><span data-type="index-term">about data</span>, <a data-type="index:locator" href="#idm45831185481640">CT Scans</a>, <a data-type="index:locator" href="#idm45831167828280">Case Study Using Multiple Tools</a>, <a data-type="index:locator" href="#idm45831167812392">The Denoising CT Scans Example</a></li><li><span data-type="index-term">Apache Spark</span>, <a data-type="index:locator" href="#idm45831175585976">Filtering out bad data</a>, <a data-type="index:locator" href="#ch09-dnz5">The CT Scan Denoising Pipeline</a>-<a data-type="index:locator" href="#idm45831165191880">The pipeline</a></li><li><span data-type="index-term">data preparation</span>, <a data-type="index:locator" href="#idm45831167796456">Data Prep with Python</a></li><li><span data-type="index-term">decomposing CT scan</span>, <a data-type="index:locator" href="#idm45831167195832">DS-SVD with Apache Spark</a></li><li><span data-type="index-term">denoising pipeline</span>, <a data-type="index:locator" href="#ch09-dnz3">The CT Scan Denoising Pipeline</a>-<a data-type="index:locator" href="#idm45831165405992">The pipeline</a></li><li><span data-type="index-term">open source method</span>, <a data-type="index:locator" href="#idm45831167820744">Case Study Using Multiple Tools</a>, <a data-type="index:locator" href="#idm45831167808184">The Denoising CT Scans Example</a></li><li><span data-type="index-term">resource on math</span>, <a data-type="index:locator" href="#idm45831167802648">The Denoising CT Scans Example</a></li><li><span data-type="index-term">sharing the pipeline</span>, <a data-type="index:locator" href="#idm45831165188456">Sharing the Pipeline</a></li><li><span data-type="index-term">singular value decomposition</span>, <a data-type="index:locator" href="#idm45831167825272">Case Study Using Multiple Tools</a>, <a data-type="index:locator" href="#idm45831167236088">DS-SVD with Apache Spark</a></li><li><span data-type="index-term">visualizing denoised DICOMs</span>, <a data-type="index:locator" href="#ch09-viz2">Visualization</a>-<a data-type="index:locator" href="#idm45831165631736">Recomposing the matrix into denoised images</a></li></ul></li><li><span data-type="index-term">custom containers</span>, <a data-type="index:locator" href="#idm45831177069256">Custom Containers</a></li><li><span data-type="index-term">custom resource definitions (CRDs)</span><ul><li><span data-type="index-term">Knative Serving</span>, <a data-type="index:locator" href="#idm45831179790536">Knative</a></li><li><span data-type="index-term">Pipeline Service</span>, <a data-type="index:locator" href="#idm45831180158808">Kubeflow Pipelines</a></li></ul></li><li><span data-type="index-term">custom resources on Kubernetes</span>, <a data-type="index:locator" href="#idm45831172136568">Deploying a TensorFlow Training Job</a><ul><li><span data-type="index-term">Kubeflow Katib</span>, <a data-type="index:locator" href="#idm45831165200424">Katib Concepts</a>, <a data-type="index:locator" href="#idm45831164370056">Configuring an Experiment</a></li></ul></li></ul></div><div data-type="indexdiv"><h3>D</h3><ul><li><span data-type="index-term">data</span><ul><li><span data-type="index-term">data lineage</span>, <a data-type="index:locator" href="#idm45831174952328">Artifact and Metadata Store</a></li><li><span data-type="index-term">DICOM file format</span>, <a data-type="index:locator" href="#idm45831167793896">Data Prep with Python</a></li><li><span data-type="index-term">distributed object storage server</span>, <a data-type="index:locator" href="#idm45831180014472">MinIO</a>, <a data-type="index:locator" href="#idm45831178940136">Storing Data Between Steps</a></li><li><span data-type="index-term">environment variables for pipelines</span>, <a data-type="index:locator" href="#idm45831178503576">Building a Pipeline Using Existing Images</a></li><li><span data-type="index-term">exploring new</span>, <a data-type="index:locator" href="#idm45831175031128">Putting It Together in a Pipeline</a></li><li><span data-type="index-term">file-fetching component</span>, <a data-type="index:locator" href="#idm45831178069912">Kubeflow Pipeline Components</a></li><li><span data-type="index-term">Kubernetes Pods storing</span>, <a data-type="index:locator" href="#idm45831180149048">Kubeflow Pipelines</a></li><li><span data-type="index-term">metadata definition</span>, <a data-type="index:locator" href="#idm45831174953864">Artifact and Metadata Store</a><ul><li>(<span data-gentext="see">see also</span> metadata)</li></ul></li><li><span data-type="index-term">persistent volumes</span>, <a data-type="index:locator" href="#ch04-pvs3">Storing Data Between Steps</a>-<a data-type="index:locator" href="#idm45831178981848">Storing Data Between Steps</a><ul><li><span data-type="index-term">Apache Spark output</span>, <a data-type="index:locator" href="#idm45831175492792">Saving the output</a></li><li><span data-type="index-term">filesystem/get_file component</span>, <a data-type="index:locator" href="#idm45831176831944">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">local data preparation</span>, <a data-type="index:locator" href="#idm45831177472248">Fetching the Data</a></li></ul></li><li><span data-type="index-term">preparation of</span>, <a data-type="index:locator" href="#idm45831190336024">Data/Feature Preparation</a>, <a data-type="index:locator" href="#idm45831177515016">Data and Feature Preparation</a><ul><li>(<span data-gentext="see">see also</span> data preparation)</li></ul></li><li><span data-type="index-term">sources for datasets</span>, <a data-type="index:locator" href="#idm45831167792952">Data Prep with Python</a></li><li><span data-type="index-term">tracked by Kubeflow</span>, <a data-type="index:locator" href="#idm45831180131592">Kubeflow Pipelines</a></li><li><span data-type="index-term">validation via TensorFlow Extended</span>, <a data-type="index:locator" href="#ch05-val">Keeping your data quality: TensorFlow data validation</a>-<a data-type="index:locator" href="#idm45831176610264">Keeping your data quality: TensorFlow data validation</a></li></ul></li><li><span data-type="index-term">data cleaning</span><ul><li><span data-type="index-term">about CT scan data</span>, <a data-type="index:locator" href="#idm45831185479048">CT Scans</a>, <a data-type="index:locator" href="#idm45831167813336">The Denoising CT Scans Example</a></li><li><span data-type="index-term">Apache Spark</span>, <a data-type="index:locator" href="#ch09-dns4">The CT Scan Denoising Pipeline</a>-<a data-type="index:locator" href="#idm45831165191208">The pipeline</a></li><li><span data-type="index-term">denoising pipeline</span>, <a data-type="index:locator" href="#ch09-dns2">The CT Scan Denoising Pipeline</a>-<a data-type="index:locator" href="#idm45831165190536">The pipeline</a></li><li><span data-type="index-term">open source method</span>, <a data-type="index:locator" href="#idm45831167821688">Case Study Using Multiple Tools</a>, <a data-type="index:locator" href="#idm45831167806264">The Denoising CT Scans Example</a></li></ul></li><li><span data-type="index-term">data parallelism distributed training</span>, <a data-type="index:locator" href="#idm45831172041928">Distributed Training</a></li><li><span data-type="index-term">data plane of KFServing</span>, <a data-type="index:locator" href="#ch08-inf3">Data Plane</a>-<a data-type="index:locator" href="#idm45831168933368">Data Plane</a>, <a data-type="index:locator" href="#idm45831167917192">Model serving</a></li><li><span data-type="index-term">data preparation</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831177515864">Data and Feature Preparation</a></li><li><span data-type="index-term">AutoML for</span>, <a data-type="index:locator" href="#idm45831165185720">AutoML: An Overview</a></li><li><span data-type="index-term">CT scan data denoised</span>, <a data-type="index:locator" href="#idm45831167795512">Data Prep with Python</a></li><li><span data-type="index-term">distributed</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831177060376">Distributed Tooling</a></li><li><span data-type="index-term">Apache Spark feature preparation</span>, <a data-type="index:locator" href="#idm45831175395864">Distributed Feature Preparation Using Apache Spark</a></li><li><span data-type="index-term">Apache Spark for</span>, <a data-type="index:locator" href="#ch05-sprk">Spark operators in Kubeflow</a>-<a data-type="index:locator" href="#idm45831175255144">Distributed Feature Preparation Using Apache Spark</a></li><li><span data-type="index-term">Apache Spark setup</span>, <a data-type="index:locator" href="#ch05-spse">Distributed Data Using Apache Spark</a>-<a data-type="index:locator" href="#idm45831175814552">Spark operators in Kubeflow</a></li><li><span data-type="index-term">data validation</span>, <a data-type="index:locator" href="#ch05-val2">Keeping your data quality: TensorFlow data validation</a>-<a data-type="index:locator" href="#idm45831176609592">Keeping your data quality: TensorFlow data validation</a>, <a data-type="index:locator" href="#ch05-dvsp">Spark operators in Kubeflow</a>-<a data-type="index:locator" href="#idm45831175254536">Distributed Feature Preparation Using Apache Spark</a></li><li><span data-type="index-term">missing data</span>, <a data-type="index:locator" href="#idm45831175664120">Handling missing fields</a></li><li><span data-type="index-term">rejected records check</span>, <a data-type="index:locator" href="#idm45831176611768">Keeping your data quality: TensorFlow data validation</a></li></ul></li><li><span data-type="index-term">feature preparation</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831177506920">Data and Feature Preparation</a>, <a data-type="index:locator" href="#idm45831177085176">Feature Preparation</a></li><li><span data-type="index-term">Apache Spark</span>, <a data-type="index:locator" href="#idm45831175394680">Distributed Feature Preparation Using Apache Spark</a></li><li><span data-type="index-term">AutoML for</span>, <a data-type="index:locator" href="#idm45831164735912">AutoML: An Overview</a></li><li><span data-type="index-term">data formatting and</span>, <a data-type="index:locator" href="#idm45831177089560">Formatting the Data</a></li><li><span data-type="index-term">recommendation system</span>, <a data-type="index:locator" href="#idm45831173124952">Starting a New Notebook Session</a></li><li><span data-type="index-term">TensorFlow Transform</span>, <a data-type="index:locator" href="#ch05-tft2">TensorFlow Transform, with TensorFlow Extended on Beam</a>-<a data-type="index:locator" href="#idm45831176533784">TensorFlow Transform, with TensorFlow Extended on Beam</a></li></ul></li><li><span data-type="index-term">local</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831177489416">Local Data and Feature Preparation</a></li><li><span data-type="index-term">custom containers</span>, <a data-type="index:locator" href="#idm45831177070472">Custom Containers</a></li><li><span data-type="index-term">fetching the data</span>, <a data-type="index:locator" href="#idm45831177483480">Fetching the Data</a></li><li><span data-type="index-term">filtering out junk</span>, <a data-type="index:locator" href="#idm45831177357160">Data Cleaning: Filtering Out the Junk</a></li><li><span data-type="index-term">formatting the data</span>, <a data-type="index:locator" href="#idm45831177090808">Formatting the Data</a></li><li><span data-type="index-term">missing data</span>, <a data-type="index:locator" href="#idm45831177348376">Data Cleaning: Filtering Out the Junk</a></li><li><span data-type="index-term">SpamAssassin package</span>, <a data-type="index:locator" href="#idm45831177188824">Data Cleaning: Filtering Out the Junk</a></li></ul></li><li><span data-type="index-term">missing data</span><ul><li><span data-type="index-term">distributed platform</span>, <a data-type="index:locator" href="#idm45831175662872">Handling missing fields</a></li><li><span data-type="index-term">local</span>, <a data-type="index:locator" href="#idm45831177196952">Data Cleaning: Filtering Out the Junk</a></li><li><span data-type="index-term">Scikit-learn and</span>, <a data-type="index:locator" href="#idm45831171815720">Data Preparation</a></li></ul></li><li><span data-type="index-term">putting together into a pipeline</span>, <a data-type="index:locator" href="#ch05-tog">Putting It Together in a Pipeline</a>-<a data-type="index:locator" href="#idm45831174890072">Using an Entire Notebook as a Data Preparation 
Pipeline Stage</a><ul><li><span data-type="index-term">entire notebook as pipeline stage</span>, <a data-type="index:locator" href="#idm45831174909928">Using an Entire Notebook as a Data Preparation 
Pipeline Stage</a></li></ul></li><li><span data-type="index-term">Random Forest algorithm</span>, <a data-type="index:locator" href="#ch07-sk">Data Preparation</a>-<a data-type="index:locator" href="#idm45831171780328">Data Preparation</a></li><li><span data-type="index-term">Scikit-learn and missing data</span>, <a data-type="index:locator" href="#idm45831171888328">Data Preparation</a></li><li><span data-type="index-term">tools online resource</span>, <a data-type="index:locator" href="#idm45831177497576">Deciding on the Correct Tooling</a></li><li><span data-type="index-term">tools, local versus distributed</span>, <a data-type="index:locator" href="#idm45831177495064">Deciding on the Correct Tooling</a></li><li><span data-type="index-term">US Census dataset</span>, <a data-type="index:locator" href="#ch07-sksk">Data Preparation</a>-<a data-type="index:locator" href="#idm45831171524216">Data Preparation</a></li></ul></li><li><span data-type="index-term">Databricks MLflow</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831189897464">MLflow (Databricks)</a>, <a data-type="index:locator" href="#idm45831174777640">Artifact and Metadata Store</a></li><li><span data-type="index-term">metadata tools</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831174084920">Using MLflow’s Metadata Tools with Kubeflow</a></li><li><span data-type="index-term">logging data on runs</span>, <a data-type="index:locator" href="#idm45831173889160">Logging Data on Runs</a></li><li><span data-type="index-term">MLflow Tracking</span>, <a data-type="index:locator" href="#idm45831174041144">Using MLflow’s Metadata Tools with Kubeflow</a></li><li><span data-type="index-term">MLflow Tracking Server</span>, <a data-type="index:locator" href="#ch06-trk2">Using MLflow’s Metadata Tools with Kubeflow</a>-<a data-type="index:locator" href="#idm45831173894264">Creating and Deploying an MLflow Tracking Server</a></li><li><span data-type="index-term">UI</span>, <a data-type="index:locator" href="#idm45831173270456">Using the MLflow UI</a></li></ul></li></ul></li><li><span data-type="index-term">datasets</span> (<span data-gentext="see">see</span> data)</li><li><span data-type="index-term">debugging</span><ul><li><span data-type="index-term">KFServing</span><ul><li><span data-type="index-term">InferenceService</span>, <a data-type="index:locator" href="#idm45831168084776">Debugging an InferenceService</a></li><li><span data-type="index-term">performance</span>, <a data-type="index:locator" href="#idm45831168066472">Debugging performance</a></li></ul></li><li><span data-type="index-term">TFJob deployment</span>, <a data-type="index:locator" href="#idm45831172054808">Deploying a TensorFlow Training Job</a></li></ul></li><li><span data-type="index-term">deep learning</span>, <a data-type="index:locator" href="#idm45831173235656">Building a Recommender with TensorFlow</a><ul><li><span data-type="index-term">sharing a pipeline</span>, <a data-type="index:locator" href="#idm45831165585848">Sharing the Pipeline</a></li></ul></li><li><span data-type="index-term">denoising data</span><ul><li><span data-type="index-term">about CT scan data</span>, <a data-type="index:locator" href="#idm45831185479992">CT Scans</a>, <a data-type="index:locator" href="#idm45831167829256">Case Study Using Multiple Tools</a>, <a data-type="index:locator" href="#idm45831167814312">The Denoising CT Scans Example</a></li><li><span data-type="index-term">Apache Spark</span>, <a data-type="index:locator" href="#idm45831175586920">Filtering out bad data</a>, <a data-type="index:locator" href="#ch09-dnz4">The CT Scan Denoising Pipeline</a>-<a data-type="index:locator" href="#idm45831165405320">The pipeline</a></li><li><span data-type="index-term">CT scan case study</span>, <a data-type="index:locator" href="#ch09-ct8">Case Study Using Multiple Tools</a>-<a data-type="index:locator" href="#idm45831165399224">Sharing the Pipeline</a></li><li><span data-type="index-term">data preparation</span>, <a data-type="index:locator" href="#idm45831167797512">Data Prep with Python</a></li><li><span data-type="index-term">decomposing CT scan</span>, <a data-type="index:locator" href="#idm45831167196808">DS-SVD with Apache Spark</a></li><li><span data-type="index-term">denoising pipeline</span>, <a data-type="index:locator" href="#ch09-dnz2">The CT Scan Denoising Pipeline</a>-<a data-type="index:locator" href="#idm45831165406664">The pipeline</a></li><li><span data-type="index-term">open source method</span>, <a data-type="index:locator" href="#idm45831167822632">Case Study Using Multiple Tools</a>, <a data-type="index:locator" href="#idm45831167807208">The Denoising CT Scans Example</a></li><li><span data-type="index-term">resource on math</span>, <a data-type="index:locator" href="#idm45831167803624">The Denoising CT Scans Example</a></li><li><span data-type="index-term">sharing the pipeline</span>, <a data-type="index:locator" href="#idm45831165162264">Sharing the Pipeline</a></li><li><span data-type="index-term">singular value decomposition</span>, <a data-type="index:locator" href="#idm45831167824280">Case Study Using Multiple Tools</a>, <a data-type="index:locator" href="#idm45831167237032">DS-SVD with Apache Spark</a></li><li><span data-type="index-term">visualizing denoised DICOMs</span>, <a data-type="index:locator" href="#ch09-viz">Visualization</a>-<a data-type="index:locator" href="#idm45831165632440">Recomposing the matrix into denoised images</a></li></ul></li><li><span data-type="index-term">deployment of Kubeflow</span><ul><li><span data-type="index-term">click-to-deploy on Google Cloud</span>, <a data-type="index:locator" href="#idm45831185449016">Getting Set Up with Kubeflow</a></li><li><span data-type="index-term">model serving options</span>, <a data-type="index:locator" href="#idm45831180086280">Model Inference</a></li><li><span data-type="index-term">namespace</span>, <a data-type="index:locator" href="#idm45831180713864">Creating Our First Kubeflow Project</a></li></ul></li><li><span data-type="index-term">DICOM file format</span>, <a data-type="index:locator" href="#idm45831167794568">Data Prep with Python</a><ul><li><span data-type="index-term">sources for datasets</span>, <a data-type="index:locator" href="#idm45831167790888">Data Prep with Python</a></li></ul></li><li><span data-type="index-term">Differentiable Architecture Search (DARTS)</span>, <a data-type="index:locator" href="#idm45831164173448">Neural Architecture Search</a></li><li><span data-type="index-term">disk space for Minikube</span>, <a data-type="index:locator" href="#idm45831180973320">Minikube</a></li><li><span data-type="index-term">display_schema</span>, <a data-type="index:locator" href="#idm45831176793480">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">distributed stochastic singular value decomposition (DS-SVD)</span>, <a data-type="index:locator" href="#idm45831167819800">Case Study Using Multiple Tools</a>, <a data-type="index:locator" href="#idm45831167723368">DS-SVD with Apache Spark</a></li><li><span data-type="index-term">distributed training</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831172045128">Distributed Training</a></li></ul></li><li><span data-type="index-term">Docker</span><ul><li><span data-type="index-term">Apache Spark on Jupyter notebooks</span>, <a data-type="index:locator" href="#idm45831176517048">Distributed Data Using Apache Spark</a></li><li><span data-type="index-term">Argo Workflows executor</span>, <a data-type="index:locator" href="#idm45831178826808">Argo: the Foundation of Pipelines</a>, <a data-type="index:locator" href="#idm45831164112264">Argo Executor Configurations and Trade-Offs</a></li><li><span data-type="index-term">container registry</span>, <a data-type="index:locator" href="#idm45831180916280">Setting up Docker</a></li><li><span data-type="index-term">deploying recommender training code</span>, <a data-type="index:locator" href="#idm45831172133448">Deploying a TensorFlow Training Job</a></li><li><span data-type="index-term">entire notebook as pipeline stage</span>, <a data-type="index:locator" href="#idm45831174905176">Using an Entire Notebook as a Data Preparation 
Pipeline Stage</a></li><li><span data-type="index-term">installing</span>, <a data-type="index:locator" href="#idm45831180946472">Setting up Docker</a></li><li><span data-type="index-term">installing Minikube</span>, <a data-type="index:locator" href="#idm45831180989560">Minikube</a></li><li><span data-type="index-term">parameters passed by value</span>, <a data-type="index:locator" href="#idm45831178469928">Building a Pipeline Using Existing Images</a></li><li><span data-type="index-term">prebuilt Docker images</span>, <a data-type="index:locator" href="#ch04-doc">Building a Pipeline Using Existing Images</a>-<a data-type="index:locator" href="#idm45831178342664">Building a Pipeline Using Existing Images</a></li><li><span data-type="index-term">Seldon Core local testing</span>, <a data-type="index:locator" href="#idm45831169886328">Local testing with Docker</a></li></ul></li></ul></div><div data-type="indexdiv"><h3>E</h3><ul><li><span data-type="index-term">Efficient Neural Architecture Search (ENAS)</span>, <a data-type="index:locator" href="#idm45831164172840">Neural Architecture Search</a></li><li><span data-type="index-term">EMR native Spark operator</span>, <a data-type="index:locator" href="#ch05-emr2">Spark operators in Kubeflow</a>-<a data-type="index:locator" href="#idm45831175815896">Spark operators in Kubeflow</a></li><li><span data-type="index-term">environment variables for pipelines</span>, <a data-type="index:locator" href="#idm45831178504232">Building a Pipeline Using Existing Images</a></li><li><span data-type="index-term">events via Knative Eventing</span><ul><li><span data-type="index-term">KafkaSource to send events</span>, <a data-type="index:locator" href="#idm45831167982632">Knative Eventing</a></li><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#idm45831180064792">Model Inference</a>, <a data-type="index:locator" href="#idm45831168017480">Knative Eventing</a></li><li><span data-type="index-term">online documentation</span>, <a data-type="index:locator" href="#idm45831168016568">Knative Eventing</a></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169068472">Outlier and drift detection</a></li></ul></li><li><span data-type="index-term">example generators in TensorFlow Extended</span>, <a data-type="index:locator" href="#idm45831176829976">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">executors for Argo Workflows</span>, <a data-type="index:locator" href="#idm45831178827496">Argo: the Foundation of Pipelines</a>, <a data-type="index:locator" href="#idm45831164113720">Argo Executor Configurations and Trade-Offs</a></li><li><span data-type="index-term">experiments</span>, <a data-type="index:locator" href="#idm45831179694984">Exploring the Prepackaged Sample Pipelines</a>, <a data-type="index:locator" href="#idm45831179312920">Building a Simple Pipeline in Python</a>, <a data-type="index:locator" href="#idm45831178488616">Building a Pipeline Using Existing Images</a><ul><li><span data-type="index-term">Kubeflow Katib</span>, <a data-type="index:locator" href="#idm45831180112520">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164656744">Katib Concepts</a><ul><li><span data-type="index-term">configuring</span>, <a data-type="index:locator" href="#idm45831164374248">Configuring an Experiment</a></li></ul></li><li><span data-type="index-term">reproducibility by sharing pipeline</span>, <a data-type="index:locator" href="#idm45831165161320">Sharing the Pipeline</a></li></ul></li><li><span data-type="index-term">explaining the model</span><ul><li><span data-type="index-term">Scikit-learn</span>, <a data-type="index:locator" href="#ch07-exp2">Scikit-Learn Training</a>-<a data-type="index:locator" href="#idm45831170916376">Explaining the Model</a></li></ul></li><li><span data-type="index-term">explaining the model, importance of</span>, <a data-type="index:locator" href="#idm45831171413096">Scikit-Learn Training</a>, <a data-type="index:locator" href="#idm45831170738680">Model Accuracy, Drift, and Explainability</a></li></ul></div><div data-type="indexdiv"><h3>F</h3><ul><li><span data-type="index-term">failover</span>, <a data-type="index:locator" href="#idm45831180202920">Training Operators</a></li><li><span data-type="index-term">feature preparation</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831177505832">Data and Feature Preparation</a>, <a data-type="index:locator" href="#idm45831177083928">Feature Preparation</a></li><li><span data-type="index-term">Apache Spark</span>, <a data-type="index:locator" href="#idm45831175393464">Distributed Feature Preparation Using Apache Spark</a></li><li><span data-type="index-term">AutoML for</span>, <a data-type="index:locator" href="#idm45831165183128">AutoML: An Overview</a></li><li><span data-type="index-term">data formatting and</span>, <a data-type="index:locator" href="#idm45831177088344">Formatting the Data</a></li><li><span data-type="index-term">recommendation system</span>, <a data-type="index:locator" href="#idm45831173125928">Starting a New Notebook Session</a></li><li><span data-type="index-term">TensorFlow Transform</span>, <a data-type="index:locator" href="#ch05-tft">TensorFlow Transform, with TensorFlow Extended on Beam</a>-<a data-type="index:locator" href="#idm45831176534280">TensorFlow Transform, with TensorFlow Extended on Beam</a></li></ul></li><li><span data-type="index-term">file-fetching component</span>, <a data-type="index:locator" href="#idm45831178070584">Kubeflow Pipeline Components</a></li><li><span data-type="index-term">filesystem/get_file component</span>, <a data-type="index:locator" href="#idm45831176833864">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">file_output mechanism</span>, <a data-type="index:locator" href="#idm45831178935144">Storing Data Between Steps</a>, <a data-type="index:locator" href="#idm45831176945832">Keeping your data quality: TensorFlow data validation</a></li></ul></div><div data-type="indexdiv"><h3>G</h3><ul><li><span data-type="index-term">getting started</span><ul><li><span data-type="index-term">getting started guide</span>, <a data-type="index:locator" href="#idm45831180271576">Going Beyond a Local Deployment</a></li><li><span data-type="index-term">installing Kubeflow</span>, <a data-type="index:locator" href="#ch02-inst">Getting Set Up with Kubeflow</a>-<a data-type="index:locator" href="#idm45831190037640">Installing Kubeflow and Its Dependencies</a>, <a data-type="index:locator" href="#idm45831180721032">Creating Our First Kubeflow Project</a>, <a data-type="index:locator" href="#idm45831180272936">Going Beyond a Local Deployment</a><ul><li><span data-type="index-term">first project</span>, <a data-type="index:locator" href="#ch02-frst3">Creating Our First Kubeflow Project</a>-<a data-type="index:locator" href="#idm45831180283880">Test Query</a></li></ul></li><li><span data-type="index-term">installing Kubeflow Katib</span>, <a data-type="index:locator" href="#idm45831164497080">Installing Katib</a><ul><li><span data-type="index-term">first experiment</span>, <a data-type="index:locator" href="#ch10-fstexx">Running Your First Katib Experiment</a>-<a data-type="index:locator" href="#idm45831164290248">Running the Experiment</a></li></ul></li><li><span data-type="index-term">machine learning</span>, <a data-type="index:locator" href="#pref-rsrc2">Our Assumption About You</a>-<a data-type="index:locator" href="#idm45831190137704">Our Assumption About You</a></li></ul></li><li><span data-type="index-term">Google BigQuery example generators</span>, <a data-type="index:locator" href="#idm45831176827960">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">Google Cloud Platform (GCP)</span><ul><li><span data-type="index-term">click-to-deploy Kubeflow app</span>, <a data-type="index:locator" href="#idm45831185451064">Getting Set Up with Kubeflow</a></li><li><span data-type="index-term">Google-specific components</span>, <a data-type="index:locator" href="#idm45831164089384">Google Cloud</a></li><li><span data-type="index-term">TPU-accelerated instances</span>, <a data-type="index:locator" href="#idm45831164085672">TPU-Accelerated Instances</a></li></ul></li><li><span data-type="index-term">Google codelabs</span>, <a data-type="index:locator" href="#idm45831185467256">Conclusion</a></li><li><span data-type="index-term">Google Dataflow</span><ul><li><span data-type="index-term">Apache Beam for</span>, <a data-type="index:locator" href="#idm45831177056504">Distributed Tooling</a></li><li><span data-type="index-term">TensorFlow Extended configured for</span>, <a data-type="index:locator" href="#idm45831164079544">Dataflow for TFX</a></li></ul></li><li><span data-type="index-term">Google Dataproc for Apache Spark</span>, <a data-type="index:locator" href="#ch05-emr3">Spark operators in Kubeflow</a>-<a data-type="index:locator" href="#idm45831175815224">Spark operators in Kubeflow</a></li><li><span data-type="index-term">Google Kubernetes Engine (GKE)</span>, <a data-type="index:locator" href="#idm45831179713816">Exploring the Prepackaged Sample Pipelines</a></li><li><span data-type="index-term">Google Vizier</span>, <a data-type="index:locator" href="#idm45831180122584">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164952008">Hyperparameter Tuning with Kubeflow Katib</a></li><li><span data-type="index-term">GPUs</span><ul><li><span data-type="index-term">autoscaling in KFServing</span>, <a data-type="index:locator" href="#idm45831168986008">KFServing</a>, <a data-type="index:locator" href="#idm45831168214312">Going layer by layer</a>, <a data-type="index:locator" href="#idm45831167914152">Model serving</a></li><li><span data-type="index-term">autoscaling lacking in Seldon Core</span>, <a data-type="index:locator" href="#idm45831169027352">Model serving</a></li><li><span data-type="index-term">resource marking in code</span>, <a data-type="index:locator" href="#idm45831179600824">Building a Simple Pipeline in Python</a>, <a data-type="index:locator" href="#idm45831174892536">Using an Entire Notebook as a Data Preparation 
Pipeline Stage</a></li><li><span data-type="index-term">training using</span>, <a data-type="index:locator" href="#idm45831171944216">Using GPUs</a></li></ul></li><li><span data-type="index-term">grid search in Katib</span>, <a data-type="index:locator" href="#idm45831164524840">Katib Concepts</a></li></ul></div><div data-type="indexdiv"><h3>H</h3><ul><li><span data-type="index-term">handwriting recognition via RandomForestClassifier</span>, <a data-type="index:locator" href="#ch02-rand2">Creating Our First Kubeflow Project</a>-<a data-type="index:locator" href="#idm45831180281864">Test Query</a></li><li><span data-type="index-term">hello world project</span>, <a data-type="index:locator" href="#ch02-hllo">Creating Our First Kubeflow Project</a>-<a data-type="index:locator" href="#idm45831180281192">Test Query</a></li><li><span data-type="index-term">hyperparameters</span><ul><li><span data-type="index-term">AutoML for tuning</span>, <a data-type="index:locator" href="#idm45831164671064">AutoML: An Overview</a></li><li><span data-type="index-term">definition</span>, <a data-type="index:locator" href="#idm45831189938760">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164843640">Hyperparameter Tuning with Kubeflow Katib</a></li><li><span data-type="index-term">Kubeflow Katib for tuning</span>, <a data-type="index:locator" href="#ch03-cpy2">Hyperparameter Tuning</a>-<a data-type="index:locator" href="#idm45831180092856">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164846760">Hyperparameter Tuning with Kubeflow Katib</a>, <a data-type="index:locator" href="#idm45831164139768">Advantages of Katib over Other Frameworks</a><ul><li>(<span data-gentext="see">see also</span> Kubeflow Katib)</li></ul></li><li><span data-type="index-term">neural architecture search</span>, <a data-type="index:locator" href="#idm45831164905512">AutoML: An Overview</a></li><li><span data-type="index-term">TensorFlow recommender</span>, <a data-type="index:locator" href="#idm45831172748872">TensorFlow Training</a></li><li><span data-type="index-term">tuning supported by Kubeflow</span>, <a data-type="index:locator" href="#idm45831189937816">Hyperparameter Tuning</a></li></ul></li></ul></div><div data-type="indexdiv"><h3>I</h3><ul><li><span data-type="index-term">income predictor model</span>, <a data-type="index:locator" href="#ch08-incp">US Census income predictor model example</a>-<a data-type="index:locator" href="#idm45831169086808">US Census income predictor model example</a></li><li><span data-type="index-term">Istio</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831179855816">Istio</a>, <a data-type="index:locator" href="#idm45831168235880">Going layer by layer</a></li><li><span data-type="index-term">KFServing</span><ul><li><span data-type="index-term">infrastructure</span>, <a data-type="index:locator" href="#idm45831168237336">Going layer by layer</a></li></ul></li><li><span data-type="index-term">model inference</span>, <a data-type="index:locator" href="#idm45831180076040">Model Inference</a></li></ul></li></ul></div><div data-type="indexdiv"><h3>J</h3><ul><li><span data-type="index-term">Jupyter notebooks</span><ul><li><span data-type="index-term">adding system software</span>, <a data-type="index:locator" href="#idm45831177189768">Data Cleaning: Filtering Out the Junk</a></li><li><span data-type="index-term">Apache Spark via Dockerfile</span>, <a data-type="index:locator" href="#idm45831176517960">Distributed Data Using Apache Spark</a></li><li><span data-type="index-term">data and feature preparation</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831177488168">Local Data and Feature Preparation</a></li><li><span data-type="index-term">adding system software</span>, <a data-type="index:locator" href="#idm45831177190952">Data Cleaning: Filtering Out the Junk</a></li><li><span data-type="index-term">entire notebook as pipeline stage</span>, <a data-type="index:locator" href="#idm45831174908648">Using an Entire Notebook as a Data Preparation 
Pipeline Stage</a></li></ul></li><li><span data-type="index-term">GPU resources</span>, <a data-type="index:locator" href="#idm45831179598088">Building a Simple Pipeline in Python</a>, <a data-type="index:locator" href="#idm45831174938008">Using an Entire Notebook as a Data Preparation 
Pipeline Stage</a></li><li><span data-type="index-term">JupyterHub</span>, <a data-type="index:locator" href="#idm45831180242952">Notebooks (JupyterHub)</a>, <a data-type="index:locator" href="#idm45831180217112">Training Operators</a>, <a data-type="index:locator" href="#idm45831174907272">Using an Entire Notebook as a Data Preparation 
Pipeline Stage</a></li><li><span data-type="index-term">kubectl for Kubernetes management</span>, <a data-type="index:locator" href="#idm45831180238296">Notebooks (JupyterHub)</a></li><li><span data-type="index-term">Kubeflow component support via</span>, <a data-type="index:locator" href="#idm45831180222152">Notebooks (JupyterHub)</a></li><li><span data-type="index-term">Kubeflow support for</span>, <a data-type="index:locator" href="#idm45831190343768">Data Exploration with Notebooks</a>, <a data-type="index:locator" href="#idm45831180967624">Setting Up Your Kubeflow Development Environment</a></li><li><span data-type="index-term">multiuser isolation</span>, <a data-type="index:locator" href="#idm45831179755272">Kubeflow Multiuser Isolation</a></li><li><span data-type="index-term">Scikit-learn notebook setup</span>, <a data-type="index:locator" href="#ch07-sci">Training a Model Using Scikit-Learn</a>-<a data-type="index:locator" href="#idm45831171910760">Training a Model Using Scikit-Learn</a></li><li><span data-type="index-term">TensorFlow recommender notebook setup</span>, <a data-type="index:locator" href="#ch07-nb4">Building a Recommender with TensorFlow</a>-<a data-type="index:locator" href="#idm45831173113352">Starting a New Notebook Session</a></li></ul></li></ul></div><div data-type="indexdiv"><h3>K</h3><ul><li><span data-type="index-term">KafkaSource to send Knative events</span>, <a data-type="index:locator" href="#idm45831167985512">Knative Eventing</a></li><li><span data-type="index-term">Katib (Kubeflow)</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831180124504">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164844584">Hyperparameter Tuning with Kubeflow Katib</a>, <a data-type="index:locator" href="#idm45831164502744">Katib Concepts</a></li><li><span data-type="index-term">about katib meaning</span>, <a data-type="index:locator" href="#idm45831164953928">Hyperparameter Tuning with Kubeflow Katib</a></li><li><span data-type="index-term">advantages of</span>, <a data-type="index:locator" href="#idm45831164140616">Advantages of Katib over Other Frameworks</a></li><li><span data-type="index-term">distributed training jobs</span>, <a data-type="index:locator" href="#idm45831164222680">Tuning Distributed Training Jobs</a></li><li><span data-type="index-term">experiments</span>, <a data-type="index:locator" href="#idm45831180114440">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164655768">Katib Concepts</a><ul><li><span data-type="index-term">configuring</span>, <a data-type="index:locator" href="#idm45831164375464">Configuring an Experiment</a></li><li><span data-type="index-term">first experiment</span>, <a data-type="index:locator" href="#ch10-fstex">Running Your First Katib Experiment</a>-<a data-type="index:locator" href="#idm45831164291592">Running the Experiment</a></li></ul></li><li><span data-type="index-term">first experiment</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831164487080">Running Your First Katib Experiment</a></li><li><span data-type="index-term">configuring experiment</span>, <a data-type="index:locator" href="#idm45831164377928">Configuring an Experiment</a></li><li><span data-type="index-term">prepping training code</span>, <a data-type="index:locator" href="#idm45831164459192">Prepping Your Training Code</a></li><li><span data-type="index-term">running</span>, <a data-type="index:locator" href="#ch10-fexr">Running the Experiment</a>-<a data-type="index:locator" href="#idm45831164289576">Running the Experiment</a></li></ul></li><li><span data-type="index-term">installing</span>, <a data-type="index:locator" href="#idm45831164499000">Installing Katib</a></li><li><span data-type="index-term">jobs</span>, <a data-type="index:locator" href="#idm45831180103624">Hyperparameter Tuning</a></li><li><span data-type="index-term">metrics collector</span>, <a data-type="index:locator" href="#idm45831164506136">Katib Concepts</a></li><li><span data-type="index-term">neural architecture search support</span>, <a data-type="index:locator" href="#idm45831164176840">Neural Architecture Search</a><ul><li><span data-type="index-term">about NAS</span>, <a data-type="index:locator" href="#idm45831164186152">Neural Architecture Search</a></li><li><span data-type="index-term">example DARTS experiment</span>, <a data-type="index:locator" href="#ch10-mm">Neural Architecture Search</a>-<a data-type="index:locator" href="#idm45831164144904">Neural Architecture Search</a></li><li><span data-type="index-term">model manager</span>, <a data-type="index:locator" href="#idm45831164162808">Neural Architecture Search</a></li></ul></li><li><span data-type="index-term">search algorithms</span><ul><li><span data-type="index-term">Bayesian optimization</span>, <a data-type="index:locator" href="#idm45831164514728">Katib Concepts</a></li><li><span data-type="index-term">grid search</span>, <a data-type="index:locator" href="#idm45831165193976">Katib Concepts</a></li><li><span data-type="index-term">hyperbrand</span>, <a data-type="index:locator" href="#idm45831164511528">Katib Concepts</a></li><li><span data-type="index-term">random search</span>, <a data-type="index:locator" href="#idm45831164522536">Katib Concepts</a></li></ul></li><li><span data-type="index-term">suggestions</span>, <a data-type="index:locator" href="#idm45831180101400">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164649448">Katib Concepts</a></li><li><span data-type="index-term">trials</span>, <a data-type="index:locator" href="#idm45831180108152">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164645272">Katib Concepts</a></li><li><span data-type="index-term">UI</span>, <a data-type="index:locator" href="#idm45831164247928">Katib User Interface</a></li></ul></li><li><span data-type="index-term">Keras API</span>, <a data-type="index:locator" href="#idm45831173222520">Building a Recommender with TensorFlow</a></li><li><span data-type="index-term">kfctl repository</span>, <a data-type="index:locator" href="#idm45831179744520">Kubeflow Multiuser Isolation</a></li><li><span data-type="index-term">KFServing</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831168998696">KFServing</a>, <a data-type="index:locator" href="#idm45831167925656">Review</a></li><li><span data-type="index-term">API documentation</span>, <a data-type="index:locator" href="#idm45831167931640">API documentation</a>, <a data-type="index:locator" href="#idm45831167930664">API documentation</a></li><li><span data-type="index-term">capabilities of</span>, <a data-type="index:locator" href="#idm45831167936296">Additional features</a></li><li><span data-type="index-term">comparison chart</span>, <a data-type="index:locator" href="#idm45831170659160">Model Inference in Kubeflow</a></li><li><span data-type="index-term">curl 404 Not Found</span>, <a data-type="index:locator" href="#idm45831168352840">Recommender example</a></li><li><span data-type="index-term">data plane</span>, <a data-type="index:locator" href="#ch08-inf2">Data Plane</a>-<a data-type="index:locator" href="#idm45831168934040">Data Plane</a><ul><li><span data-type="index-term">component in</span>, <a data-type="index:locator" href="#idm45831168951384">Data Plane</a></li><li><span data-type="index-term">endpoint in</span>, <a data-type="index:locator" href="#idm45831168960520">Data Plane</a></li><li><span data-type="index-term">explainer in</span>, <a data-type="index:locator" href="#idm45831168944632">Data Plane</a></li><li><span data-type="index-term">prediction protocol in</span>, <a data-type="index:locator" href="#idm45831168938008">Data Plane</a></li><li><span data-type="index-term">predictor in</span>, <a data-type="index:locator" href="#idm45831168947368">Data Plane</a></li><li><span data-type="index-term">transformer in</span>, <a data-type="index:locator" href="#idm45831168941560">Data Plane</a></li></ul></li><li><span data-type="index-term">debugging InferenceService</span>, <a data-type="index:locator" href="#idm45831168085736">Debugging an InferenceService</a></li><li><span data-type="index-term">debugging performance</span>, <a data-type="index:locator" href="#idm45831168067416">Debugging performance</a></li><li><span data-type="index-term">deployment strategies</span>, <a data-type="index:locator" href="#idm45831167893208">Model updating</a></li><li><span data-type="index-term">endpoints</span><ul><li><span data-type="index-term">blue-green deployment</span>, <a data-type="index:locator" href="#idm45831168956408">Data Plane</a></li></ul></li><li><span data-type="index-term">inference</span>, <a data-type="index:locator" href="#ch08-kfs2">KFServing</a>-<a data-type="index:locator" href="#idm45831167847656">Summary</a></li><li><span data-type="index-term">InferenceService</span><ul><li><span data-type="index-term">autoscaling via escape hatches</span>, <a data-type="index:locator" href="#ch08-ausch">Escape hatches</a>-<a data-type="index:locator" href="#idm45831168092968">Escape hatches</a></li><li><span data-type="index-term">debugging</span>, <a data-type="index:locator" href="#idm45831168089448">Debugging an InferenceService</a></li><li><span data-type="index-term">escape hatches</span>, <a data-type="index:locator" href="#ch08-esch">Escape hatches</a>-<a data-type="index:locator" href="#idm45831168051768">Escape hatches</a></li><li><span data-type="index-term">examples</span>, <a data-type="index:locator" href="#ch08-exkfs">Simplicity and extensibility</a>-<a data-type="index:locator" href="#idm45831168430552">Simplicity and extensibility</a>, <a data-type="index:locator" href="#idm45831168294200">Recommender example</a></li><li><span data-type="index-term">KafkaSource to send Knative events</span>, <a data-type="index:locator" href="#idm45831167984824">Knative Eventing</a></li><li><span data-type="index-term">namespace</span>, <a data-type="index:locator" href="#idm45831168517128">Recommender example</a></li><li><span data-type="index-term">recommender</span>, <a data-type="index:locator" href="#ch08-rex">Recommender example</a>-<a data-type="index:locator" href="#idm45831168280328">Recommender example</a></li></ul></li><li><span data-type="index-term">infrastructure stack</span>, <a data-type="index:locator" href="#idm45831168276040">Peeling Back the Underlying Infrastructure</a><ul><li><span data-type="index-term">debugging InferenceService</span>, <a data-type="index:locator" href="#idm45831168083560">Debugging an InferenceService</a></li><li><span data-type="index-term">debugging performance</span>, <a data-type="index:locator" href="#idm45831168065256">Debugging performance</a></li><li><span data-type="index-term">escape hatches</span>, <a data-type="index:locator" href="#ch08-esch3">Escape hatches</a>-<a data-type="index:locator" href="#idm45831168093608">Escape hatches</a></li><li><span data-type="index-term">Istio</span>, <a data-type="index:locator" href="#idm45831168238584">Going layer by layer</a></li><li><span data-type="index-term">Knative</span>, <a data-type="index:locator" href="#idm45831168970152">Data Plane</a>, <a data-type="index:locator" href="#idm45831168224856">Going layer by layer</a>, <a data-type="index:locator" href="#idm45831168222696">Going layer by layer</a>, <a data-type="index:locator" href="#idm45831168021128">Knative Eventing</a></li><li><span data-type="index-term">Knative Eventing</span>, <a data-type="index:locator" href="#idm45831180062664">Model Inference</a>, <a data-type="index:locator" href="#idm45831168057960">Knative Eventing</a></li><li><span data-type="index-term">Knative Serving</span>, <a data-type="index:locator" href="#idm45831168218728">Going layer by layer</a>, <a data-type="index:locator" href="#idm45831168024472">Knative Eventing</a>, <a data-type="index:locator" href="#idm45831167855320">Model updating</a></li><li><span data-type="index-term">Kubernetes</span>, <a data-type="index:locator" href="#idm45831168241944">Going layer by layer</a></li></ul></li><li><span data-type="index-term">model monitoring</span>, <a data-type="index:locator" href="#idm45831167911480">Model monitoring</a></li><li><span data-type="index-term">model serving</span>, <a data-type="index:locator" href="#idm45831167921544">Model serving</a></li><li><span data-type="index-term">model updating</span>, <a data-type="index:locator" href="#idm45831167899176">Model updating</a></li><li><span data-type="index-term">network monitoring and telemetry</span>, <a data-type="index:locator" href="#idm45831167903720">Model monitoring</a></li><li><span data-type="index-term">SDK documentation</span>, <a data-type="index:locator" href="#idm45831168295176">Recommender example</a></li><li><span data-type="index-term">serverless inferencing</span>, <a data-type="index:locator" href="#idm45831168991304">KFServing</a>, <a data-type="index:locator" href="#idm45831168290968">Recommender example</a>, <a data-type="index:locator" href="#idm45831168026360">Knative Eventing</a></li><li><span data-type="index-term">service plane</span>, <a data-type="index:locator" href="#idm45831168980008">Serverless and the Service Plane</a></li><li><span data-type="index-term">setting up</span>, <a data-type="index:locator" href="#ch08-kfinf">Setting up KFServing</a>-<a data-type="index:locator" href="#idm45831168894584">Setting up KFServing</a><ul><li><span data-type="index-term">namespaces and</span>, <a data-type="index:locator" href="#idm45831168905832">Setting up KFServing</a></li></ul></li><li><span data-type="index-term">troubleshooting guide online</span>, <a data-type="index:locator" href="#idm45831168893384">Setting up KFServing</a></li></ul></li><li><span data-type="index-term">Knative</span><ul><li><span data-type="index-term">architecture</span>, <a data-type="index:locator" href="#idm45831179777000">Knative</a></li><li><span data-type="index-term">components in Kubeflow</span>, <a data-type="index:locator" href="#idm45831179797208">Knative</a></li><li><span data-type="index-term">Eventing</span><ul><li><span data-type="index-term">KafkaSource to send events</span>, <a data-type="index:locator" href="#idm45831167986792">Knative Eventing</a></li><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#idm45831180063880">Model Inference</a>, <a data-type="index:locator" href="#idm45831168018696">Knative Eventing</a></li><li><span data-type="index-term">online documentation</span>, <a data-type="index:locator" href="#idm45831168019912">Knative Eventing</a></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169069688">Outlier and drift detection</a></li></ul></li><li><span data-type="index-term">KFServing infrastructure</span>, <a data-type="index:locator" href="#idm45831168971112">Data Plane</a>, <a data-type="index:locator" href="#idm45831168223608">Going layer by layer</a>, <a data-type="index:locator" href="#idm45831168022040">Knative Eventing</a></li><li><span data-type="index-term">Serving</span>, <a data-type="index:locator" href="#idm45831179799128">Knative</a>, <a data-type="index:locator" href="#idm45831168987864">KFServing</a><ul><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#idm45831168219816">Going layer by layer</a>, <a data-type="index:locator" href="#idm45831168023256">Knative Eventing</a>, <a data-type="index:locator" href="#idm45831167856648">Model updating</a></li></ul></li></ul></li><li><span data-type="index-term">kubectl</span><ul><li><span data-type="index-term">deployment of Kubeflow</span>, <a data-type="index:locator" href="#idm45831180717128">Creating Our First Kubeflow Project</a></li><li><span data-type="index-term">installing</span>, <a data-type="index:locator" href="#idm45831185440680">Installing Kubeflow and Its Dependencies</a></li><li><span data-type="index-term">Jupyter notebook incorporation</span>, <a data-type="index:locator" href="#idm45831180237256">Notebooks (JupyterHub)</a></li></ul></li><li><span data-type="index-term">Kubeflow</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831188261320">Kubeflow: What It Is and Who It Is For</a>, <a data-type="index:locator" href="#idm45831190365464">Why Kubernetes?</a></li><li><span data-type="index-term">alternatives to</span>, <a data-type="index:locator" href="#ch01-alt">Alternatives to Kubeflow</a>-<a data-type="index:locator" href="#idm45831189887880">Others</a>, <a data-type="index:locator" href="#idm45831177009272">TensorFlow Extended</a></li><li><span data-type="index-term">Apache 2 license</span>, <a data-type="index:locator" href="#idm45831189723368">Code Examples</a></li><li><span data-type="index-term">core components</span>, <a data-type="index:locator" href="#ch01-comp">Kubeflow’s Design and Core Components</a>-<a data-type="index:locator" href="#idm45831189913112">Component Overview</a><ul><li>(<span data-gentext="see">see also</span> components)</li></ul></li><li><span data-type="index-term">dataset tools</span>, <a data-type="index:locator" href="#idm45831190335048">Data/Feature Preparation</a></li><li><span data-type="index-term">first project</span>, <a data-type="index:locator" href="#ch02-frst">Creating Our First Kubeflow Project</a>-<a data-type="index:locator" href="#idm45831180284552">Test Query</a></li><li><span data-type="index-term">getting started guide</span>, <a data-type="index:locator" href="#idm45831180269688">Going Beyond a Local Deployment</a></li><li><span data-type="index-term">hyperparameter tuning</span>, <a data-type="index:locator" href="#idm45831189940008">Hyperparameter Tuning</a><ul><li>(<span data-gentext="see">see also</span> hyperparameters)</li></ul></li><li><span data-type="index-term">installing</span>, <a data-type="index:locator" href="#ch02-inst2">Getting Set Up with Kubeflow</a>-<a data-type="index:locator" href="#idm45831190037032">Installing Kubeflow and Its Dependencies</a>, <a data-type="index:locator" href="#idm45831180722008">Creating Our First Kubeflow Project</a>, <a data-type="index:locator" href="#idm45831180692392">Training and Monitoring Progress</a>, <a data-type="index:locator" href="#idm45831180273880">Going Beyond a Local Deployment</a></li><li><span data-type="index-term">installing development environment</span>, <a data-type="index:locator" href="#idm45831180969320">Setting Up Your Kubeflow Development Environment</a></li><li><span data-type="index-term">local installation</span>, <a data-type="index:locator" href="#idm45831181000024">Setting Up Local Kubernetes</a></li><li><span data-type="index-term">local to distributed with ease</span>, <a data-type="index:locator" href="#idm45831185456424">Getting Set Up with Kubeflow</a>, <a data-type="index:locator" href="#idm45831180275816">Going Beyond a Local Deployment</a></li><li><span data-type="index-term">online community</span>, <a data-type="index:locator" href="#idm45831185473752">Conclusion</a>, <a data-type="index:locator" href="#idm45831164121064">Conclusion</a></li><li><span data-type="index-term">overhead</span>, <a data-type="index:locator" href="#idm45831190374024">Why Containerize?</a></li><li><span data-type="index-term">training frameworks</span>, <a data-type="index:locator" href="#idm45831189955560">Training</a></li><li><span data-type="index-term">web UI installation</span>, <a data-type="index:locator" href="#idm45831180694248">Training and Monitoring Progress</a></li></ul></li><li><span data-type="index-term">Kubeflow Katib</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831180123528">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164845528">Hyperparameter Tuning with Kubeflow Katib</a>, <a data-type="index:locator" href="#idm45831164501768">Katib Concepts</a></li><li><span data-type="index-term">about katib meaning</span>, <a data-type="index:locator" href="#idm45831164952952">Hyperparameter Tuning with Kubeflow Katib</a></li><li><span data-type="index-term">advantages of</span>, <a data-type="index:locator" href="#idm45831164141464">Advantages of Katib over Other Frameworks</a></li><li><span data-type="index-term">distributed training jobs</span>, <a data-type="index:locator" href="#idm45831164223528">Tuning Distributed Training Jobs</a></li><li><span data-type="index-term">experiments</span>, <a data-type="index:locator" href="#idm45831180113464">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164654824">Katib Concepts</a><ul><li><span data-type="index-term">configuring</span>, <a data-type="index:locator" href="#idm45831164373032">Configuring an Experiment</a></li><li><span data-type="index-term">first experiment</span>, <a data-type="index:locator" href="#ch10-fstex2">Running Your First Katib Experiment</a>-<a data-type="index:locator" href="#idm45831164290920">Running the Experiment</a></li></ul></li><li><span data-type="index-term">first experiment</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831164464456">Running Your First Katib Experiment</a></li><li><span data-type="index-term">configuring experiment</span>, <a data-type="index:locator" href="#idm45831164376680">Configuring an Experiment</a></li><li><span data-type="index-term">prepping training code</span>, <a data-type="index:locator" href="#idm45831164457944">Prepping Your Training Code</a></li><li><span data-type="index-term">running</span>, <a data-type="index:locator" href="#ch10-fexr2">Running the Experiment</a>-<a data-type="index:locator" href="#idm45831164250968">Running the Experiment</a></li></ul></li><li><span data-type="index-term">installing</span>, <a data-type="index:locator" href="#idm45831164498024">Installing Katib</a></li><li><span data-type="index-term">jobs</span>, <a data-type="index:locator" href="#idm45831180104600">Hyperparameter Tuning</a></li><li><span data-type="index-term">metrics collector</span>, <a data-type="index:locator" href="#idm45831164505160">Katib Concepts</a></li><li><span data-type="index-term">neural architecture search support</span>, <a data-type="index:locator" href="#idm45831164175992">Neural Architecture Search</a><ul><li><span data-type="index-term">about NAS</span>, <a data-type="index:locator" href="#idm45831164185064">Neural Architecture Search</a></li><li><span data-type="index-term">example DARTS experiment</span>, <a data-type="index:locator" href="#ch10-mm2">Neural Architecture Search</a>-<a data-type="index:locator" href="#idm45831164144296">Neural Architecture Search</a></li><li><span data-type="index-term">model manager</span>, <a data-type="index:locator" href="#idm45831164163896">Neural Architecture Search</a></li></ul></li><li><span data-type="index-term">search algorithms</span><ul><li><span data-type="index-term">Bayesian optimization</span>, <a data-type="index:locator" href="#idm45831164515976">Katib Concepts</a></li><li><span data-type="index-term">grid search</span>, <a data-type="index:locator" href="#idm45831165192728">Katib Concepts</a></li><li><span data-type="index-term">hyperbrand</span>, <a data-type="index:locator" href="#idm45831164510280">Katib Concepts</a></li><li><span data-type="index-term">random search</span>, <a data-type="index:locator" href="#idm45831164521288">Katib Concepts</a></li></ul></li><li><span data-type="index-term">suggestions</span>, <a data-type="index:locator" href="#idm45831180100424">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164652232">Katib Concepts</a></li><li><span data-type="index-term">trials</span>, <a data-type="index:locator" href="#idm45831180109128">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164646248">Katib Concepts</a></li><li><span data-type="index-term">UI</span>, <a data-type="index:locator" href="#idm45831164247080">Katib User Interface</a></li></ul></li><li><span data-type="index-term">Kubeflow ML Metadata</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831174785560">Artifact and Metadata Store</a>, <a data-type="index:locator" href="#idm45831174771208">Kubeflow ML Metadata</a></li><li><span data-type="index-term">dataset tracking</span>, <a data-type="index:locator" href="#idm45831174634920">Kubeflow ML Metadata</a></li><li><span data-type="index-term">defining a workspace</span>, <a data-type="index:locator" href="#idm45831174755256">Kubeflow ML Metadata</a></li><li><span data-type="index-term">information about model and metrics</span>, <a data-type="index:locator" href="#idm45831174625192">Kubeflow ML Metadata</a></li><li><span data-type="index-term">information organization</span>, <a data-type="index:locator" href="#idm45831174744424">Kubeflow ML Metadata</a></li><li><span data-type="index-term">limitations of</span>, <a data-type="index:locator" href="#idm45831174089480">Kubeflow Metadata UI</a></li><li><span data-type="index-term">Python only APIs</span>, <a data-type="index:locator" href="#idm45831174769288">Kubeflow ML Metadata</a></li><li><span data-type="index-term">required imports</span>, <a data-type="index:locator" href="#idm45831174765448">Kubeflow ML Metadata</a></li></ul></li><li><span data-type="index-term">Kubeflow Pipelines</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831189922904">Pipelines</a>, <a data-type="index:locator" href="#idm45831185457400">Getting Set Up with Kubeflow</a>, <a data-type="index:locator" href="#idm45831180181288">Kubeflow Pipelines</a>, <a data-type="index:locator" href="#idm45831180134616">Kubeflow Pipelines</a>, <a data-type="index:locator" href="#idm45831179730568">Kubeflow Pipelines</a></li><li><span data-type="index-term">and TensorFlow Extended</span>, <a data-type="index:locator" href="#idm45831177008296">TensorFlow Extended</a></li><li><span data-type="index-term">annotations</span>, <a data-type="index:locator" href="#idm45831179316280">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">Argo alternative</span>, <a data-type="index:locator" href="#idm45831178658952">Argo: the Foundation of Pipelines</a></li><li><span data-type="index-term">Argo Workflows enhanced by</span>, <a data-type="index:locator" href="#idm45831178518088">What Kubeflow Pipelines Adds to Argo Workflow</a></li><li><span data-type="index-term">Argo Workflows foundation</span>, <a data-type="index:locator" href="#idm45831180177512">Kubeflow Pipelines</a>, <a data-type="index:locator" href="#ch04-pip">Introduction to Kubeflow Pipelines Components</a>-<a data-type="index:locator" href="#idm45831178663304">Argo: the Foundation of Pipelines</a></li><li><span data-type="index-term">building examples in Python</span>, <a data-type="index:locator" href="#ch04-ex">Building a Simple Pipeline in Python</a>-<a data-type="index:locator" href="#idm45831179485976">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">camelCase function name bug in DSL</span>, <a data-type="index:locator" href="#idm45831179648136">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">compiler</span>, <a data-type="index:locator" href="#idm45831180166600">Kubeflow Pipelines</a>, <a data-type="index:locator" href="#idm45831179568968">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">components of</span>, <a data-type="index:locator" href="#idm45831180175368">Kubeflow Pipelines</a>, <a data-type="index:locator" href="#idm45831178080568">Kubeflow Pipeline Components</a></li><li><span data-type="index-term">conditional execution</span>, <a data-type="index:locator" href="#ch04-cond">Conditional Execution of Pipeline Stages</a>-<a data-type="index:locator" href="#idm45831177533048">Conditional Execution of Pipeline Stages</a></li><li><span data-type="index-term">custom code and tools inside</span>, <a data-type="index:locator" href="#idm45831178508872">Building a Pipeline Using Existing Images</a>, <a data-type="index:locator" href="#idm45831177074248">Custom Containers</a></li><li><span data-type="index-term">data and feature preparation</span>, <a data-type="index:locator" href="#idm45831177502232">Data and Feature Preparation</a>, <a data-type="index:locator" href="#ch05-tog2">Putting It Together in a Pipeline</a>-<a data-type="index:locator" href="#idm45831174889432">Using an Entire Notebook as a Data Preparation 
Pipeline Stage</a></li><li><span data-type="index-term">environment variables</span>, <a data-type="index:locator" href="#idm45831178505080">Building a Pipeline Using Existing Images</a></li><li><span data-type="index-term">experiments</span>, <a data-type="index:locator" href="#idm45831179695960">Exploring the Prepackaged Sample Pipelines</a>, <a data-type="index:locator" href="#idm45831179312280">Building a Simple Pipeline in Python</a>, <a data-type="index:locator" href="#idm45831178487432">Building a Pipeline Using Existing Images</a><ul><li><span data-type="index-term">reproducibility by sharing pipeline</span>, <a data-type="index:locator" href="#idm45831165160360">Sharing the Pipeline</a></li></ul></li><li><span data-type="index-term">exploring sample</span>, <a data-type="index:locator" href="#idm45831179718488">Exploring the Prepackaged Sample Pipelines</a></li><li><span data-type="index-term">generic versus Google Kubernetes Engine</span>, <a data-type="index:locator" href="#idm45831179714824">Exploring the Prepackaged Sample Pipelines</a></li><li><span data-type="index-term">GPU resource marking</span>, <a data-type="index:locator" href="#idm45831174938952">Using an Entire Notebook as a Data Preparation 
Pipeline Stage</a></li><li><span data-type="index-term">GPU resource marking in DSL</span>, <a data-type="index:locator" href="#idm45831179599000">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">language capabilities</span>, <a data-type="index:locator" href="#idm45831177075224">Custom Containers</a>, <a data-type="index:locator" href="#idm45831167835560">Case Study Using Multiple Tools</a><ul><li><span data-type="index-term">denoising CT scan case study</span>, <a data-type="index:locator" href="#ch09-ct">Case Study Using Multiple Tools</a>-<a data-type="index:locator" href="#idm45831165583432">Sharing the Pipeline</a></li></ul></li><li><span data-type="index-term">load_component function warning</span>, <a data-type="index:locator" href="#idm45831178050184">Kubeflow Pipeline Components</a>, <a data-type="index:locator" href="#idm45831176984424">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">load_component_from_file function</span>, <a data-type="index:locator" href="#idm45831176981288">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">operators chaining execution</span>, <a data-type="index:locator" href="#idm45831180202248">Training Operators</a></li><li><span data-type="index-term">periodic execution of</span>, <a data-type="index:locator" href="#idm45831177528680">Running Pipelines on Schedule</a></li><li><span data-type="index-term">prebuilt Docker images</span>, <a data-type="index:locator" href="#ch04-doc2">Building a Pipeline Using Existing Images</a>-<a data-type="index:locator" href="#idm45831178088968">Building a Pipeline Using Existing Images</a></li><li><span data-type="index-term">Python SDK</span>, <a data-type="index:locator" href="#idm45831180170392">Kubeflow Pipelines</a></li><li><span data-type="index-term">running</span>, <a data-type="index:locator" href="#idm45831179698232">Exploring the Prepackaged Sample Pipelines</a>, <a data-type="index:locator" href="#idm45831179364952">Building a Simple Pipeline in Python</a><ul><li><span data-type="index-term">on a schedule</span>, <a data-type="index:locator" href="#idm45831177527832">Running Pipelines on Schedule</a></li></ul></li><li><span data-type="index-term">SDK installation</span>, <a data-type="index:locator" href="#idm45831180968472">Setting Up Your Kubeflow Development Environment</a></li><li><span data-type="index-term">Service</span>, <a data-type="index:locator" href="#idm45831180162296">Kubeflow Pipelines</a><ul><li><span data-type="index-term">repository</span>, <a data-type="index:locator" href="#idm45831179742056">Kubeflow Multiuser Isolation</a></li></ul></li><li><span data-type="index-term">shared storage</span>, <a data-type="index:locator" href="#idm45831180013512">MinIO</a></li><li><span data-type="index-term">training first project</span>, <a data-type="index:locator" href="#idm45831180704424">Training and Monitoring Progress</a></li><li><span data-type="index-term">training integrated into</span>, <a data-type="index:locator" href="#idm45831170875544">Integration into Pipelines</a></li><li><span data-type="index-term">transformation code</span>, <a data-type="index:locator" href="#idm45831176565560">TensorFlow Transform, with TensorFlow Extended on Beam</a></li><li><span data-type="index-term">UI</span>, <a data-type="index:locator" href="#idm45831179716568">Exploring the Prepackaged Sample Pipelines</a></li></ul></li><li><span data-type="index-term">Kubeflow Slack workspace</span>, <a data-type="index:locator" href="#idm45831185471192">Conclusion</a>, <a data-type="index:locator" href="#idm45831164118760">Conclusion</a></li><li><span data-type="index-term">Kubernetes</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831190369448">Why Kubernetes?</a></li><li><span data-type="index-term">Argo Workflows</span>, <a data-type="index:locator" href="#idm45831180178456">Kubeflow Pipelines</a>, <a data-type="index:locator" href="#idm45831178835816">Introduction to Kubeflow Pipelines Components</a></li><li><span data-type="index-term">beginners’ resources</span>, <a data-type="index:locator" href="#idm45831190139560">Our Assumption About You</a></li><li><span data-type="index-term">client</span>, <a data-type="index:locator" href="#idm45831178447128">Building a Pipeline Using Existing Images</a></li><li><span data-type="index-term">cloud native microservices</span>, <a data-type="index:locator" href="#idm45831172141816">Deploying a TensorFlow Training Job</a></li><li><span data-type="index-term">custom resources</span><ul><li><span data-type="index-term">Kubeflow Katib</span>, <a data-type="index:locator" href="#idm45831165199480">Katib Concepts</a>, <a data-type="index:locator" href="#idm45831164371272">Configuring an Experiment</a></li></ul></li><li><span data-type="index-term">custom resources APIs</span>, <a data-type="index:locator" href="#idm45831172137512">Deploying a TensorFlow Training Job</a>, <a data-type="index:locator" href="#idm45831165198264">Katib Concepts</a></li><li><span data-type="index-term">installing Kubeflow</span>, <a data-type="index:locator" href="#idm45831185441688">Installing Kubeflow and Its Dependencies</a></li><li><span data-type="index-term">KFServing infrastructure</span>, <a data-type="index:locator" href="#idm45831168240696">Going layer by layer</a></li><li><span data-type="index-term">kubectl</span><ul><li><span data-type="index-term">deployment of Kubeflow</span>, <a data-type="index:locator" href="#idm45831180716152">Creating Our First Kubeflow Project</a></li><li><span data-type="index-term">installing</span>, <a data-type="index:locator" href="#idm45831185439736">Installing Kubeflow and Its Dependencies</a></li><li><span data-type="index-term">Jupyter notebook incorporation</span>, <a data-type="index:locator" href="#idm45831180236296">Notebooks (JupyterHub)</a></li></ul></li><li><span data-type="index-term">local cluster via Minikube</span>, <a data-type="index:locator" href="#idm45831180998136">Setting Up Local Kubernetes</a></li><li><span data-type="index-term">Pipeline Service custom resource definitions</span>, <a data-type="index:locator" href="#idm45831180159880">Kubeflow Pipelines</a></li><li><span data-type="index-term">Pods</span><ul><li><span data-type="index-term">data stored by</span>, <a data-type="index:locator" href="#idm45831180150264">Kubeflow Pipelines</a></li><li><span data-type="index-term">deployment of Kubeflow</span>, <a data-type="index:locator" href="#idm45831180711368">Creating Our First Kubeflow Project</a></li></ul></li><li><span data-type="index-term">resource creation</span>, <a data-type="index:locator" href="#idm45831180226936">Notebooks (JupyterHub)</a></li><li><span data-type="index-term">YAML configuration editing</span>, <a data-type="index:locator" href="#idm45831180855528">Editing YAML</a></li></ul></li><li><span data-type="index-term">Kubernetes custom resources</span><ul></ul></li></ul></div><div data-type="indexdiv"><h3>L</h3><ul><li><span data-type="index-term">language capabilities of pipelines</span>, <a data-type="index:locator" href="#idm45831177068584">Custom Containers</a>, <a data-type="index:locator" href="#idm45831167833080">Case Study Using Multiple Tools</a><ul><li><span data-type="index-term">denoising CT scan case study</span>, <a data-type="index:locator" href="#ch09-ct2">Case Study Using Multiple Tools</a>-<a data-type="index:locator" href="#idm45831165399896">Sharing the Pipeline</a></li></ul></li><li><span data-type="index-term">libraries</span><ul><li><span data-type="index-term">data validation via TensorFlow Extended</span>, <a data-type="index:locator" href="#idm45831176995912">Keeping your data quality: TensorFlow data validation</a>, <a data-type="index:locator" href="#idm45831176724072">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">importing</span>, <a data-type="index:locator" href="#idm45831179469416">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">Kubernetes Python library</span>, <a data-type="index:locator" href="#idm45831178502568">Building a Pipeline Using Existing Images</a></li><li><span data-type="index-term">Scikit-learn</span>, <a data-type="index:locator" href="#idm45831175241160">Putting It Together in a Pipeline</a></li><li><span data-type="index-term">stream processing</span>, <a data-type="index:locator" href="#idm45831164000792">Stream Processing Engines and Libraries</a></li></ul></li><li><span data-type="index-term">lightweight Python functions</span>, <a data-type="index:locator" href="#ch04-lit">Building a Simple Pipeline in Python</a>-<a data-type="index:locator" href="#idm45831179285400">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">load_component warning</span>, <a data-type="index:locator" href="#idm45831176985032">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">load_component_from_file</span>, <a data-type="index:locator" href="#idm45831176983544">Keeping your data quality: TensorFlow data validation</a></li></ul></div><div data-type="indexdiv"><h3>M</h3><ul><li><span data-type="index-term">MaaS (model serving as a service)</span>, <a data-type="index:locator" href="#ch08-maas3">Model Serving</a>-<a data-type="index:locator" href="#idm45831170763384">Model Serving</a><ul><li><span data-type="index-term">APIs</span>, <a data-type="index:locator" href="#idm45831170769096">Model Serving</a></li></ul></li><li><span data-type="index-term">machine learning (ML)</span><ul><li><span data-type="index-term">AutoML</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831165229432">Hyperparameter Tuning and Automated 
Machine Learning</a></li><li><span data-type="index-term">continuous learning as</span>, <a data-type="index:locator" href="#idm45831170670360">Summary of Inference Requirements</a></li><li><span data-type="index-term">Kubeflow Katib</span>, <a data-type="index:locator" href="#idm45831180120952">Hyperparameter Tuning</a>, <a data-type="index:locator" href="#idm45831164802504">Hyperparameter Tuning with Kubeflow Katib</a></li></ul></li><li><span data-type="index-term">beginners’ resources</span>, <a data-type="index:locator" href="#pref-rsrc3">Our Assumption About You</a>-<a data-type="index:locator" href="#idm45831190137032">Our Assumption About You</a></li><li><span data-type="index-term">biases</span>, <a data-type="index:locator" href="#idm45831190150696">Your Responsibility as a Practitioner</a></li><li><span data-type="index-term">explainability importance</span>, <a data-type="index:locator" href="#idm45831171412488">Scikit-Learn Training</a>, <a data-type="index:locator" href="#idm45831170737992">Model Accuracy, Drift, and Explainability</a></li><li><span data-type="index-term">framework selection</span>, <a data-type="index:locator" href="#idm45831171481640">Scikit-Learn Training</a></li><li><span data-type="index-term">no single model works best</span>, <a data-type="index:locator" href="#idm45831164938744">Hyperparameter Tuning and Automated 
Machine Learning</a></li><li><span data-type="index-term">reproducibility importance</span>, <a data-type="index:locator" href="#idm45831174946328">Artifact and Metadata Store</a></li></ul></li><li><span data-type="index-term">mailing list data preparation</span><ul><li><span data-type="index-term">about mailing list data</span>, <a data-type="index:locator" href="#idm45831185495208">Mailing List Data</a></li><li><span data-type="index-term">Apache SpamAssassin package</span>, <a data-type="index:locator" href="#idm45831177192504">Data Cleaning: Filtering Out the Junk</a>, <a data-type="index:locator" href="#idm45831175029336">Putting It Together in a Pipeline</a></li><li><span data-type="index-term">Apache Spark</span><ul><li><span data-type="index-term">filtering out bad data</span>, <a data-type="index:locator" href="#idm45831175588104">Filtering out bad data</a></li><li><span data-type="index-term">handling missing data</span>, <a data-type="index:locator" href="#idm45831175661656">Handling missing fields</a></li><li><span data-type="index-term">reading input data</span>, <a data-type="index:locator" href="#idm45831175811032">Reading the input data</a></li><li><span data-type="index-term">saving the output</span>, <a data-type="index:locator" href="#idm45831175521800">Saving the output</a>, <a data-type="index:locator" href="#idm45831175010456">Putting It Together in a Pipeline</a></li><li><span data-type="index-term">SQL</span>, <a data-type="index:locator" href="#idm45831175563624">Filtering out bad data</a></li></ul></li><li><span data-type="index-term">fetching the data</span>, <a data-type="index:locator" href="#idm45831177482232">Fetching the Data</a></li><li><span data-type="index-term">filtering out junk</span>, <a data-type="index:locator" href="#idm45831177355064">Data Cleaning: Filtering Out the Junk</a></li><li><span data-type="index-term">parallelize for data fetching</span>, <a data-type="index:locator" href="#idm45831175751448">Reading the input data</a></li><li><span data-type="index-term">putting together into a pipeline</span>, <a data-type="index:locator" href="#ch05-tog3">Putting It Together in a Pipeline</a>-<a data-type="index:locator" href="#idm45831174894408">Using an Entire Notebook as a Data Preparation 
Pipeline Stage</a></li></ul></li><li><span data-type="index-term">manual profile creation</span>, <a data-type="index:locator" href="#idm45831180249000">Getting Around the Central Dashboard</a></li><li><span data-type="index-term">metadata</span><ul><li><span data-type="index-term">about storing model creation metadata</span>, <a data-type="index:locator" href="#idm45831174604408">Kubeflow ML Metadata</a></li><li><span data-type="index-term">artifact store and</span>, <a data-type="index:locator" href="#idm45831174099048">Kubeflow Metadata UI</a></li><li><span data-type="index-term">defined</span>, <a data-type="index:locator" href="#idm45831174954840">Artifact and Metadata Store</a></li><li><span data-type="index-term">Kubeflow Metadata</span> (<span data-gentext="see">see</span> Kubeflow ML Metadata)</li><li><span data-type="index-term">Kubernetes Pods</span>, <a data-type="index:locator" href="#idm45831180146536">Kubeflow Pipelines</a></li><li><span data-type="index-term">management component</span>, <a data-type="index:locator" href="#idm45831180048760">Metadata</a></li><li><span data-type="index-term">reproducibility importance</span>, <a data-type="index:locator" href="#idm45831174947352">Artifact and Metadata Store</a></li><li><span data-type="index-term">resource on</span>, <a data-type="index:locator" href="#idm45831174951384">Artifact and Metadata Store</a></li><li><span data-type="index-term">tracked by Kubeflow</span>, <a data-type="index:locator" href="#idm45831180133176">Kubeflow Pipelines</a>, <a data-type="index:locator" href="#idm45831180049704">Metadata</a></li><li><span data-type="index-term">tracking tool in Kubeflow</span>, <a data-type="index:locator" href="#idm45831174786760">Artifact and Metadata Store</a><ul><li>(<span data-gentext="see">see also</span> Kubeflow ML Metadata)</li></ul></li><li><span data-type="index-term">viewing</span><ul><li><span data-type="index-term">Metadata UI</span>, <a data-type="index:locator" href="#idm45831174105016">Kubeflow Metadata UI</a></li><li><span data-type="index-term">programmatic query</span>, <a data-type="index:locator" href="#ch06-pm">Programmatic Query</a>-<a data-type="index:locator" href="#idm45831174124232">Programmatic Query</a></li></ul></li></ul></li><li><span data-type="index-term">Minikube</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831180999080">Setting Up Local Kubernetes</a></li><li><span data-type="index-term">local installation of Kubeflow</span>, <a data-type="index:locator" href="#idm45831181001016">Setting Up Local Kubernetes</a></li><li><span data-type="index-term">memory for</span>, <a data-type="index:locator" href="#idm45831180986664">Minikube</a></li><li><span data-type="index-term">resources online</span>, <a data-type="index:locator" href="#idm45831180992600">Minikube</a></li></ul></li><li><span data-type="index-term">MinIO</span>, <a data-type="index:locator" href="#ch03-min2">MinIO</a>-<a data-type="index:locator" href="#idm45831179860728">MinIO</a><ul><li><span data-type="index-term">Apache Spark configuration</span>, <a data-type="index:locator" href="#idm45831175825320">Spark operators in Kubeflow</a></li><li><span data-type="index-term">Client exporting a model</span>, <a data-type="index:locator" href="#idm45831172635096">TensorFlow Training</a></li><li><span data-type="index-term">data validation via TensorFlow Extended</span>, <a data-type="index:locator" href="#idm45831176914824">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">distributed object storage server</span>, <a data-type="index:locator" href="#ch03-min">MinIO</a>-<a data-type="index:locator" href="#idm45831179861336">MinIO</a></li><li><span data-type="index-term">file_output</span>, <a data-type="index:locator" href="#idm45831178939224">Storing Data Between Steps</a>, <a data-type="index:locator" href="#idm45831176913880">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">Hadoop version for</span>, <a data-type="index:locator" href="#idm45831179911048">MinIO</a></li><li><span data-type="index-term">secrets for credentials</span>, <a data-type="index:locator" href="#idm45831179908424">MinIO</a></li></ul></li><li><span data-type="index-term">mirrored distributed training strategy</span>, <a data-type="index:locator" href="#idm45831172033720">Distributed Training</a></li><li><span data-type="index-term">ML</span> (<span data-gentext="see">see</span> machine learning)</li><li><span data-type="index-term">ML Metadata TensorFlow Extended (TFX)</span>, <a data-type="index:locator" href="#idm45831174784616">Artifact and Metadata Store</a></li><li><span data-type="index-term">MLflow (Databricks)</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831189898440">MLflow (Databricks)</a>, <a data-type="index:locator" href="#idm45831174776664">Artifact and Metadata Store</a></li><li><span data-type="index-term">metadata tools</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831174086168">Using MLflow’s Metadata Tools with Kubeflow</a></li><li><span data-type="index-term">logging data on runs</span>, <a data-type="index:locator" href="#idm45831173890408">Logging Data on Runs</a></li><li><span data-type="index-term">MLflow Tracking</span>, <a data-type="index:locator" href="#idm45831174779688">Artifact and Metadata Store</a>, <a data-type="index:locator" href="#idm45831174042392">Using MLflow’s Metadata Tools with Kubeflow</a></li><li><span data-type="index-term">MLflow Tracking Server</span>, <a data-type="index:locator" href="#ch06-trk">Using MLflow’s Metadata Tools with Kubeflow</a>-<a data-type="index:locator" href="#idm45831173894968">Creating and Deploying an MLflow Tracking Server</a></li><li><span data-type="index-term">UI</span>, <a data-type="index:locator" href="#idm45831173300632">Using the MLflow UI</a></li></ul></li></ul></li><li><span data-type="index-term">MNIST (Modified National Institute of Standards and Technology)</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831185501496">Modified National Institute of Standards and Technology</a></li><li><span data-type="index-term">data registration example</span>, <a data-type="index:locator" href="#idm45831174634008">Kubeflow ML Metadata</a></li><li><span data-type="index-term">distributed training</span>, <a data-type="index:locator" href="#ch07-MN">Distributed Training</a>-<a data-type="index:locator" href="#idm45831171948232">Distributed Training</a></li><li><span data-type="index-term">hello world project</span>, <a data-type="index:locator" href="#ch02-frst4">Creating Our First Kubeflow Project</a>-<a data-type="index:locator" href="#idm45831180283208">Test Query</a></li><li><span data-type="index-term">Kubeflow Katib first experiment</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831164463240">Running Your First Katib Experiment</a></li><li><span data-type="index-term">configuring experiment</span>, <a data-type="index:locator" href="#idm45831164369144">Configuring an Experiment</a></li><li><span data-type="index-term">prepping training code</span>, <a data-type="index:locator" href="#idm45831164440088">Prepping Your Training Code</a></li><li><span data-type="index-term">running experiment</span>, <a data-type="index:locator" href="#ch10-fexr3">Running the Experiment</a>-<a data-type="index:locator" href="#idm45831164250360">Running the Experiment</a></li></ul></li><li><span data-type="index-term">Python script for first project</span>, <a data-type="index:locator" href="#idm45831180679512">Test Query</a></li></ul></li><li><span data-type="index-term">model as data MaaS</span>, <a data-type="index:locator" href="#idm45831170773912">Model Serving</a></li><li><span data-type="index-term">model development life cycle (MDLC)</span>, <a data-type="index:locator" href="#idm45831188256824">Model Development Life Cycle</a>, <a data-type="index:locator" href="#ch01-mdlc">Data Exploration with Notebooks</a>-<a data-type="index:locator" href="#idm45831189926280">Inference/Prediction</a>, <a data-type="index:locator" href="#idm45831170677112">Summary of Inference Requirements</a></li><li><span data-type="index-term">model drift</span>, <a data-type="index:locator" href="#idm45831170740968">Model Accuracy, Drift, and Explainability</a>, <a data-type="index:locator" href="#idm45831170740296">Model Accuracy, Drift, and Explainability</a><ul><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169079288">Outlier and drift detection</a>, <a data-type="index:locator" href="#idm45831169078344">Outlier and drift detection</a></li></ul></li><li><span data-type="index-term">model explainability</span><ul><li><span data-type="index-term">importance of</span>, <a data-type="index:locator" href="#idm45831171382600">Scikit-Learn Training</a>, <a data-type="index:locator" href="#idm45831170737032">Model Accuracy, Drift, and Explainability</a></li><li><span data-type="index-term">Scikit-learn</span>, <a data-type="index:locator" href="#ch07-exxxp">Scikit-Learn Training</a>-<a data-type="index:locator" href="#idm45831170913656">Explaining the Model</a>, <a data-type="index:locator" href="#idm45831170736088">Model Accuracy, Drift, and Explainability</a></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169622616">Model explainability</a></li></ul></li><li><span data-type="index-term">model inference</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831170883480">Model Inference</a>, <a data-type="index:locator" href="#idm45831170676440">Summary of Inference Requirements</a></li><li><span data-type="index-term">accuracy</span>, <a data-type="index:locator" href="#idm45831170739624">Model Accuracy, Drift, and Explainability</a></li><li><span data-type="index-term">as code Maas</span>, <a data-type="index:locator" href="#idm45831170774920">Model Serving</a></li><li><span data-type="index-term">components</span>, <a data-type="index:locator" href="#idm45831180089112">Model Inference</a></li><li><span data-type="index-term">continuous learning</span>, <a data-type="index:locator" href="#idm45831170672264">Summary of Inference Requirements</a></li><li><span data-type="index-term">debugging TFJob deployment</span>, <a data-type="index:locator" href="#idm45831172055848">Deploying a TensorFlow Training Job</a></li><li><span data-type="index-term">deployment of in distributed training MNIST example</span>, <a data-type="index:locator" href="#ch07-dr">Distributed Training</a>-<a data-type="index:locator" href="#idm45831171948904">Distributed Training</a></li><li><span data-type="index-term">deployment reproducibility importance</span>, <a data-type="index:locator" href="#idm45831174944680">Artifact and Metadata Store</a></li><li><span data-type="index-term">deployment strategies</span>, <a data-type="index:locator" href="#idm45831172150312">Deploying a TensorFlow Training Job</a></li><li><span data-type="index-term">first project test query</span>, <a data-type="index:locator" href="#idm45831180685432">Test Query</a></li><li><span data-type="index-term">Istio</span>, <a data-type="index:locator" href="#idm45831180075096">Model Inference</a></li><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#ch08-kfs">KFServing</a>-<a data-type="index:locator" href="#idm45831167848360">Summary</a><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831168999912">KFServing</a>, <a data-type="index:locator" href="#idm45831167924680">Review</a></li><li><span data-type="index-term">autoscaling via escape hatches</span>, <a data-type="index:locator" href="#ch08-ausch2">Escape hatches</a>-<a data-type="index:locator" href="#idm45831168092296">Escape hatches</a></li><li><span data-type="index-term">capabilities of</span>, <a data-type="index:locator" href="#idm45831167935320">Additional features</a></li><li><span data-type="index-term">comparison chart</span>, <a data-type="index:locator" href="#idm45831170661288">Model Inference in Kubeflow</a></li><li><span data-type="index-term">data plane</span>, <a data-type="index:locator" href="#ch08-infzzy">Data Plane</a>-<a data-type="index:locator" href="#idm45831168934744">Data Plane</a></li><li><span data-type="index-term">debugging InferenceService</span>, <a data-type="index:locator" href="#idm45831168086968">Debugging an InferenceService</a></li><li><span data-type="index-term">debugging performance</span>, <a data-type="index:locator" href="#idm45831168068664">Debugging performance</a></li><li><span data-type="index-term">deployment strategies</span>, <a data-type="index:locator" href="#idm45831167892264">Model updating</a></li><li><span data-type="index-term">InferenceService debugging</span>, <a data-type="index:locator" href="#idm45831168088200">Debugging an InferenceService</a></li><li><span data-type="index-term">InferenceService escape hatches</span>, <a data-type="index:locator" href="#ch08-esch2">Escape hatches</a>-<a data-type="index:locator" href="#idm45831168094216">Escape hatches</a></li><li><span data-type="index-term">InferenceService namespace</span>, <a data-type="index:locator" href="#idm45831168515912">Recommender example</a></li><li><span data-type="index-term">InferenceService recommender</span>, <a data-type="index:locator" href="#ch08-rex2">Recommender example</a>-<a data-type="index:locator" href="#idm45831168279624">Recommender example</a></li><li><span data-type="index-term">infrastructure stack</span>, <a data-type="index:locator" href="#idm45831168275064">Peeling Back the Underlying Infrastructure</a></li><li><span data-type="index-term">model monitoring</span>, <a data-type="index:locator" href="#idm45831167910504">Model monitoring</a></li><li><span data-type="index-term">model serving</span>, <a data-type="index:locator" href="#idm45831167920568">Model serving</a></li><li><span data-type="index-term">model updating</span>, <a data-type="index:locator" href="#idm45831167898200">Model updating</a></li><li><span data-type="index-term">network monitoring and telemetry</span>, <a data-type="index:locator" href="#idm45831167902728">Model monitoring</a></li><li><span data-type="index-term">serverless inferencing</span>, <a data-type="index:locator" href="#idm45831168989416">KFServing</a>, <a data-type="index:locator" href="#idm45831168289048">Recommender example</a></li><li><span data-type="index-term">service plane</span>, <a data-type="index:locator" href="#idm45831168979064">Serverless and the Service Plane</a></li><li><span data-type="index-term">setting up</span>, <a data-type="index:locator" href="#ch08-kfinf2">Setting up KFServing</a>-<a data-type="index:locator" href="#idm45831168894056">Setting up KFServing</a></li><li><span data-type="index-term">setting up and namespaces</span>, <a data-type="index:locator" href="#idm45831168904616">Setting up KFServing</a></li></ul></li><li><span data-type="index-term">Kubeflow</span>, <a data-type="index:locator" href="#idm45831180088168">Model Inference</a></li><li><span data-type="index-term">Kubeflow model inference</span>, <a data-type="index:locator" href="#idm45831189929928">Inference/Prediction</a>, <a data-type="index:locator" href="#idm45831170666536">Model Inference in Kubeflow</a></li><li><span data-type="index-term">model monitoring</span>, <a data-type="index:locator" href="#idm45831170751192">Model Monitoring</a><ul><li><span data-type="index-term">accuracy, drift, explainability</span>, <a data-type="index:locator" href="#idm45831170742200">Model Accuracy, Drift, and Explainability</a></li><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#idm45831167909288">Model monitoring</a></li><li><span data-type="index-term">requirements</span>, <a data-type="index:locator" href="#idm45831170722680">Model Monitoring Requirements</a></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169661992">Monitoring Your Models</a>, <a data-type="index:locator" href="#idm45831169076456">Outlier and drift detection</a>, <a data-type="index:locator" href="#idm45831169019096">Model monitoring</a></li><li><span data-type="index-term">TensorFlow Serving</span>, <a data-type="index:locator" href="#idm45831170370024">Model monitoring</a></li></ul></li><li><span data-type="index-term">model serving</span>, <a data-type="index:locator" href="#idm45831170852536">Model Serving</a><ul><li><span data-type="index-term">embedded</span>, <a data-type="index:locator" href="#idm45831170849096">Model Serving</a></li><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#idm45831167919352">Model serving</a></li><li><span data-type="index-term">model serving as a service</span>, <a data-type="index:locator" href="#ch08-infyyz">Model Serving</a>-<a data-type="index:locator" href="#idm45831170764056">Model Serving</a></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169030120">Model serving</a></li><li><span data-type="index-term">TensorFlow Serving</span>, <a data-type="index:locator" href="#idm45831170378552">Model serving</a></li></ul></li><li><span data-type="index-term">model serving requirements</span>, <a data-type="index:locator" href="#ch08-infz">Model Serving Requirements</a>-<a data-type="index:locator" href="#idm45831170753672">Model Serving Requirements</a></li><li><span data-type="index-term">model updating</span>, <a data-type="index:locator" href="#idm45831170707880">Model Updating</a><ul><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#idm45831167896984">Model updating</a></li><li><span data-type="index-term">requirements</span>, <a data-type="index:locator" href="#idm45831170684536">Model Updating Requirements</a></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169012872">Model updating</a></li><li><span data-type="index-term">TensorFlow Serving</span>, <a data-type="index:locator" href="#idm45831170332648">Model updating</a></li></ul></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#ch08-sc">Seldon Core</a>-<a data-type="index:locator" href="#idm45831169005896">Summary</a><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831170318232">Seldon Core</a></li><li><span data-type="index-term">comparison chart</span>, <a data-type="index:locator" href="#idm45831170662504">Model Inference in Kubeflow</a></li><li><span data-type="index-term">deployment</span>, <a data-type="index:locator" href="#idm45831170165880">Creating a SeldonDeployment</a></li><li><span data-type="index-term">example graphs</span>, <a data-type="index:locator" href="#ch08-sceg2">Creating a SeldonDeployment</a>-<a data-type="index:locator" href="#idm45831169978056">Creating a SeldonDeployment</a></li><li><span data-type="index-term">explaining the model</span>, <a data-type="index:locator" href="#idm45831169625016">Model explainability</a></li><li><span data-type="index-term">income predictor model</span>, <a data-type="index:locator" href="#ch08-incp3">US Census income predictor model example</a>-<a data-type="index:locator" href="#idm45831169085528">US Census income predictor model example</a></li><li><span data-type="index-term">inference graph</span>, <a data-type="index:locator" href="#idm45831170314920">Seldon Core</a></li><li><span data-type="index-term">model serving</span>, <a data-type="index:locator" href="#idm45831169031336">Model serving</a></li><li><span data-type="index-term">model updating</span>, <a data-type="index:locator" href="#idm45831169014088">Model updating</a></li><li><span data-type="index-term">monitoring models</span>, <a data-type="index:locator" href="#idm45831169663208">Monitoring Your Models</a>, <a data-type="index:locator" href="#idm45831169081464">Outlier and drift detection</a>, <a data-type="index:locator" href="#idm45831169022744">Model monitoring</a></li><li><span data-type="index-term">packaging the model</span>, <a data-type="index:locator" href="#idm45831170171704">Packaging your model</a></li><li><span data-type="index-term">SeldonMessage</span>, <a data-type="index:locator" href="#idm45831169737736">Serving Requests</a></li><li><span data-type="index-term">sentiment prediction model</span>, <a data-type="index:locator" href="#ch08-issz">Sentiment prediction model</a>-<a data-type="index:locator" href="#idm45831169353144">Sentiment prediction model</a></li><li><span data-type="index-term">serverless primitives lacking</span>, <a data-type="index:locator" href="#idm45831168994312">KFServing</a></li><li><span data-type="index-term">serving requests</span>, <a data-type="index:locator" href="#idm45831169776904">Serving Requests</a></li><li><span data-type="index-term">setting up</span>, <a data-type="index:locator" href="#idm45831170284280">Setting up Seldon Core</a></li><li><span data-type="index-term">testing the model</span>, <a data-type="index:locator" href="#idm45831169975448">Testing Your Model</a>, <a data-type="index:locator" href="#idm45831169829160">Local testing with Docker</a></li></ul></li><li><span data-type="index-term">TensorFlow recommender deployment</span><ul><li><span data-type="index-term">with TensorFlow Serving</span>, <a data-type="index:locator" href="#ch08-tfsrec2">TensorFlow Serving</a>-<a data-type="index:locator" href="#idm45831170385496">Review</a></li></ul></li><li><span data-type="index-term">TensorFlow Serving</span>, <a data-type="index:locator" href="#ch08-tfs">TensorFlow Serving</a>-<a data-type="index:locator" href="#idm45831170387512">Review</a><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831170627480">TensorFlow Serving</a></li><li><span data-type="index-term">comparison chart</span>, <a data-type="index:locator" href="#idm45831170663784">Model Inference in Kubeflow</a></li><li><span data-type="index-term">model monitoring</span>, <a data-type="index:locator" href="#idm45831170368808">Model monitoring</a></li><li><span data-type="index-term">model serving</span>, <a data-type="index:locator" href="#idm45831170379768">Model serving</a></li><li><span data-type="index-term">model updating</span>, <a data-type="index:locator" href="#idm45831170333896">Model updating</a></li><li><span data-type="index-term">recommendation system</span>, <a data-type="index:locator" href="#ch08-myyz">TensorFlow Serving</a>-<a data-type="index:locator" href="#idm45831170383480">Review</a></li><li><span data-type="index-term">serverless primitives lacking</span>, <a data-type="index:locator" href="#idm45831168996456">KFServing</a></li><li><span data-type="index-term">TensorFlow Extended and</span>, <a data-type="index:locator" href="#idm45831177054648">Distributed Tooling</a></li></ul></li><li><span data-type="index-term">updating models</span>, <a data-type="index:locator" href="#idm45831170705992">Model Updating</a><ul><li><span data-type="index-term">TensorFlow Serving</span>, <a data-type="index:locator" href="#idm45831170330520">Model updating</a></li></ul></li></ul></li><li><span data-type="index-term">model serving</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831170851560">Model Serving</a></li><li><span data-type="index-term">custom applications</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831164013288">Using Model Serving in Applications</a></li><li><span data-type="index-term">batch applications</span>, <a data-type="index:locator" href="#app-bat2">Building Batch Applications Leveraging Model Serving</a>-<a data-type="index:locator" href="#idm45831163895432">Building Batch Applications Leveraging Model Serving</a></li><li><span data-type="index-term">streaming applications</span>, <a data-type="index:locator" href="#app-ms">Building Streaming Applications Leveraging 
Model Serving</a>-<a data-type="index:locator" href="#idm45831163921608">Introducing Cloudflow</a></li></ul></li><li><span data-type="index-term">embedded</span>, <a data-type="index:locator" href="#idm45831170850104">Model Serving</a></li><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#idm45831167918136">Model serving</a></li><li><span data-type="index-term">model serving as a service</span>, <a data-type="index:locator" href="#ch08-maas">Model Serving</a>-<a data-type="index:locator" href="#idm45831170765400">Model Serving</a></li><li><span data-type="index-term">model updating</span>, <a data-type="index:locator" href="#idm45831170708856">Model Updating</a><ul><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#idm45831167894552">Model updating</a></li><li><span data-type="index-term">requirements</span>, <a data-type="index:locator" href="#idm45831170685816">Model Updating Requirements</a></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169011656">Model updating</a></li><li><span data-type="index-term">TensorFlow Serving</span>, <a data-type="index:locator" href="#idm45831170329304">Model updating</a></li></ul></li><li><span data-type="index-term">monitoring</span>, <a data-type="index:locator" href="#idm45831170750216">Model Monitoring</a><ul><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#idm45831167906856">Model monitoring</a></li><li><span data-type="index-term">Knative serving project</span>, <a data-type="index:locator" href="#idm45831180066952">Model Inference</a></li><li><span data-type="index-term">requirements</span>, <a data-type="index:locator" href="#idm45831170721464">Model Monitoring Requirements</a></li><li><span data-type="index-term">resources on</span>, <a data-type="index:locator" href="#idm45831170718360">Model Monitoring Requirements</a></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169659560">Monitoring Your Models</a>, <a data-type="index:locator" href="#idm45831169075240">Outlier and drift detection</a>, <a data-type="index:locator" href="#idm45831169021528">Model monitoring</a></li><li><span data-type="index-term">TensorFlow Serving</span>, <a data-type="index:locator" href="#idm45831170371240">Model monitoring</a></li></ul></li><li><span data-type="index-term">requirements</span>, <a data-type="index:locator" href="#ch08-req">Model Serving Requirements</a>-<a data-type="index:locator" href="#idm45831170754376">Model Serving Requirements</a></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169028904">Model serving</a></li><li><span data-type="index-term">TensorFlow Serving</span>, <a data-type="index:locator" href="#idm45831170377336">Model serving</a></li></ul></li><li><span data-type="index-term">model serving as a service (MaaS)</span>, <a data-type="index:locator" href="#ch08-maas2">Model Serving</a>-<a data-type="index:locator" href="#idm45831170764728">Model Serving</a></li><li><span data-type="index-term">model training</span> (<span data-gentext="see">see</span> training)</li><li><span data-type="index-term">models</span><ul><li><span data-type="index-term">about the impact of</span>, <a data-type="index:locator" href="#idm45831190154680">Your Responsibility as a Practitioner</a></li><li><span data-type="index-term">continuous learning</span>, <a data-type="index:locator" href="#idm45831170673928">Summary of Inference Requirements</a></li><li><span data-type="index-term">evaluating competing</span>, <a data-type="index:locator" href="#idm45831170689304">Model Updating</a></li><li><span data-type="index-term">explainability importance</span>, <a data-type="index:locator" href="#idm45831171413944">Scikit-Learn Training</a><ul><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169623800">Model explainability</a></li></ul></li><li><span data-type="index-term">exporting</span><ul><li><span data-type="index-term">Scikit-learn</span>, <a data-type="index:locator" href="#idm45831170907848">Exporting Model</a></li><li><span data-type="index-term">TensorFlow</span>, <a data-type="index:locator" href="#idm45831172636312">TensorFlow Training</a></li></ul></li><li><span data-type="index-term">monitoring</span>, <a data-type="index:locator" href="#idm45831170748328">Model Monitoring</a><ul><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#idm45831167908072">Model monitoring</a></li><li><span data-type="index-term">Knative serving project</span>, <a data-type="index:locator" href="#idm45831180068200">Model Inference</a></li><li><span data-type="index-term">requirements</span>, <a data-type="index:locator" href="#idm45831170723928">Model Monitoring Requirements</a></li><li><span data-type="index-term">resources on</span>, <a data-type="index:locator" href="#idm45831170717144">Model Monitoring Requirements</a></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169660776">Monitoring Your Models</a>, <a data-type="index:locator" href="#idm45831169074024">Outlier and drift detection</a>, <a data-type="index:locator" href="#idm45831169020312">Model monitoring</a></li><li><span data-type="index-term">TensorFlow Serving</span>, <a data-type="index:locator" href="#idm45831170367592">Model monitoring</a></li></ul></li><li><span data-type="index-term">serving options</span>, <a data-type="index:locator" href="#idm45831180087224">Model Inference</a></li><li><span data-type="index-term">updating</span>, <a data-type="index:locator" href="#idm45831170706936">Model Updating</a><ul><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#idm45831167895768">Model updating</a></li><li><span data-type="index-term">requirements</span>, <a data-type="index:locator" href="#idm45831170683320">Model Updating Requirements</a></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169010440">Model updating</a></li><li><span data-type="index-term">TensorFlow Serving</span>, <a data-type="index:locator" href="#idm45831170328088">Model updating</a></li></ul></li><li><span data-type="index-term">validation</span>, <a data-type="index:locator" href="#idm45831189933928">Model Validation</a>, <a data-type="index:locator" href="#idm45831180278696">Test Query</a>, <a data-type="index:locator" href="#idm45831170744472">Model Accuracy, Drift, and Explainability</a></li></ul></li><li><span data-type="index-term">Modified National Institute of Standards and Technology</span> (<span data-gentext="see">see</span> MNIST)</li><li><span data-type="index-term">monitoring</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831170749272">Model Monitoring</a></li><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#idm45831167905640">Model monitoring</a></li><li><span data-type="index-term">Knative serving project</span>, <a data-type="index:locator" href="#idm45831180065736">Model Inference</a></li><li><span data-type="index-term">model serving</span>, <a data-type="index:locator" href="#idm45831170735144">Model Accuracy, Drift, and Explainability</a></li><li><span data-type="index-term">requirements</span>, <a data-type="index:locator" href="#idm45831170720248">Model Monitoring Requirements</a></li><li><span data-type="index-term">resources on</span>, <a data-type="index:locator" href="#idm45831170719304">Model Monitoring Requirements</a></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169658344">Monitoring Your Models</a>, <a data-type="index:locator" href="#idm45831169077400">Outlier and drift detection</a>, <a data-type="index:locator" href="#idm45831169024664">Model monitoring</a></li><li><span data-type="index-term">TensorFlow Serving</span>, <a data-type="index:locator" href="#idm45831170372184">Model monitoring</a></li></ul></li><li><span data-type="index-term">multi-armed bandits</span>, <a data-type="index:locator" href="#idm45831167859224">Model updating</a></li><li><span data-type="index-term">multiuser isolation</span>, <a data-type="index:locator" href="#idm45831179766824">Kubeflow Multiuser Isolation</a></li><li><span data-type="index-term">multiworker mirrored distributed training strategy</span>, <a data-type="index:locator" href="#idm45831172029496">Distributed Training</a></li></ul></div><div data-type="indexdiv"><h3>N</h3><ul><li><span data-type="index-term">namespaces</span><ul><li><span data-type="index-term">deployment of Kubeflow</span>, <a data-type="index:locator" href="#idm45831180714808">Creating Our First Kubeflow Project</a></li><li><span data-type="index-term">KFServing</span><ul><li><span data-type="index-term">InferenceService</span>, <a data-type="index:locator" href="#idm45831168518376">Recommender example</a></li><li><span data-type="index-term">setup</span>, <a data-type="index:locator" href="#idm45831168907080">Setting up KFServing</a></li></ul></li><li><span data-type="index-term">manual profile creation</span>, <a data-type="index:locator" href="#idm45831180248296">Getting Around the Central Dashboard</a></li><li><span data-type="index-term">profile definition</span>, <a data-type="index:locator" href="#idm45831179756856">Kubeflow Multiuser Isolation</a></li><li><span data-type="index-term">Seldon Core installation</span>, <a data-type="index:locator" href="#idm45831170276920">Setting up Seldon Core</a></li></ul></li><li><span data-type="index-term">NAS</span> (<span data-gentext="see">see</span> neural architecture search)</li><li><span data-type="index-term">natural language processing (NLP)</span>, <a data-type="index:locator" href="#idm45831188241000">Why Containerize?</a></li><li><span data-type="index-term">neural architecture search (NAS)</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831164907512">AutoML: An Overview</a>, <a data-type="index:locator" href="#idm45831164187000">Neural Architecture Search</a></li><li><span data-type="index-term">AutoML</span>, <a data-type="index:locator" href="#idm45831164906472">AutoML: An Overview</a></li><li><span data-type="index-term">Differentiable Architecture Search</span>, <a data-type="index:locator" href="#idm45831164174296">Neural Architecture Search</a></li><li><span data-type="index-term">Efficient Neural Architecture Search</span>, <a data-type="index:locator" href="#idm45831164172232">Neural Architecture Search</a></li><li><span data-type="index-term">generation versus mutation methods</span>, <a data-type="index:locator" href="#idm45831164180152">Neural Architecture Search</a></li><li><span data-type="index-term">Kubeflow Katib supporting</span>, <a data-type="index:locator" href="#idm45831164175144">Neural Architecture Search</a><ul><li><span data-type="index-term">example DARTS experiment</span>, <a data-type="index:locator" href="#ch10-mm3">Neural Architecture Search</a>-<a data-type="index:locator" href="#idm45831164143688">Neural Architecture Search</a></li><li><span data-type="index-term">model manager</span>, <a data-type="index:locator" href="#idm45831164161720">Neural Architecture Search</a></li></ul></li></ul></li><li><span data-type="index-term">notebooks</span> (<span data-gentext="see">see</span> Jupyter notebooks)</li></ul></div><div data-type="indexdiv"><h3>O</h3><ul><li><span data-type="index-term">object stores</span><ul><li><span data-type="index-term">distributed object storage server</span>, <a data-type="index:locator" href="#idm45831180017544">MinIO</a>, <a data-type="index:locator" href="#idm45831178942072">Storing Data Between Steps</a><ul><li>(<span data-gentext="see">see also</span> MinIO)</li></ul></li><li><span data-type="index-term">using with Apache Spark</span>, <a data-type="index:locator" href="#idm45831175483928">Saving the output</a></li></ul></li><li><span data-type="index-term">observability automated by operators</span>, <a data-type="index:locator" href="#idm45831180203560">Training Operators</a></li><li><span data-type="index-term">one-hot encoding in Scikit-learn</span>, <a data-type="index:locator" href="#idm45831171786584">Data Preparation</a></li><li><span data-type="index-term">online community for Kubeflow</span>, <a data-type="index:locator" href="#idm45831185471832">Conclusion</a>, <a data-type="index:locator" href="#idm45831164119368">Conclusion</a></li><li><span data-type="index-term">online resources</span> (<span data-gentext="see">see</span> resources)</li><li><span data-type="index-term">orchestration controllers</span>, <a data-type="index:locator" href="#idm45831180155544">Kubeflow Pipelines</a>, <a data-type="index:locator" href="#idm45831180061448">Model Inference</a></li></ul></div><div data-type="indexdiv"><h3>P</h3><ul><li><span data-type="index-term">parallelize for data fetching</span>, <a data-type="index:locator" href="#idm45831175750536">Reading the input data</a></li><li><span data-type="index-term">parameter server distributed training</span>, <a data-type="index:locator" href="#idm45831172025144">Distributed Training</a></li><li><span data-type="index-term">persistent volume storage</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#ch04-pvs2">Storing Data Between Steps</a>-<a data-type="index:locator" href="#idm45831178982520">Storing Data Between Steps</a></li><li><span data-type="index-term">Apache Spark output</span>, <a data-type="index:locator" href="#idm45831175494920">Saving the output</a></li><li><span data-type="index-term">filesystem/get_file component</span>, <a data-type="index:locator" href="#idm45831176834920">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">local data preparation</span>, <a data-type="index:locator" href="#idm45831177474216">Fetching the Data</a></li></ul></li><li><span data-type="index-term">pinned deployments</span>, <a data-type="index:locator" href="#idm45831170691896">Model Updating</a><ul><li><span data-type="index-term">KFServing endpoints</span>, <a data-type="index:locator" href="#idm45831168958296">Data Plane</a>, <a data-type="index:locator" href="#idm45831167860648">Model updating</a></li></ul></li><li><span data-type="index-term">Pods (Kubernetes)</span><ul><li><span data-type="index-term">data stored by</span>, <a data-type="index:locator" href="#idm45831180151240">Kubeflow Pipelines</a></li><li><span data-type="index-term">deployment of Kubeflow</span>, <a data-type="index:locator" href="#idm45831180712312">Creating Our First Kubeflow Project</a></li></ul></li><li><span data-type="index-term">portability of Kubeflow</span><ul><li><span data-type="index-term">Kubernetes foundation</span>, <a data-type="index:locator" href="#idm45831190352168">Kubeflow’s Design and Core Components</a>, <a data-type="index:locator" href="#idm45831172142760">Deploying a TensorFlow Training Job</a></li><li><span data-type="index-term">object storage and</span>, <a data-type="index:locator" href="#idm45831178870888">Storing Data Between Steps</a></li></ul></li><li><span data-type="index-term">prediction</span> (<span data-gentext="see">see</span> model inference)</li><li><span data-type="index-term">product recommender</span> (<span data-gentext="see">see</span> recommendation systems)</li><li><span data-type="index-term">profiles</span><ul><li><span data-type="index-term">automatic creation</span>, <a data-type="index:locator" href="#idm45831179753752">Kubeflow Multiuser Isolation</a></li><li><span data-type="index-term">definition</span>, <a data-type="index:locator" href="#idm45831179757912">Kubeflow Multiuser Isolation</a></li><li><span data-type="index-term">manual creation</span>, <a data-type="index:locator" href="#idm45831180247352">Getting Around the Central Dashboard</a></li><li><span data-type="index-term">multiuser isolation</span>, <a data-type="index:locator" href="#idm45831179765176">Kubeflow Multiuser Isolation</a></li></ul></li><li><span data-type="index-term">Python</span><ul><li><span data-type="index-term">Apache Beam support of</span>, <a data-type="index:locator" href="#idm45831177014920">TensorFlow Extended</a></li><li><span data-type="index-term">Apache Spark</span><ul><li><span data-type="index-term">basics</span>, <a data-type="index:locator" href="#idm45831176089672">Spark operators in Kubeflow</a></li><li><span data-type="index-term">reading input data</span>, <a data-type="index:locator" href="#idm45831175809720">Reading the input data</a></li></ul></li><li><span data-type="index-term">building example pipelines</span>, <a data-type="index:locator" href="#ch04-ex2">Building a Simple Pipeline in Python</a>-<a data-type="index:locator" href="#idm45831179485272">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">camelCase function name bug</span>, <a data-type="index:locator" href="#idm45831179649080">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">client for Python-wrapped models</span>, <a data-type="index:locator" href="#idm45831169961736">Python client for Python language wrapped models</a></li><li><span data-type="index-term">DSL compiler</span>, <a data-type="index:locator" href="#idm45831180165592">Kubeflow Pipelines</a></li><li><span data-type="index-term">GPU resource marking</span>, <a data-type="index:locator" href="#idm45831179599944">Building a Simple Pipeline in Python</a>, <a data-type="index:locator" href="#idm45831174891592">Using an Entire Notebook as a Data Preparation 
Pipeline Stage</a></li><li><span data-type="index-term">installing</span>, <a data-type="index:locator" href="#idm45831180964136">Setting up the Pipeline SDK</a></li><li><span data-type="index-term">KFServing API documentation</span>, <a data-type="index:locator" href="#idm45831167929720">API documentation</a></li><li><span data-type="index-term">Kubeflow ML Metadata</span>, <a data-type="index:locator" href="#idm45831174770232">Kubeflow ML Metadata</a></li><li><span data-type="index-term">Kubeflow native Spark operator</span>, <a data-type="index:locator" href="#idm45831176379080">Spark operators in Kubeflow</a></li><li><span data-type="index-term">Kubernetes client</span>, <a data-type="index:locator" href="#idm45831178472872">Building a Pipeline Using Existing Images</a></li><li><span data-type="index-term">library imports</span>, <a data-type="index:locator" href="#idm45831179459544">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">lightweight Python functions</span>, <a data-type="index:locator" href="#ch04-lit2">Building a Simple Pipeline in Python</a>-<a data-type="index:locator" href="#idm45831179284728">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">MNIST image script</span>, <a data-type="index:locator" href="#idm45831180680520">Test Query</a></li><li><span data-type="index-term">Pandas and</span>, <a data-type="index:locator" href="#idm45831176827272">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">pipeline components</span>, <a data-type="index:locator" href="#idm45831180171368">Kubeflow Pipelines</a></li><li><span data-type="index-term">pipeline custom code and tools</span>, <a data-type="index:locator" href="#idm45831178506984">Building a Pipeline Using Existing Images</a>, <a data-type="index:locator" href="#idm45831177072376">Custom Containers</a></li><li><span data-type="index-term">Scikit-learn</span>, <a data-type="index:locator" href="#idm45831171921400">Training a Model Using Scikit-Learn</a></li><li><span data-type="index-term">TensorFlow Extended as Python tool</span>, <a data-type="index:locator" href="#idm45831177013976">TensorFlow Extended</a></li><li><span data-type="index-term">virtual environments for projects</span>, <a data-type="index:locator" href="#idm45831180963288">Setting up the Pipeline SDK</a></li></ul></li><li><span data-type="index-term">PyTorch for distributed training</span>, <a data-type="index:locator" href="#idm45831171935896">Using Other Frameworks for Distributed Training</a><ul><li><span data-type="index-term">job spec example</span>, <a data-type="index:locator" href="#idm45831171933000">Using Other Frameworks for Distributed Training</a></li></ul></li></ul></div><div data-type="indexdiv"><h3>Q</h3><ul><li><span data-type="index-term">quality of data maintained</span>, <a data-type="index:locator" href="#idm45831176997480">Keeping your data quality: TensorFlow data validation</a></li></ul></div><div data-type="indexdiv"><h3>R</h3><ul><li><span data-type="index-term">Random Forest algorithm</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831171915384">Training a Model Using Scikit-Learn</a></li><li><span data-type="index-term">data preparation</span>, <a data-type="index:locator" href="#ch07-rfdp">Data Preparation</a>-<a data-type="index:locator" href="#idm45831171778472">Data Preparation</a></li><li><span data-type="index-term">running</span>, <a data-type="index:locator" href="#idm45831171519944">Scikit-Learn Training</a></li></ul></li><li><span data-type="index-term">random search in Katib</span>, <a data-type="index:locator" href="#idm45831164519128">Katib Concepts</a></li><li><span data-type="index-term">recommendation systems</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831185492248">Product Recommender</a>, <a data-type="index:locator" href="#idm45831173231400">Building a Recommender with TensorFlow</a></li><li><span data-type="index-term">collaborative filtering</span>, <a data-type="index:locator" href="#idm45831173154248">Building a Recommender with TensorFlow</a></li><li><span data-type="index-term">KFServing InferenceService</span>, <a data-type="index:locator" href="#ch08-rex3">Recommender example</a>-<a data-type="index:locator" href="#idm45831168278952">Recommender example</a></li><li><span data-type="index-term">TensorFlow</span><ul><li><span data-type="index-term">deployment with TFServing</span>, <a data-type="index:locator" href="#ch08-tfsrec3">TensorFlow Serving</a>-<a data-type="index:locator" href="#idm45831170384824">Review</a></li></ul></li></ul></li><li><span data-type="index-term">repositories</span><ul><li><span data-type="index-term">components</span>, <a data-type="index:locator" href="#idm45831179747848">Kubeflow Multiuser Isolation</a>, <a data-type="index:locator" href="#idm45831178082456">Kubeflow Pipeline Components</a></li><li><span data-type="index-term">COVID-19 CT scans</span>, <a data-type="index:locator" href="#idm45831167789832">Data Prep with Python</a></li><li><span data-type="index-term">kfctl</span>, <a data-type="index:locator" href="#idm45831179743816">Kubeflow Multiuser Isolation</a></li><li><span data-type="index-term">Pipelines Service</span>, <a data-type="index:locator" href="#idm45831179740776">Kubeflow Multiuser Isolation</a></li></ul></li><li><span data-type="index-term">reproducibility in machine learning</span>, <a data-type="index:locator" href="#idm45831174945368">Artifact and Metadata Store</a></li><li><span data-type="index-term">ResourceOp request validation</span>, <a data-type="index:locator" href="#idm45831176345992">Spark operators in Kubeflow</a></li><li><span data-type="index-term">resources for Kubeflow</span><ul><li><span data-type="index-term">alternatives</span>, <a data-type="index:locator" href="#idm45831189891944">Others</a></li><li><span data-type="index-term">click-to-deploy Kubeflow app</span>, <a data-type="index:locator" href="#idm45831185449976">Getting Set Up with Kubeflow</a></li><li><span data-type="index-term">code examples for download</span>, <a data-type="index:locator" href="#idm45831189726056">Code Examples</a></li><li><span data-type="index-term">component repositories</span>, <a data-type="index:locator" href="#idm45831179746904">Kubeflow Multiuser Isolation</a>, <a data-type="index:locator" href="#idm45831178081512">Kubeflow Pipeline Components</a></li><li><span data-type="index-term">getting started guide</span>, <a data-type="index:locator" href="#idm45831180270632">Going Beyond a Local Deployment</a></li><li><span data-type="index-term">installation guide</span>, <a data-type="index:locator" href="#idm45831180720088">Creating Our First Kubeflow Project</a></li><li><span data-type="index-term">online community</span>, <a data-type="index:locator" href="#idm45831185472776">Conclusion</a>, <a data-type="index:locator" href="#idm45831164120216">Conclusion</a></li></ul></li></ul></div><div data-type="indexdiv"><h3>S</h3><ul><li><span data-type="index-term">scalability of Kubeflow</span><ul><li><span data-type="index-term">Apache Spark feature preparation</span>, <a data-type="index:locator" href="#idm45831175384744">Distributed Feature Preparation Using Apache Spark</a></li><li><span data-type="index-term">inference component</span>, <a data-type="index:locator" href="#idm45831180085336">Model Inference</a></li><li><span data-type="index-term">KFServing for inferencing</span>, <a data-type="index:locator" href="#idm45831168986920">KFServing</a><ul><li><span data-type="index-term">autoscaling via escape hatches</span>, <a data-type="index:locator" href="#ch08-ausch3">Escape hatches</a>-<a data-type="index:locator" href="#idm45831168091624">Escape hatches</a></li></ul></li><li><span data-type="index-term">Kubernetes foundation</span>, <a data-type="index:locator" href="#idm45831190349480">Kubeflow’s Design and Core Components</a>, <a data-type="index:locator" href="#idm45831180274824">Going Beyond a Local Deployment</a>, <a data-type="index:locator" href="#idm45831172143736">Deploying a TensorFlow Training Job</a></li><li><span data-type="index-term">operators automating</span>, <a data-type="index:locator" href="#idm45831180204536">Training Operators</a></li></ul></li><li><span data-type="index-term">schema</span><ul><li><span data-type="index-term">data validation via TensorFlow Extended</span>, <a data-type="index:locator" href="#idm45831176980328">Keeping your data quality: TensorFlow data validation</a>, <a data-type="index:locator" href="#ch05-schem">Keeping your data quality: TensorFlow data validation</a>-<a data-type="index:locator" href="#idm45831176606904">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">inferred by TensorFlow Data Validation</span>, <a data-type="index:locator" href="#idm45831176751944">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">inspecting</span>, <a data-type="index:locator" href="#idm45831176795304">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">rejected records check</span>, <a data-type="index:locator" href="#idm45831176612648">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">saved to catch changes</span>, <a data-type="index:locator" href="#idm45831176797144">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">tool for modifying</span>, <a data-type="index:locator" href="#idm45831176708696">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">validation by Apache Spark</span>, <a data-type="index:locator" href="#idm45831175745464">Validating the schema</a></li></ul></li><li><span data-type="index-term">Scikit-learn</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831171926168">Training a Model Using Scikit-Learn</a></li><li><span data-type="index-term">explaining the model</span>, <a data-type="index:locator" href="#ch07-exs7">Scikit-Learn Training</a>-<a data-type="index:locator" href="#idm45831170912984">Explaining the Model</a></li><li><span data-type="index-term">exporting the model</span>, <a data-type="index:locator" href="#idm45831170906632">Exporting Model</a></li><li><span data-type="index-term">library</span>, <a data-type="index:locator" href="#idm45831175240248">Putting It Together in a Pipeline</a></li><li><span data-type="index-term">missing data and</span>, <a data-type="index:locator" href="#idm45831171814504">Data Preparation</a></li><li><span data-type="index-term">one-hot encoding</span>, <a data-type="index:locator" href="#idm45831171787464">Data Preparation</a></li><li><span data-type="index-term">RandomForestClassifier</span>, <a data-type="index:locator" href="#ch02-rand">Training and Monitoring Progress</a>-<a data-type="index:locator" href="#idm45831180282536">Test Query</a></li></ul></li><li><span data-type="index-term">Scikit-learn Jupyter notebook setup</span>, <a data-type="index:locator" href="#ch07-sci2">Training a Model Using Scikit-Learn</a>-<a data-type="index:locator" href="#idm45831171910056">Training a Model Using Scikit-Learn</a></li><li><span data-type="index-term">Seldon Core</span>, <a data-type="index:locator" href="#idm45831169756072">Serving Requests</a><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831170319176">Seldon Core</a></li><li><span data-type="index-term">comparison chart</span>, <a data-type="index:locator" href="#idm45831170658216">Model Inference in Kubeflow</a></li><li><span data-type="index-term">deployment</span>, <a data-type="index:locator" href="#idm45831170166856">Creating a SeldonDeployment</a></li><li><span data-type="index-term">example graphs</span>, <a data-type="index:locator" href="#ch08-sceg">Creating a SeldonDeployment</a>-<a data-type="index:locator" href="#idm45831169978760">Creating a SeldonDeployment</a></li><li><span data-type="index-term">explaining the model</span>, <a data-type="index:locator" href="#idm45831169625992">Model explainability</a></li><li><span data-type="index-term">income predictor model</span>, <a data-type="index:locator" href="#ch08-incp4">US Census income predictor model example</a>-<a data-type="index:locator" href="#idm45831169084856">US Census income predictor model example</a></li><li><span data-type="index-term">inference</span>, <a data-type="index:locator" href="#ch08-sc2">Seldon Core</a>-<a data-type="index:locator" href="#idm45831169005192">Summary</a></li><li><span data-type="index-term">inference graph</span>, <a data-type="index:locator" href="#idm45831170315896">Seldon Core</a></li><li><span data-type="index-term">Istio ingress gateway and</span>, <a data-type="index:locator" href="#idm45831169775688">Serving Requests</a></li><li><span data-type="index-term">model serving</span>, <a data-type="index:locator" href="#idm45831169032312">Model serving</a></li><li><span data-type="index-term">model updating</span>, <a data-type="index:locator" href="#idm45831169015064">Model updating</a></li><li><span data-type="index-term">monitoring models</span>, <a data-type="index:locator" href="#idm45831169664184">Monitoring Your Models</a>, <a data-type="index:locator" href="#idm45831169082440">Outlier and drift detection</a>, <a data-type="index:locator" href="#idm45831169023688">Model monitoring</a></li><li><span data-type="index-term">outlier and drift detection in</span>, <a data-type="index:locator" href="#idm45831169080248">Outlier and drift detection</a></li><li><span data-type="index-term">packaging the model</span>, <a data-type="index:locator" href="#idm45831170172680">Packaging your model</a></li><li><span data-type="index-term">SeldonMessage</span>, <a data-type="index:locator" href="#idm45831169738600">Serving Requests</a></li><li><span data-type="index-term">sentiment prediction model</span>, <a data-type="index:locator" href="#ch08-issz2">Sentiment prediction model</a>-<a data-type="index:locator" href="#idm45831169352648">Sentiment prediction model</a></li><li><span data-type="index-term">serverless primitives lacking</span>, <a data-type="index:locator" href="#idm45831168995272">KFServing</a></li><li><span data-type="index-term">serving requests</span>, <a data-type="index:locator" href="#idm45831169777880">Serving Requests</a></li><li><span data-type="index-term">setting up</span>, <a data-type="index:locator" href="#idm45831170285256">Setting up Seldon Core</a></li><li><span data-type="index-term">testing the model</span>, <a data-type="index:locator" href="#idm45831169974200">Testing Your Model</a><ul><li><span data-type="index-term">local testing with Docker</span>, <a data-type="index:locator" href="#idm45831169887560">Local testing with Docker</a></li></ul></li></ul></li><li><span data-type="index-term">sentiment prediction model</span>, <a data-type="index:locator" href="#ch08-issz3">Sentiment prediction model</a>-<a data-type="index:locator" href="#idm45831169352008">Sentiment prediction model</a></li><li><span data-type="index-term">serverless</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831168982664">Serverless and the Service Plane</a></li><li><span data-type="index-term">containers on Kubernetes</span>, <a data-type="index:locator" href="#idm45831180076952">Model Inference</a></li><li><span data-type="index-term">KFServing</span>, <a data-type="index:locator" href="#idm45831168992280">KFServing</a>, <a data-type="index:locator" href="#idm45831168289992">Recommender example</a>, <a data-type="index:locator" href="#idm45831168025416">Knative Eventing</a></li><li><span data-type="index-term">Knative Serving</span>, <a data-type="index:locator" href="#idm45831168990360">KFServing</a></li><li><span data-type="index-term">Knative serving and</span>, <a data-type="index:locator" href="#idm45831180074152">Model Inference</a></li></ul></li><li><span data-type="index-term">serverless applications</span><ul><li><span data-type="index-term">Knative Serving</span>, <a data-type="index:locator" href="#idm45831179798152">Knative</a></li></ul></li><li><span data-type="index-term">service mesh</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831168233992">Going layer by layer</a></li><li><span data-type="index-term">components, with Istio</span>, <a data-type="index:locator" href="#idm45831179854808">Istio</a></li><li><span data-type="index-term">with Istio</span>, <a data-type="index:locator" href="#idm45831168234936">Going layer by layer</a></li></ul></li><li><span data-type="index-term">service plane</span>, <a data-type="index:locator" href="#idm45831168981352">Serverless and the Service Plane</a></li><li><span data-type="index-term">shadow models</span>, <a data-type="index:locator" href="#idm45831170702840">Model Updating</a></li><li><span data-type="index-term">single-worker TensorFlow jobs</span>, <a data-type="index:locator" href="#idm45831172048248">Distributed Training</a></li><li><span data-type="index-term">singular value decomposition (SVD)</span>, <a data-type="index:locator" href="#idm45831167823320">Case Study Using Multiple Tools</a>, <a data-type="index:locator" href="#idm45831167724008">DS-SVD with Apache Spark</a></li><li><span data-type="index-term">SpamAssassin package</span>, <a data-type="index:locator" href="#idm45831177191624">Data Cleaning: Filtering Out the Junk</a>, <a data-type="index:locator" href="#idm45831175024088">Putting It Together in a Pipeline</a></li><li><span data-type="index-term">SQL in Apache Spark</span>, <a data-type="index:locator" href="#idm45831175562440">Filtering out bad data</a></li><li><span data-type="index-term">storage</span><ul><li><span data-type="index-term">distributed object storage server</span>, <a data-type="index:locator" href="#idm45831180012568">MinIO</a>, <a data-type="index:locator" href="#idm45831178940984">Storing Data Between Steps</a></li><li><span data-type="index-term">Minikube requirements</span>, <a data-type="index:locator" href="#idm45831180972712">Minikube</a></li><li><span data-type="index-term">persistent volumes</span>, <a data-type="index:locator" href="#ch04-pvs">Storing Data Between Steps</a>-<a data-type="index:locator" href="#idm45831178983224">Storing Data Between Steps</a><ul><li><span data-type="index-term">Apache Spark output</span>, <a data-type="index:locator" href="#idm45831175494008">Saving the output</a></li><li><span data-type="index-term">filesystem/get_file component</span>, <a data-type="index:locator" href="#idm45831176833176">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">local data preparation</span>, <a data-type="index:locator" href="#idm45831177473368">Fetching the Data</a></li></ul></li><li><span data-type="index-term">storage classes</span>, <a data-type="index:locator" href="#idm45831178995768">Storing Data Between Steps</a></li><li><span data-type="index-term">storing data between steps</span>, <a data-type="index:locator" href="#ch04-stor">Storing Data Between Steps</a>-<a data-type="index:locator" href="#idm45831178839960">Storing Data Between Steps</a></li><li><span data-type="index-term">UI to explore</span>, <a data-type="index:locator" href="#idm45831180007784">MinIO</a></li></ul></li><li><span data-type="index-term">streaming applications</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831164006344">Building Streaming Applications Leveraging 
Model Serving</a></li><li><span data-type="index-term">Cloudflow</span>, <a data-type="index:locator" href="#app-cl2">Introducing Cloudflow</a>-<a data-type="index:locator" href="#idm45831163922280">Introducing Cloudflow</a></li><li><span data-type="index-term">processing engines versus libraries</span>, <a data-type="index:locator" href="#idm45831163999816">Stream Processing Engines and Libraries</a></li></ul></li><li><span data-type="index-term">suggestions (Kubeflow Katib)</span>, <a data-type="index:locator" href="#idm45831164648504">Katib Concepts</a></li></ul></div><div data-type="indexdiv"><h3>T</h3><ul><li><span data-type="index-term">tag for pushing containers</span>, <a data-type="index:locator" href="#idm45831180905640">Setting up Docker</a></li><li><span data-type="index-term">Tekton for running pipelines</span>, <a data-type="index:locator" href="#idm45831178657976">Argo: the Foundation of Pipelines</a></li><li><span data-type="index-term">TensorFlow</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831173239016">Building a Recommender with TensorFlow</a></li><li><span data-type="index-term">distributed training</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831172044184">Distributed Training</a></li><li><span data-type="index-term">distribution strategies</span>, <a data-type="index:locator" href="#idm45831172036952">Distributed Training</a></li><li><span data-type="index-term">MNIST example</span>, <a data-type="index:locator" href="#ch07-MN2">Distributed Training</a>-<a data-type="index:locator" href="#idm45831171947560">Distributed Training</a></li></ul></li><li><span data-type="index-term">jobs as Kubernetes custom resources</span>, <a data-type="index:locator" href="#idm45831172138504">Deploying a TensorFlow Training Job</a></li><li><span data-type="index-term">recommender</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831185490328">Product Recommender</a>, <a data-type="index:locator" href="#idm45831173230424">Building a Recommender with TensorFlow</a></li><li><span data-type="index-term">creating TensorFlow session</span>, <a data-type="index:locator" href="#idm45831173110408">TensorFlow Training</a></li><li><span data-type="index-term">deployment</span>, <a data-type="index:locator" href="#ch07-dep2">Deploying a TensorFlow Training Job</a>-<a data-type="index:locator" href="#idm45831172053864">Deploying a TensorFlow Training Job</a></li><li><span data-type="index-term">deployment with TFServing</span>, <a data-type="index:locator" href="#ch08-tfsrec4">TensorFlow Serving</a>-<a data-type="index:locator" href="#idm45831170384152">Review</a></li><li><span data-type="index-term">exporting model</span>, <a data-type="index:locator" href="#idm45831172639272">TensorFlow Training</a></li><li><span data-type="index-term">hyperparameters</span>, <a data-type="index:locator" href="#idm45831172757816">TensorFlow Training</a></li><li><span data-type="index-term">Keras API</span>, <a data-type="index:locator" href="#idm45831173226312">Building a Recommender with TensorFlow</a></li><li><span data-type="index-term">model selection</span>, <a data-type="index:locator" href="#idm45831173220648">Building a Recommender with TensorFlow</a></li><li><span data-type="index-term">notebook setup</span>, <a data-type="index:locator" href="#ch07-nb2">Building a Recommender with TensorFlow</a>-<a data-type="index:locator" href="#idm45831173122648">Starting a New Notebook Session</a></li><li><span data-type="index-term">running training code</span>, <a data-type="index:locator" href="#idm45831172738664">TensorFlow Training</a></li></ul></li><li><span data-type="index-term">single-worker jobs</span>, <a data-type="index:locator" href="#idm45831172049224">Distributed Training</a></li></ul></li><li><span data-type="index-term">TensorFlow Data Validation (TFDV)</span>, <a data-type="index:locator" href="#ch05-val3">Keeping your data quality: TensorFlow data validation</a>-<a data-type="index:locator" href="#idm45831176608920">Keeping your data quality: TensorFlow data validation</a><ul><li><span data-type="index-term">installing</span>, <a data-type="index:locator" href="#idm45831176993960">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">schema inferred by</span>, <a data-type="index:locator" href="#idm45831176751096">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">schema inspection</span>, <a data-type="index:locator" href="#idm45831176794392">Keeping your data quality: TensorFlow data validation</a></li></ul></li><li><span data-type="index-term">TensorFlow Extended (TFX)</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831177055560">Distributed Tooling</a>, <a data-type="index:locator" href="#idm45831177050200">TensorFlow Extended</a></li><li><span data-type="index-term">Apache Beam Python support and</span>, <a data-type="index:locator" href="#idm45831177013016">TensorFlow Extended</a></li><li><span data-type="index-term">example generators</span>, <a data-type="index:locator" href="#idm45831176829176">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">Google Dataflow</span>, <a data-type="index:locator" href="#idm45831164078696">Dataflow for TFX</a></li><li><span data-type="index-term">installing</span>, <a data-type="index:locator" href="#idm45831176994920">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">installing components</span>, <a data-type="index:locator" href="#idm45831176993000">Keeping your data quality: TensorFlow data validation</a>, <a data-type="index:locator" href="#idm45831176979368">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">Kubeflow pipelines and TFX pipelines</span>, <a data-type="index:locator" href="#idm45831177007352">TensorFlow Extended</a></li><li><span data-type="index-term">ML Metadata</span>, <a data-type="index:locator" href="#idm45831174783928">Artifact and Metadata Store</a></li><li><span data-type="index-term">Pandas dataframes accepted by</span>, <a data-type="index:locator" href="#idm45831176826328">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">schema inferred by TFDV</span>, <a data-type="index:locator" href="#idm45831176815480">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">TensorFlow Data Validation</span>, <a data-type="index:locator" href="#ch05-val4">Keeping your data quality: TensorFlow data validation</a>-<a data-type="index:locator" href="#idm45831176608248">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">Transform feature preparation</span>, <a data-type="index:locator" href="#ch05-tft4">TensorFlow Transform, with TensorFlow Extended on Beam</a>-<a data-type="index:locator" href="#idm45831176532536">TensorFlow Transform, with TensorFlow Extended on Beam</a></li></ul></li><li><span data-type="index-term">TensorFlow Model Analysis</span>, <a data-type="index:locator" href="#idm45831176598728">TensorFlow Transform, with TensorFlow Extended on Beam</a></li><li><span data-type="index-term">TensorFlow Serving (TFServing)</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831170628392">TensorFlow Serving</a></li><li><span data-type="index-term">batch applications</span>, <a data-type="index:locator" href="#idm45831163915832">Building Batch Applications Leveraging Model Serving</a></li><li><span data-type="index-term">comparison chart</span>, <a data-type="index:locator" href="#idm45831170660072">Model Inference in Kubeflow</a></li><li><span data-type="index-term">inference</span>, <a data-type="index:locator" href="#ch08-tfs2">TensorFlow Serving</a>-<a data-type="index:locator" href="#idm45831170386840">Review</a></li><li><span data-type="index-term">model monitoring</span>, <a data-type="index:locator" href="#idm45831170373128">Model monitoring</a></li><li><span data-type="index-term">model serving</span>, <a data-type="index:locator" href="#idm45831170380712">Model serving</a></li><li><span data-type="index-term">model updating</span>, <a data-type="index:locator" href="#idm45831170331432">Model updating</a></li><li><span data-type="index-term">recommendation system</span>, <a data-type="index:locator" href="#ch08-tfsrec">TensorFlow Serving</a>-<a data-type="index:locator" href="#idm45831170386168">Review</a></li><li><span data-type="index-term">serverless primitives lacking</span>, <a data-type="index:locator" href="#idm45831168997336">KFServing</a></li><li><span data-type="index-term">TensorFlow Extended integrating with</span>, <a data-type="index:locator" href="#idm45831177053432">Distributed Tooling</a></li></ul></li><li><span data-type="index-term">TensorFlow Transform (TFT)</span><ul><li><span data-type="index-term">feature preparation</span>, <a data-type="index:locator" href="#ch05-tft3">TensorFlow Transform, with TensorFlow Extended on Beam</a>-<a data-type="index:locator" href="#idm45831176533176">TensorFlow Transform, with TensorFlow Extended on Beam</a></li><li><span data-type="index-term">Kubeflow support for</span>, <a data-type="index:locator" href="#idm45831190332792">Data/Feature Preparation</a></li><li><span data-type="index-term">Model Analysis integration</span>, <a data-type="index:locator" href="#idm45831176599704">TensorFlow Transform, with TensorFlow Extended on Beam</a></li></ul></li><li><span data-type="index-term">testing</span><ul><li><span data-type="index-term">Argo installation</span>, <a data-type="index:locator" href="#idm45831178815384">Argo: the Foundation of Pipelines</a></li><li><span data-type="index-term">Docker installation</span>, <a data-type="index:locator" href="#idm45831180905000">Setting up Docker</a></li><li><span data-type="index-term">first project test query</span>, <a data-type="index:locator" href="#idm45831180686376">Test Query</a></li><li><span data-type="index-term">Seldon Core inference model</span>, <a data-type="index:locator" href="#idm45831169973256">Testing Your Model</a><ul><li><span data-type="index-term">local testing with Docker</span>, <a data-type="index:locator" href="#idm45831169888744">Local testing with Docker</a></li><li><span data-type="index-term">Python-wrapped models</span>, <a data-type="index:locator" href="#idm45831169960744">Python client for Python language wrapped models</a></li></ul></li></ul></li><li><span data-type="index-term">TFJob for deployment</span><ul><li><span data-type="index-term">multiworker distributed training</span>, <a data-type="index:locator" href="#idm45831172023640">Distributed Training</a></li><li><span data-type="index-term">specifications</span>, <a data-type="index:locator" href="#idm45831172121272">Deploying a TensorFlow Training Job</a></li><li><span data-type="index-term">TensorFlow recommender</span>, <a data-type="index:locator" href="#ch07-dep5">Deploying a TensorFlow Training Job</a>-<a data-type="index:locator" href="#idm45831172052520">Deploying a TensorFlow Training Job</a></li></ul></li><li><span data-type="index-term">TFServing</span> (<span data-gentext="see">see</span> TensorFlow Serving)</li><li><span data-type="index-term">TFX</span> (<span data-gentext="see">see</span> TensorFlow Extended)</li><li><span data-type="index-term">tfx/Transform component</span>, <a data-type="index:locator" href="#idm45831176564616">TensorFlow Transform, with TensorFlow Extended on Beam</a></li><li><span data-type="index-term">TPU distributed training strategy</span>, <a data-type="index:locator" href="#idm45831172031592">Distributed Training</a></li><li><span data-type="index-term">TPU-accelerated instances</span>, <a data-type="index:locator" href="#idm45831164084824">TPU-Accelerated Instances</a></li><li><span data-type="index-term">tracking data and metadata</span>, <a data-type="index:locator" href="#idm45831180132232">Kubeflow Pipelines</a></li><li><span data-type="index-term">training</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831173245528">Training a Machine Learning Model</a></li><li><span data-type="index-term">deep learning</span>, <a data-type="index:locator" href="#idm45831173234952">Building a Recommender with TensorFlow</a><ul><li><span data-type="index-term">sharing a pipeline</span>, <a data-type="index:locator" href="#idm45831165584904">Sharing the Pipeline</a></li></ul></li><li><span data-type="index-term">distributed training</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831172046344">Distributed Training</a></li><li><span data-type="index-term">data versus model parallelism</span>, <a data-type="index:locator" href="#idm45831172040264">Distributed Training</a></li><li><span data-type="index-term">GPUs for</span>, <a data-type="index:locator" href="#idm45831171945464">Using GPUs</a></li><li><span data-type="index-term">Kubeflow Katib</span>, <a data-type="index:locator" href="#idm45831164221832">Tuning Distributed Training Jobs</a></li><li><span data-type="index-term">MNIST example</span>, <a data-type="index:locator" href="#ch07-dist">Distributed Training</a>-<a data-type="index:locator" href="#idm45831171949608">Distributed Training</a></li><li><span data-type="index-term">other frameworks for</span>, <a data-type="index:locator" href="#idm45831171937144">Using Other Frameworks for Distributed Training</a></li></ul></li><li><span data-type="index-term">first Kubeflow project</span>, <a data-type="index:locator" href="#idm45831180705400">Training and Monitoring Progress</a></li><li><span data-type="index-term">frameworks supported</span>, <a data-type="index:locator" href="#idm45831189954488">Training</a></li><li><span data-type="index-term">impact of using more data</span>, <a data-type="index:locator" href="#idm45831177511736">Data and Feature Preparation</a></li><li><span data-type="index-term">Kubeflow components</span>, <a data-type="index:locator" href="#idm45831180215192">Training Operators</a></li><li><span data-type="index-term">model selection</span>, <a data-type="index:locator" href="#idm45831173156232">Building a Recommender with TensorFlow</a><ul><li><span data-type="index-term">AutoML for</span>, <a data-type="index:locator" href="#idm45831164732312">AutoML: An Overview</a></li></ul></li><li><span data-type="index-term">operators</span>, <a data-type="index:locator" href="#idm45831180206456">Training Operators</a>, <a data-type="index:locator" href="#idm45831170874632">Integration into Pipelines</a></li><li><span data-type="index-term">pipeline integration</span>, <a data-type="index:locator" href="#idm45831170873688">Integration into Pipelines</a></li><li><span data-type="index-term">Scikit-learn</span><ul><li><span data-type="index-term">about</span>, <a data-type="index:locator" href="#idm45831171927416">Training a Model Using Scikit-Learn</a></li><li><span data-type="index-term">about Random Forest</span>, <a data-type="index:locator" href="#idm45831171914408">Training a Model Using Scikit-Learn</a></li><li><span data-type="index-term">data preparation</span>, <a data-type="index:locator" href="#ch07-sk4">Data Preparation</a>-<a data-type="index:locator" href="#idm45831171779112">Data Preparation</a></li><li><span data-type="index-term">explaining the model</span>, <a data-type="index:locator" href="#ch07-exp5">Scikit-Learn Training</a>-<a data-type="index:locator" href="#idm45831170915000">Explaining the Model</a></li><li><span data-type="index-term">exporting the model</span>, <a data-type="index:locator" href="#idm45831170909064">Exporting Model</a></li><li><span data-type="index-term">running Random Forest</span>, <a data-type="index:locator" href="#idm45831171521160">Scikit-Learn Training</a></li><li><span data-type="index-term">training with and evaluation</span>, <a data-type="index:locator" href="#idm45831171478088">Scikit-Learn Training</a></li></ul></li><li><span data-type="index-term">TensorFlow recommender</span><ul><li><span data-type="index-term">about TensorFlow</span>, <a data-type="index:locator" href="#idm45831173240264">Building a Recommender with TensorFlow</a></li><li><span data-type="index-term">creating TensorFlow session</span>, <a data-type="index:locator" href="#idm45831173109144">TensorFlow Training</a></li><li><span data-type="index-term">deployment</span>, <a data-type="index:locator" href="#ch07-dep3">Deploying a TensorFlow Training Job</a>-<a data-type="index:locator" href="#idm45831172053192">Deploying a TensorFlow Training Job</a></li><li><span data-type="index-term">deployment with TFServing</span>, <a data-type="index:locator" href="#ch08-mzzzy">TensorFlow Serving</a>-<a data-type="index:locator" href="#idm45831170382808">Review</a></li><li><span data-type="index-term">exporting model</span>, <a data-type="index:locator" href="#idm45831172638296">TensorFlow Training</a></li><li><span data-type="index-term">hyperparameters</span>, <a data-type="index:locator" href="#idm45831172750088">TensorFlow Training</a></li><li><span data-type="index-term">Keras API</span>, <a data-type="index:locator" href="#idm45831173225064">Building a Recommender with TensorFlow</a></li><li><span data-type="index-term">model selection</span>, <a data-type="index:locator" href="#idm45831173219400">Building a Recommender with TensorFlow</a></li><li><span data-type="index-term">notebook setup</span>, <a data-type="index:locator" href="#ch07-nb3">Building a Recommender with TensorFlow</a>-<a data-type="index:locator" href="#idm45831173121976">Starting a New Notebook Session</a></li><li><span data-type="index-term">running training code</span>, <a data-type="index:locator" href="#idm45831172737416">TensorFlow Training</a></li><li><span data-type="index-term">single-worker jobs</span>, <a data-type="index:locator" href="#idm45831172047560">Distributed Training</a></li></ul></li></ul></li><li><span data-type="index-term">trials (Kubeflow Katib)</span>, <a data-type="index:locator" href="#idm45831164644328">Katib Concepts</a></li></ul></div><div data-type="indexdiv"><h3>U</h3><ul><li><span data-type="index-term">US Census dataset</span><ul><li><span data-type="index-term">about dataset</span>, <a data-type="index:locator" href="#idm45831171918264">Training a Model Using Scikit-Learn</a></li><li><span data-type="index-term">about Random Forest</span>, <a data-type="index:locator" href="#idm45831171913192">Training a Model Using Scikit-Learn</a></li><li><span data-type="index-term">data preparation</span>, <a data-type="index:locator" href="#ch07-sk3">Data Preparation</a>-<a data-type="index:locator" href="#idm45831171779720">Data Preparation</a></li><li><span data-type="index-term">explaining the model</span>, <a data-type="index:locator" href="#ch07-exp4">Scikit-Learn Training</a>-<a data-type="index:locator" href="#idm45831170915672">Explaining the Model</a></li><li><span data-type="index-term">exporting the model</span>, <a data-type="index:locator" href="#idm45831170910040">Exporting Model</a></li><li><span data-type="index-term">income predictor model</span>, <a data-type="index:locator" href="#ch08-incp2">US Census income predictor model example</a>-<a data-type="index:locator" href="#idm45831169086200">US Census income predictor model example</a></li><li><span data-type="index-term">training</span>, <a data-type="index:locator" href="#idm45831171522136">Scikit-Learn Training</a></li><li><span data-type="index-term">training with and evaluation</span>, <a data-type="index:locator" href="#idm45831171479032">Scikit-Learn Training</a></li></ul></li><li><span data-type="index-term">user interfaces (UI)</span><ul><li><span data-type="index-term">Argo UI</span>, <a data-type="index:locator" href="#idm45831178638584">Argo: the Foundation of Pipelines</a><ul><li><span data-type="index-term">installation</span>, <a data-type="index:locator" href="#idm45831178683256">Argo: the Foundation of Pipelines</a></li></ul></li><li><span data-type="index-term">central dashboard</span>, <a data-type="index:locator" href="#idm45831180253352">Getting Around the Central Dashboard</a></li><li><span data-type="index-term">display_schema</span>, <a data-type="index:locator" href="#idm45831176725016">Keeping your data quality: TensorFlow data validation</a></li><li><span data-type="index-term">installation of Kubeflow web UI</span>, <a data-type="index:locator" href="#idm45831180693304">Training and Monitoring Progress</a></li><li><span data-type="index-term">Katib</span>, <a data-type="index:locator" href="#idm45831164246232">Katib User Interface</a></li><li><span data-type="index-term">Kubeflow Pipelines UI</span>, <a data-type="index:locator" href="#idm45831179717512">Exploring the Prepackaged Sample Pipelines</a>, <a data-type="index:locator" href="#idm45831179567048">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">Metadata UI</span>, <a data-type="index:locator" href="#idm45831174103768">Kubeflow Metadata UI</a></li><li><span data-type="index-term">MinIO to explore storage</span>, <a data-type="index:locator" href="#idm45831180008680">MinIO</a></li><li><span data-type="index-term">MLflow (Databricks)</span>, <a data-type="index:locator" href="#idm45831173269240">Using the MLflow UI</a></li></ul></li><li><span data-type="index-term">user of Kubeflow cluster</span>, <a data-type="index:locator" href="#idm45831179760040">Kubeflow Multiuser Isolation</a></li></ul></div><div data-type="indexdiv"><h3>V</h3><ul><li><span data-type="index-term">validation of data</span><ul><li><span data-type="index-term">Apache Spark</span>, <a data-type="index:locator" href="#ch05-sprk2">Spark operators in Kubeflow</a>-<a data-type="index:locator" href="#idm45831175253896">Distributed Feature Preparation Using Apache Spark</a></li><li><span data-type="index-term">TensorFlow Extended</span>, <a data-type="index:locator" href="#ch05-val5">Keeping your data quality: TensorFlow data validation</a>-<a data-type="index:locator" href="#idm45831176607576">Keeping your data quality: TensorFlow data validation</a><ul><li><span data-type="index-term">schema inferred</span>, <a data-type="index:locator" href="#idm45831176814568">Keeping your data quality: TensorFlow data validation</a></li></ul></li></ul></li><li><span data-type="index-term">validation of models</span><ul><li><span data-type="index-term">importance of</span>, <a data-type="index:locator" href="#idm45831180279672">Test Query</a></li><li><span data-type="index-term">Kubeflow support for</span>, <a data-type="index:locator" href="#idm45831189932952">Model Validation</a></li><li><span data-type="index-term">model accuracy</span>, <a data-type="index:locator" href="#idm45831170743144">Model Accuracy, Drift, and Explainability</a></li></ul></li><li><span data-type="index-term">virtual environments in Python</span>, <a data-type="index:locator" href="#idm45831180962440">Setting up the Pipeline SDK</a></li></ul></div><div data-type="indexdiv"><h3>W</h3><ul><li><span data-type="index-term">web UI for Pipeline</span>, <a data-type="index:locator" href="#idm45831179715496">Exploring the Prepackaged Sample Pipelines</a>, <a data-type="index:locator" href="#idm45831179320712">Building a Simple Pipeline in Python</a></li></ul></div><div data-type="indexdiv"><h3>Y</h3><ul><li><span data-type="index-term">YAML</span><ul><li><span data-type="index-term">component options</span>, <a data-type="index:locator" href="#idm45831178030104">Kubeflow Pipeline Components</a></li><li><span data-type="index-term">DSL compiler producing</span>, <a data-type="index:locator" href="#idm45831180164648">Kubeflow Pipelines</a>, <a data-type="index:locator" href="#idm45831179705912">Exploring the Prepackaged Sample Pipelines</a>, <a data-type="index:locator" href="#idm45831179567992">Building a Simple Pipeline in Python</a></li><li><span data-type="index-term">editing</span>, <a data-type="index:locator" href="#idm45831180856504">Editing YAML</a></li><li><span data-type="index-term">KafkaSource to send Knative events</span>, <a data-type="index:locator" href="#idm45831167983592">Knative Eventing</a></li><li><span data-type="index-term">KFServing InferenceService</span>, <a data-type="index:locator" href="#idm45831168511480">Recommender example</a></li><li><span data-type="index-term">resource creation</span>, <a data-type="index:locator" href="#idm45831180223800">Notebooks (JupyterHub)</a></li><li><span data-type="index-term">secrets for MinIO credentials</span>, <a data-type="index:locator" href="#idm45831179907480">MinIO</a></li><li><span data-type="index-term">TensorFlow distributed training job</span>, <a data-type="index:locator" href="#idm45831172013128">Distributed Training</a></li></ul></li></ul></div></div></div></section>
  
<div id="fnPopDiv" style="display: none;"><div class="up-arrow"></div><div class="down-arrow"></div><div class="content"></div></div><script>
          document.addEventListener("mouseover", getFn);

          function getFn(e) {
            const el = e.target;
            let fnPopDiv = document.getElementById('fnPopDiv');
            if([...document.querySelectorAll('a[data-type="noteref"]')].indexOf(el) == -1) {
              if(fnPopDiv.style.display == 'block')
                fnPopDiv.style.display = 'none';
              
              return;
            }

            const rect = el.getBoundingClientRect();
            const fnPopDivUpArrow = fnPopDiv.querySelector('div.up-arrow'),
              fnPopDivDownArrow = fnPopDiv.querySelector('div.down-arrow'),
              fnPopDivBody = fnPopDiv.querySelector('div.content');
            fnPopDiv.style.display = 'block';

            if(el.hash)
              fnPopDivBody.innerHTML = document.getElementById(el.hash.slice(1)).innerHTML;
            else  //not a good error-check
              fnPopDivBody.innerHTML = document.getElementById(el.parentNode.hash.slice(1)).parentNode.innerHTML;
            fnPopDivBody.innerHTML = document.getElementById(hash.slice(1)).innerHTML;
            fnPopDiv.style.top = rect.top - (fnPopDiv.offsetHeight) - 6;
            if(fnPopDiv.style.top < '1' ) {
              fnPopDiv.style.top = rect.top + el.offsetHeight + 6 + 6;
              fnPopDivUpArrow.style.top = rect.top + el.offsetHeight + 6;
              fnPopDivUpArrow.style.left = rect.left;
              fnPopDivUpArrow.style.display = 'block';
              fnPopDivDownArrow.style.display = 'none';
            }
            else {
              fnPopDivDownArrow.style.top = rect.top - 6;
              fnPopDivDownArrow.style.left = rect.left;
              fnPopDivDownArrow.style.display = 'block';
              fnPopDivUpArrow.style.display = 'none';
            }
            
            fnPopDiv.style.left = rect.left + el.offsetWidth/2 - (fnPopDiv.offsetWidth/2);
            if(fnPopDiv.style.left < '1')
              fnPopDiv.style.left = '1';
          }
          </script><div id="menuDiv" style="display: none;"></div><select id="selectList" onchange="updateMenuDiv()"><option value="hide">hide</option><option value="TOC">TOC</option><option value="EQUATION">EQUATION</option><option value="inline_EQ">inline_EQ</option><option value="EXAMPLE">EXAMPLE</option><option value="FIGURE">FIGURE</option><option value="TABLE">TABLE</option></select><div id="menuText_hide" style="display: none;"></div><div id="menuText_TOC" style="display: none;"><hr><p><strong><a href="#_foreword">Foreword</a></strong></p><hr><p><strong><a href="#idm45831190160376">Preface</a></strong></p><p><a href="#assumptions_about_the_audience">_Our Assumption About You</a></p><p><a href="#idm45831190132248">_Your Responsibility as a Practitioner</a></p><p><a href="#idm45831185817160">_Conventions Used in This Book</a></p><p><a href="#idm45831185804568">_Code Examples</a></p><p><a href="#using_code_examples">__Using Code Examples</a></p><p><a href="#idm45831189708680">_O’Reilly Online Learning</a></p><p><a href="#idm45831190065160">_How to Contact the Authors</a></p><p><a href="#idm45831190089432">_How to Contact Us</a></p><p><a href="#idm45831190075080">_Acknowledgments</a></p><p><a href="#idm45831188267096">_Grievances</a></p><hr><p><strong><a href="#who_is_kubeflow_for_ch">1. Kubeflow: What It Is and Who It Is For</a></strong></p><p><a href="#idm45831188258120">_1.1. Model Development Life Cycle</a></p><p><a href="#idm45831188251080">_1.2. Where Does Kubeflow Fit In?</a></p><p><a href="#idm45831188248120">_1.3. Why Containerize?</a></p><p><a href="#idm45831190371288">_1.4. Why Kubernetes?</a></p><p><a href="#breif_kubeflow_design_and_core_components">_1.5. Kubeflow’s Design and Core Components</a></p><p><a href="#idm45831190346120">__1.5.1. Data Exploration with Notebooks</a></p><p><a href="#idm45831190337608">__1.5.2. Data/Feature Preparation</a></p><p><a href="#idm45831189956888">__1.5.3. Training</a></p><p><a href="#idm45831189941304">__1.5.4. Hyperparameter Tuning</a></p><p><a href="#idm45831189935368">__1.5.5. Model Validation</a></p><p><a href="#idm45831189931352">__1.5.6. Inference/Prediction</a></p><p><a href="#idm45831189924360">__1.5.7. Pipelines</a></p><p><a href="#idm45831189915032">__1.5.8. Component Overview</a></p><p><a href="#brief_alternatives_to_kubeflow">_1.6. Alternatives to Kubeflow</a></p><p><a href="#idm45831189906328">__1.6.1. Clipper (RiseLabs)</a></p><p><a href="#idm45831189900296">__1.6.2. MLflow (Databricks)</a></p><p><a href="#idm45831189893464">__1.6.3. Others</a></p><p><a href="#case_studies">_1.7. Introducing Our Case Studies</a></p><p><a href="#idm45831185502792">__1.7.1. Modified National Institute of Standards and Technology</a></p><p><a href="#idm45831185497768">__1.7.2. Mailing List Data</a></p><p><a href="#idm45831185493544">__1.7.3. Product Recommender</a></p><p><a href="#idm45831185483304">__1.7.4. CT Scans</a></p><p><a href="#idm45831185475704">_1.8. Conclusion</a></p><hr><p><strong><a href="#simple_training_ch">2. Hello Kubeflow</a></strong></p><p><a href="#idm45831185459048">_2.1. Getting Set Up with Kubeflow</a></p><p><a href="#general_set_up_kubeflow">__2.1.1. Installing Kubeflow and Its Dependencies</a></p><p><a href="#idm45831185446424">__2.1.2. Setting Up Local Kubernetes</a></p><p><a href="#setting_up_minikube">___2.1.2.1. Minikube</a></p><p><a href="#idm45831180970840">__2.1.3. Setting Up Your Kubeflow Development Environment</a></p><p><a href="#install_pipeline_sdk">___2.1.3.1. Setting up the Pipeline SDK</a></p><p><a href="#idm45831180966072">___2.1.3.2. Setting up Docker</a></p><p><a href="#idm45831180857848">___2.1.3.3. Editing YAML</a></p><p><a href="#idm45831180817352">__2.1.4. Creating Our First Kubeflow Project</a></p><p><a href="#model_management">_2.2. Training and Deploying a Model</a></p><p><a href="#idm45831180707320">__2.2.1. Training and Monitoring Progress</a></p><p><a href="#idm45831180706696">__2.2.2. Test Query</a></p><p><a href="#idm45831180687624">_2.3. Going Beyond a Local Deployment</a></p><p><a href="#idm45831180266184">_2.4. Conclusion</a></p><hr><p><strong><a href="#kubeflow_design_beyond_basics">3. Kubeflow Design: Beyond the Basics</a></strong></p><p><a href="#idm45831180257240">_3.1. Getting Around the Central Dashboard</a></p><p><a href="#idm45831180244680">__3.1.1. Notebooks (JupyterHub)</a></p><p><a href="#idm45831180244056">__3.1.2. Training Operators</a></p><p><a href="#idm45831180184408">__3.1.3. Kubeflow Pipelines</a></p><p><a href="#idm45831180129288">__3.1.4. Hyperparameter Tuning</a></p><p><a href="#idm45831180092056">__3.1.5. Model Inference</a></p><p><a href="#metadata_manage">__3.1.6. Metadata</a></p><p><a href="#idm45831180023656">__3.1.7. Component Summary</a></p><p><a href="#Supp_components">_3.2. Support Components</a></p><p><a href="#minio_walkthrough">__3.2.1. MinIO</a></p><p><a href="#idm45831179859864">__3.2.2. Istio</a></p><p><a href="#knative_serving">__3.2.3. Knative</a></p><p><a href="#idm45831179773960">__3.2.4. Apache Spark</a></p><p><a href="#idm45831179768328">__3.2.5. Kubeflow Multiuser Isolation</a></p><p><a href="#idm45831179738104">_3.3. Conclusion</a></p><hr><p><strong><a href="#pipelines_ch">4. Kubeflow Pipelines</a></strong></p><p><a href="#idm45831179727224">_4.1. Getting Started with Pipelines</a></p><p><a href="#idm45831179720536">__4.1.1. Exploring the Prepackaged Sample Pipelines</a></p><p><a href="#idm45831179719912">__4.1.2. Building a Simple Pipeline in Python</a></p><p><a href="#idm45831179053480">__4.1.3. Storing Data Between Steps</a></p><p><a href="#idm45831179002392">_4.2. Introduction to Kubeflow Pipelines Components</a></p><p><a href="#argo_foundation">__4.2.1. Argo: the Foundation of Pipelines</a></p><p><a href="#idm45831178832232">__4.2.2. What Kubeflow Pipelines Adds to Argo Workflow</a></p><p><a href="#idm45831178515528">__4.2.3. Building a Pipeline Using Existing Images</a></p><p><a href="#idm45831178514936">__4.2.4. Kubeflow Pipeline Components</a></p><p><a href="#idm45831178085656">_4.3. Advanced Topics in Pipelines</a></p><p><a href="#idm45831178017224">__4.3.1. Conditional Execution of Pipeline Stages</a></p><p><a href="#run_pipelines_onsched">__4.3.2. Running Pipelines on Schedule</a></p><p><a href="#idm45831177520696">_4.4. Conclusion</a></p><hr><p><strong><a href="#data_and_feature_prep">5. Data and Feature Preparation</a></strong></p><p><a href="#idm45831177498712">_5.1. Deciding on the Correct Tooling</a></p><p><a href="#idm45831177491000">_5.2. Local Data and Feature Preparation</a></p><p><a href="#idm45831177484984">__5.2.1. Fetching the Data</a></p><p><a href="#idm45831177358696">__5.2.2. Data Cleaning: Filtering Out the Junk</a></p><p><a href="#idm45831177358232">__5.2.3. Formatting the Data</a></p><p><a href="#idm45831177086712">__5.2.4. Feature Preparation</a></p><p><a href="#idm45831177076856">__5.2.5. Custom Containers</a></p><p><a href="#idm45831177062072">_5.3. Distributed Tooling</a></p><p><a href="#idm45831177051880">__5.3.1. TensorFlow Extended</a></p><p><a href="#validating_data_sec">___5.3.1.1. Keeping your data quality: TensorFlow data validation</a></p><p><a href="#idm45831177005000">___5.3.1.2. TensorFlow Transform, with TensorFlow Extended on Beam</a></p><p><a href="#idm45831176605848">__5.3.2. Distributed Data Using Apache Spark</a></p><p><a href="#idm45831176271832">___5.3.2.1. Spark operators in Kubeflow</a></p><p><a href="#idm45831176271368">___5.3.2.2. Reading the input data</a></p><p><a href="#idm45831175747768">___5.3.2.3. Validating the schema</a></p><p><a href="#idm45831175665256">___5.3.2.4. Handling missing fields</a></p><p><a href="#idm45831175656136">___5.3.2.5. Filtering out bad data</a></p><p><a href="#idm45831175523048">___5.3.2.6. Saving the output</a></p><p><a href="#idm45831176531272">__5.3.3. Distributed Feature Preparation Using Apache Spark</a></p><p><a href="#putting_it_in_a_pipeline">_5.4. Putting It Together in a Pipeline</a></p><p><a href="#notebook_as_pipeline_stage">_5.5. Using an Entire Notebook as a Data Preparation 
Pipeline Stage</a></p><p><a href="#idm45831174935048">_5.6. Conclusion</a></p><hr><p><strong><a href="#artifact_store">6. Artifact and Metadata Store</a></strong></p><p><a href="#idm45831174773496">_6.1. Kubeflow ML Metadata</a></p><p><a href="#idm45831174347464">__6.1.1. Programmatic Query</a></p><p><a href="#idm45831174346840">__6.1.2. Kubeflow Metadata UI</a></p><p><a href="#idm45831174772872">_6.2. Using MLflow’s Metadata Tools with Kubeflow</a></p><p><a href="#idm45831174018360">__6.2.1. Creating and Deploying an MLflow Tracking Server</a></p><p><a href="#idm45831174017736">__6.2.2. Logging Data on Runs</a></p><p><a href="#idm45831173891512">__6.2.3. Using the MLflow UI</a></p><p><a href="#idm45831174087512">_6.3. Conclusion</a></p><hr><p><strong><a href="#tf_ch">7. Training a Machine Learning Model</a></strong></p><p><a href="#recommender_example">_7.1. Building a Recommender with TensorFlow</a></p><p><a href="#idm45831173141736">__7.1.1. Getting Started</a></p><p><a href="#idm45831173119752">__7.1.2. Starting a New Notebook Session</a></p><p><a href="#idm45831173112424">__7.1.3. TensorFlow Training</a></p><p><a href="#idm45831173241944">_7.2. Deploying a TensorFlow Training Job</a></p><p><a href="#idm45831172389080">_7.3. Distributed Training</a></p><p><a href="#idm45831171946760">__7.3.1. Using GPUs</a></p><p><a href="#idm45831171938760">__7.3.2. Using Other Frameworks for Distributed Training</a></p><p><a href="#model_building_income">_7.4. Training a Model Using Scikit-Learn</a></p><p><a href="#idm45831171909256">__7.4.1. Starting a New Notebook Session</a></p><p><a href="#idm45831171904680">__7.4.2. Data Preparation</a></p><p><a href="#idm45831171904056">__7.4.3. Scikit-Learn Training</a></p><p><a href="#idm45831171522952">__7.4.4. Explaining the Model</a></p><p><a href="#idm45831171410200">__7.4.5. Exporting Model</a></p><p><a href="#idm45831170876680">__7.4.6. Integration into Pipelines</a></p><p><a href="#idm45831171928792">_7.5. Conclusion</a></p><hr><p><strong><a href="#inference_ch">8. Model Inference</a></strong></p><p><a href="#idm45831170854472">_8.1. Model Serving</a></p><p><a href="#idm45831170762584">__8.1.1. Model Serving Requirements</a></p><p><a href="#Model_Monitor">_8.2. Model Monitoring</a></p><p><a href="#idm45831170746008">__8.2.1. Model Accuracy, Drift, and Explainability</a></p><p><a href="#idm45831170725448">__8.2.2. Model Monitoring Requirements</a></p><p><a href="#idm45831170710472">_8.3. Model Updating</a></p><p><a href="#idm45831170687224">__8.3.1. Model Updating Requirements</a></p><p><a href="#idm45831170678696">_8.4. Summary of Inference Requirements</a></p><p><a href="#inference_in_kubeflow">_8.5. Model Inference in Kubeflow</a></p><p><a href="#idm45831170633000">_8.6. TensorFlow Serving</a></p><p><a href="#idm45831170390264">__8.6.1. Review</a></p><p><a href="#idm45831170382008">___8.6.1.1. Model serving</a></p><p><a href="#idm45831170374648">___8.6.1.2. Model monitoring</a></p><p><a href="#idm45831170335096">___8.6.1.3. Model updating</a></p><p><a href="#idm45831170324728">___8.6.1.4. Summary</a></p><p><a href="#idm45831170632088">_8.7. Seldon Core</a></p><p><a href="#idm45831170297032">__8.7.1. Designing a Seldon Inference Graph</a></p><p><a href="#idm45831170286872">___8.7.1.1. Setting up Seldon Core</a></p><p><a href="#package_your_model">___8.7.1.2. Packaging your model</a></p><p><a href="#idm45831170168648">___8.7.1.3. Creating a SeldonDeployment</a></p><p><a href="#idm45831170296568">__8.7.2. Testing Your Model</a></p><p><a href="#idm45831169964632">___8.7.2.1. Python client for Python language wrapped models</a></p><p><a href="#idm45831169890504">___8.7.2.2. Local testing with Docker</a></p><p><a href="#idm45831169779576">__8.7.3. Serving Requests</a></p><p><a href="#Monitor_yr_Models">__8.7.4. Monitoring Your Models</a></p><p><a href="#idm45831169627560">___8.7.4.1. Model explainability</a></p><p><a href="#idm45831169589976">___8.7.4.2. Sentiment prediction model</a></p><p><a href="#idm45831169589384">___8.7.4.3. US Census income predictor model example</a></p><p><a href="#idm45831169350744">___8.7.4.4. Outlier and drift detection</a></p><p><a href="#idm45831169038520">__8.7.5. Review</a></p><p><a href="#idm45831169033784">___8.7.5.1. Model serving</a></p><p><a href="#idm45831169025960">___8.7.5.2. Model monitoring</a></p><p><a href="#idm45831169016424">___8.7.5.3. Model updating</a></p><p><a href="#idm45831169007560">___8.7.5.4. Summary</a></p><p><a href="#idm45831170322648">_8.8. KFServing</a></p><p><a href="#serverless_service_plane">__8.8.1. Serverless and the Service Plane</a></p><p><a href="#idm45831168975320">__8.8.2. Data Plane</a></p><p><a href="#idm45831168916472">__8.8.3. Example Walkthrough</a></p><p><a href="#idm45831168914760">___8.8.3.1. Setting up KFServing</a></p><p><a href="#idm45831168881912">___8.8.3.2. Simplicity and extensibility</a></p><p><a href="#idm45831168880968">___8.8.3.3. Recommender example</a></p><p><a href="#idm45831168428248">__8.8.4. Peeling Back the Underlying Infrastructure</a></p><p><a href="#idm45831168244680">___8.8.4.1. Going layer by layer</a></p><p><a href="#escape_hatches">___8.8.4.2. Escape hatches</a></p><p><a href="#idm45831168211608">___8.8.4.3. Debugging an InferenceService</a></p><p><a href="#idm45831168055256">___8.8.4.4. Debugging performance</a></p><p><a href="#knative_eventing">___8.8.4.5. Knative Eventing</a></p><p><a href="#idm45831167937496">___8.8.4.6. Additional features</a></p><p><a href="#idm45831167933064">___8.8.4.7. API documentation</a></p><p><a href="#idm45831168277112">__8.8.5. Review</a></p><p><a href="#idm45831167922840">___8.8.5.1. Model serving</a></p><p><a href="#idm45831167912776">___8.8.5.2. Model monitoring</a></p><p><a href="#idm45831167900824">___8.8.5.3. Model updating</a></p><p><a href="#idm45831167850008">___8.8.5.4. Summary</a></p><p><a href="#idm45831167846728">_8.9. Conclusion</a></p><hr><p><strong><a href="#beyond_tf">9. Case Study Using Multiple Tools</a></strong></p><p><a href="#idm45831167818232">_9.1. The Denoising CT Scans Example</a></p><p><a href="#idm45831167799352">__9.1.1. Data Prep with Python</a></p><p><a href="#idm45831167238376">__9.1.2. DS-SVD with Apache Spark</a></p><p><a href="#idm45831166298424">__9.1.3. Visualization</a></p><p><a href="#idm45831166262136">___9.1.3.1. Downloading DRMs</a></p><p><a href="#idm45831166143656">___9.1.3.2. Recomposing the matrix into denoised images</a></p><p><a href="#idm45831165818536">__9.1.4. The CT Scan Denoising Pipeline</a></p><p><a href="#idm45831165807720">___9.1.4.1. Spark operation manifest</a></p><p><a href="#idm45831165249480">___9.1.4.2. The pipeline</a></p><p><a href="#idm45831165248760">_9.2. Sharing the Pipeline</a></p><p><a href="#idm45831165397624">_9.3. Conclusion</a></p><hr><p><strong><a href="#hyperparameter_tuning">10. Hyperparameter Tuning and Automated 
Machine Learning</a></strong></p><p><a href="#idm45831165224824">_10.1. AutoML: An Overview</a></p><p><a href="#idm45831164903512">_10.2. Hyperparameter Tuning with Kubeflow Katib</a></p><p><a href="#idm45831164950312">_10.3. Katib Concepts</a></p><p><a href="#idm45831164949912">_10.4. Installing Katib</a></p><p><a href="#idm45831164484120">_10.5. Running Your First Katib Experiment</a></p><p><a href="#idm45831164461000">__10.5.1. Prepping Your Training Code</a></p><p><a href="#idm45831164379624">__10.5.2. Configuring an Experiment</a></p><p><a href="#idm45831164317832">__10.5.3. Running the Experiment</a></p><p><a href="#idm45831164249624">__10.5.4. Katib User Interface</a></p><p><a href="#idm45831164249128">_10.6. Tuning Distributed Training Jobs</a></p><p><a href="#idm45831164188376">_10.7. Neural Architecture Search</a></p><p><a href="#idm45831164142824">_10.8. Advantages of Katib over Other Frameworks</a></p><p><a href="#idm45831164126824">_10.9. Conclusion</a></p><hr><p><strong><a href="#appendix_executors">A. Argo Executor Configurations and Trade-Offs</a></strong></p><hr><p><strong><a href="#appendix_cloud_specific">B. Cloud-Specific Tools and Configuration</a></strong></p><p><a href="#google_cloud_specific">_B.1. Google Cloud</a></p><p><a href="#idm45831164086808">__B.1.1. TPU-Accelerated Instances</a></p><p><a href="#idm45831164080920">__B.1.2. Dataflow for TFX</a></p><hr><p><strong><a href="#Model_inference_appendix">C. Using Model Serving in Applications</a></strong></p><p><a href="#idm45831164010408">_C.1. Building Streaming Applications Leveraging 
Model Serving</a></p><p><a href="#idm45831164004936">__C.1.1. Stream Processing Engines and Libraries</a></p><p><a href="#idm45831164004344">__C.1.2. Introducing Cloudflow</a></p><p><a href="#idm45831163920808">_C.2. Building Batch Applications Leveraging Model Serving</a></p><hr><p><strong><a href="#idm45831163894392">Index</a></strong></p></div><div id="menuText_EQUATION" style="display: none;"></div><div id="menuText_inline_EQ" style="display: none;"></div><div id="menuText_EXAMPLE" style="display: none;"><p><a href="#install_kubectl_snap">Example 2-1. Install kubectl with snap</a></p><p><a href="#install_kubectl_homebrew">Example 2-2. Install kubectl with Homebrew</a></p><p><a href="#install_kf">Example 2-3. Install Kubeflow</a></p><p><a href="#make_venv">Example 2-4. Create a virtual environment</a></p><p><a href="#install_sdk">Example 2-5. Install Kubeflow Pipeline SDK</a></p><p><a href="#install_kf_pl_sdk">Example 2-6. Clone the Kubeflow Pipelines repo</a></p><p><a href="#trivial_docker">Example 2-7. Specify the new container is built on top of Kubeflow’s container</a></p><p><a href="#trivial_build_and_push">Example 2-8. Build the new container and push to a registry for use</a></p><p><a href="#create_example_project">Example 2-9. Create first example project</a></p><p><a href="#create_training_workflow">Example 2-10. Create training workflow example</a></p><p><a href="#model_serving">Example 2-11. Model query example</a></p><p><a href="#untitled_programlisting_1">untitled_programlisting_1</a></p><p><a href="#untitled_programlisting_2">untitled_programlisting_2</a></p><p><a href="#ex_minio_fwd">Example 3-1. Setting up port-forwarding</a></p><p><a href="#minio_homebrew">Example 3-2. Install MinIO on Mac</a></p><p><a href="#minio_linux">Example 3-3. Install MinIO on Linux</a></p><p><a href="#minio_endpoint">Example 3-4. Configure MinIO client to talk to Kubeflow’s MinIO</a></p><p><a href="#minio_bucket">Example 3-5. Create a bucket with MinIO</a></p><p><a href="#minio_secret">Example 3-6. Sample MinIO secret</a></p><p><a href="#untitled_programlisting_3">untitled_programlisting_3</a></p><p><a href="#untitled_programlisting_4">untitled_programlisting_4</a></p><p><a href="#untitled_programlisting_5">untitled_programlisting_5</a></p><p><a href="#untitled_programlisting_6">untitled_programlisting_6</a></p><p><a href="#simple_python_function">Example 4-1. A simple Python function</a></p><p><a href="#less_simple_pyfunction">Example 4-2. A less-simple Python function</a></p><p><a href="#simple_pipeline">Example 4-3. A simple pipeline</a></p><p><a href="#untitled_programlisting_7">untitled_programlisting_7</a></p><p><a href="#untitled_programlisting_8">untitled_programlisting_8</a></p><p><a href="#untitled_programlisting_9">untitled_programlisting_9</a></p><p><a href="#make_volume_ch4">Example 4-4. Mailing list data prep</a></p><p><a href="#file_output_ex">Example 4-5. File output example</a></p><p><a href="#argo_dl_linux">Example 4-6. Argo installation</a></p><p><a href="#List_Argo_executions">Example 4-7. Listing Argo executions</a></p><p><a href="#Get_Argo_execution_details">Example 4-8. Getting Argo execution details</a></p><p><a href="#Get_log_Argo_execution">Example 4-9. Getting the log of Argo execution</a></p><p><a href="#Argo_execution_log">Example 4-10. Argo execution log</a></p><p><a href="#Deleting_Argo_execution">Example 4-11. Deleting Argo execution</a></p><p><a href="#untitled_programlisting_10">untitled_programlisting_10</a></p><p><a href="#Export_kubernetes_cli">Example 4-12. Exporting Kubernetes client</a></p><p><a href="#Obtain_pipeline_exper">Example 4-13. Obtaining pipeline experiment</a></p><p><a href="#ex_rec_pipeline">Example 4-14. Example recommender pipeline</a></p><p><a href="#dl_pipeline_release">Example 4-15. Pipeline release</a></p><p><a href="#ex_load_gcs">Example 4-16. Load GCS download component</a></p><p><a href="#ex_dl_gcs">Example 4-17. Loading pipeline storage component from relative path and web link</a></p><p><a href="#Import_req_components">Example 4-18. Importing required components</a></p><p><a href="#Functions_implement">Example 4-19. Functions implementation</a></p><p><a href="#Pipeline_implement">Example 4-20. Pipeline implementation</a></p><p><a href="#scrape_mailing_list">Example 5-1. Downloading the mailing list data</a></p><p><a href="#clean_single_machine_ml">Example 5-2. Data cleaning</a></p><p><a href="#install_spamassassin">Example 5-3. Installing SpamAssassin</a></p><p><a href="#local_mailing_list_feature_prep_fun">Example 5-4. Writing and combining text-processing functions into features</a></p><p><a href="#install_tfx">Example 5-5. Installing TFX and TFDV</a></p><p><a href="#load_tfdv_components">Example 5-6. Loading the components</a></p><p><a href="#dl_recommender_data">Example 5-7. Download recommender data</a></p><p><a href="#use_csv_examples">Example 5-8. Using CSV component</a></p><p><a href="#make_schema">Example 5-9. Creating the schema</a></p><p><a href="#import_tfdv">Example 5-10. Download the schema locally</a></p><p><a href="#display_tfdv_schema">Example 5-11. Display the schema</a></p><p><a href="#tfdv_validate">Example 5-12. Validating the data</a></p><p><a href="#tft_imports">Example 5-13. TFT imports</a></p><p><a href="#tft_entry">Example 5-14. Creating the entry point</a></p><p><a href="#tft_logic">Example 5-15. Compute the vocabulary</a></p><p><a href="#use_tft">Example 5-16. Using the TFT component</a></p><p><a href="#add_spark_to_nb_image">Example 5-17. Adding Spark</a></p><p><a href="#sample_spark_nb_service">Example 5-18. Sample service definition</a></p><p><a href="#install_pyapp_and_deps">Example 5-19. Installing requirements and copying the application</a></p><p><a href="#launch_spark_with_operator">Example 5-20. Using the ResourceOp to launch a Spark job</a></p><p><a href="#launch_spark_with_operator_2">Example 5-21. Launching your Spark session</a></p><p><a href="#spark_nb_configs">Example 5-22. Configuring your Spark session</a></p><p><a href="#install_python_36_spark">Example 5-23. Installing Python 3.6 in Spark’s worker container</a></p><p><a href="#configure_spark_minio">Example 5-24. Configuring Spark to use MinIO</a></p><p><a href="#ex_load_parquet">Example 5-25. Reading our data’s Parquet-formatted output</a></p><p><a href="#ex_load_parquet_schema">Example 5-26. Specifying the schema</a></p><p><a href="#drop_na_spark">Example 5-27. Dropping records</a></p><p><a href="#filter_junk_spark">Example 5-28. Filtering out bad data</a></p><p><a href="#use_spark_sql">Example 5-29. Using Spark SQL</a></p><p><a href="#spark_to_pandas">Example 5-30. Saving to a persistent volume</a></p><p><a href="#write_big_data">Example 5-31. Writing to Parquet</a></p><p><a href="#ex_spark_feature_prep">Example 5-32. Preparing features for the mailing list</a></p><p><a href="#single_machine_pipeline">Example 5-33. Putting the functions together</a></p><p><a href="#Instal_Scikit_learn">Example 5-34. Installing Scikit-learn</a></p><p><a href="#use_spamassassin">Example 5-35. Specifying a container</a></p><p><a href="#dockerfile_run_nb">Example 5-36. Using an entire notebook as data preparation</a></p><p><a href="#add_py_deps_nb">Example 5-37. Using RUN to add Python dependencies to the container</a></p><p><a href="#required_imports">Example 6-1. Required imports</a></p><p><a href="#define_workspace">Example 6-2. Define a workspace</a></p><p><a href="#metadata_example">Example 6-3. Metadata example</a></p><p><a href="#metadata_example2">Example 6-4. Another metadata example</a></p><p><a href="#list_models">Example 6-5. List all models</a></p><p><a href="#basic_lineage">Example 6-6. Basic lineage</a></p><p><a href="#find_execution">Example 6-7. Find the execution</a></p><p><a href="#getting_events">Example 6-8. Getting all related events</a></p><p><a href="#mlflow_docker">Example 6-9. MLflow Tracking Server</a></p><p><a href="#mlflow_startup">Example 6-10. MLflow startup script</a></p><p><a href="#install_required1">Example 6-11. Installing MLflow server with Helm</a></p><p><a href="#install_required2">Example 6-12. Install required</a></p><p><a href="#import_required">Example 6-13. Import required libraries</a></p><p><a href="#set_env">Example 6-14. Set environment variables</a></p><p><a href="#create_experiment">Example 6-15. Create experiment</a></p><p><a href="#knn_model">Example 6-16. Sample KNN model</a></p><p><a href="#getting_runs">Example 6-17. Getting the runs for a given experiment</a></p><p><a href="#set_up_prerequisites">Example 7-1. Setting up prerequisites</a></p><p><a href="#creating_tensorflow_session">Example 7-2. Creating a TensorFlow session</a></p><p><a href="#DeepCollaborativeFiltering_learning">Example 7-3. DeepCollaborativeFiltering learning</a></p><p><a href="#Model_creation_ex">Example 7-4. Model creation</a></p><p><a href="#Settg_Training_config">Example 7-5. Setting Training configuration</a></p><p><a href="#Fitting_model_ex">Example 7-6. Fitting model</a></p><p><a href="#Model_training_results_ex">Example 7-7. Model training results</a></p><p><a href="#Setting_export_dest">Example 7-8. Setting export destination</a></p><p><a href="#Exporting_model_example">Example 7-9. Exporting the model</a></p><p><a href="#TFJob_Dockerfi">Example 7-10. TFJob Dockerfile</a></p><p><a href="#untitled_programlisting_11">untitled_programlisting_11</a></p><p><a href="#TFJob_exam">Example 7-11. Single-container TFJob example</a></p><p><a href="#Depl_TFJob">Example 7-12. Deploying TFJob</a></p><p><a href="#View_state_TFJob">Example 7-13. Viewing the state of TFJob</a></p><p><a href="#TF_Recommender_job_descr_ex">Example 7-14. TF Recommender job description</a></p><p><a href="#distrib_TFJob">Example 7-15. Distributed TFJob example</a></p><p><a href="#untitled_programlisting_12">untitled_programlisting_12</a></p><p><a href="#untitled_programlisting_13">untitled_programlisting_13</a></p><p><a href="#TFJob_execution_result_ex">Example 7-16. TFJob execution result</a></p><p><a href="#TFJob_w_GPU_ex">Example 7-17. TFJob with GPU example</a></p><p><a href="#pytorch_distrib">Example 7-18. Pytorch Distributed Training Example</a></p><p><a href="#Feature_pre_Ex">Example 7-19. Feature preparation</a></p><p><a href="#Comb_cols_using_col_transf">Example 7-20. Combining columns using column transformer</a></p><p><a href="#Data_transform_ex">Example 7-21. Data transformer</a></p><p><a href="#Using_RandomForestClass_ex">Example 7-22. Using RandomForestClassifier</a></p><p><a href="#Eval_train_results">Example 7-23. Evaluating training results</a></p><p><a href="#Train_results_ex">Example 7-24. Training results</a></p><p><a href="#Defining_Tab_Anchor">Example 7-25. Defining the tabular anchor</a></p><p><a href="#Tab_Anchor_ex">Example 7-26. Tabular anchor</a></p><p><a href="#Prediction_calc_ex">Example 7-27. Prediction calculation</a></p><p><a href="#Prediction_calc_result_ex">Example 7-28. Prediction calculation result</a></p><p><a href="#Model_expl_ex">Example 7-29. Model explanation</a></p><p><a href="#Model_expl_results_ex">Example 7-30. Model explanation result</a></p><p><a href="#Model_expl_ex2">Example 7-31. Model explanation</a></p><p><a href="#Model_expl_results_ex2">Example 7-32. Model explanation result</a></p><p><a href="#Exporting_model_ex">Example 7-33. Exporting model</a></p><p><a href="#untitled_programlisting_14">untitled_programlisting_14</a></p><p><a href="#Portforward_TFServ_servs">Example 8-1. Port-forwarding TFServing services</a></p><p><a href="#untitled_programlisting_15">untitled_programlisting_15</a></p><p><a href="#TFServRec_model_version_status">Example 8-2. TFServing Recommender model version status</a></p><p><a href="#untitled_programlisting_16">untitled_programlisting_16</a></p><p><a href="#req_TFServ_Rec_service">Example 8-3. Sending a request to your TFServing Recommender service</a></p><p><a href="#Output_fr_TFServ_Rec_service">Example 8-4. Output from your TFServing Recommender service</a></p><p><a href="#Helm_install_cust_SeldonCore_vers">Example 8-5. Helm install for a custom Seldon Core version</a></p><p><a href="#untitled_programlisting_17">untitled_programlisting_17</a></p><p><a href="#Seldon_Core_Istio_Gateway">Example 8-6. Seldon Core Istio Gateway</a></p><p><a href="#Simple_Seldon_Core_prep_model_server">Example 8-7. Simple Seldon Core prepackaged model server</a></p><p><a href="#Simple_Seldon_Core_cust_lang_wrapper">Example 8-8. Simple Seldon Core custom language wrapper</a></p><p><a href="#Seldon_Core_Py_model_class">Example 8-9. Seldon Core Python model class</a></p><p><a href="#untitled_programlisting_18">untitled_programlisting_18</a></p><p><a href="#Send_req_to_SeldonCore_custom_microservice">Example 8-10. Sending a request to your Seldon Core custom microservice</a></p><p><a href="#Exp_Seldon_Core_micros_loc_Docker_client">Example 8-11. Exposing Seldon Core microservice in a local Docker client</a></p><p><a href="#Send_req_local_Seldon_Core_micros">Example 8-12. Sending a request to your local Seldon Core microservice</a></p><p><a href="#untitled_programlisting_19">untitled_programlisting_19</a></p><p><a href="#SeldonMessage_contain_ndarray">Example 8-13. SeldonMessage containing an ndarray</a></p><p><a href="#SeldonMessage_containing_JSON_data">Example 8-14. SeldonMessage containing JSON data</a></p><p><a href="#SeldonDeployment_wAnchor_Expl">Example 8-15. SeldonDeployment with Anchor Explainers</a></p><p><a href="#Send_pred_request_to_SeldonCore_moviesent">Example 8-16. Sending a prediction request to your Seldon Core movie sentiment model</a></p><p><a href="#Pred_resp_frSeldonCore_moviesentiment">Example 8-17. Prediction response from your Seldon Core movie sentiment model</a></p><p><a href="#expl_req_to_SeldonC_moviesent">Example 8-18. Sending an explanation request to your Seldon Core movie sentiment model</a></p><p><a href="#Expl_resp_fr_SeldonC_moviesent">Example 8-19. Explanation response from your Seldon Core movie sentiment model</a></p><p><a href="#SeldonDeployment_inc_pred">Example 8-20. SeldonDeployment for income predictor</a></p><p><a href="#Send_pred_req_SeldonC_inc_pred_model">Example 8-21. Sending a prediction request to your Seldon Core income predictor model</a></p><p><a href="#Pred_resp_SeldonC_inc_pred_model">Example 8-22. Prediction response from your Seldon Core income predictor model</a></p><p><a href="#Send_expl_req_to_SeldonC_incpredmod">Example 8-23. Sending a explanation request to your Seldon Core income predictor model</a></p><p><a href="#Explanation_response_fr_SeldonCore_income_predictor">Example 8-24. Explanation response from your Seldon Core income predictor model</a></p><p><a href="#untitled_programlisting_20">untitled_programlisting_20</a></p><p><a href="#untitled_programlisting_21">untitled_programlisting_21</a></p><p><a href="#untitled_programlisting_22">untitled_programlisting_22</a></p><p><a href="#untitled_programlisting_23">untitled_programlisting_23</a></p><p><a href="#untitled_programlisting_24">untitled_programlisting_24</a></p><p><a href="#sklearn_ex">Example 8-25. Simple sklearn KFServing InferenceService</a></p><p><a href="#tensorflow_ex">Example 8-26. Simple TensorFlow KFServing InferenceService</a></p><p><a href="#pytorch_example">Example 8-27. Simple PyTorch KFServing InferenceService</a></p><p><a href="#Soph_Canary_KFServing_InferenceServ">Example 8-28. Sophisticated Canary KFServing InferenceService</a></p><p><a href="#KFServing_ann_Minio_secret">Example 8-29. KFServing-annotated MinIO secret</a></p><p><a href="#Serv_Acc_w_attMinio_secret">Example 8-30. Service Account with attached MinIO secret</a></p><p><a href="#KFServing_Recommender_InfServ">Example 8-31. KFServing Recommender InferenceService</a></p><p><a href="#untitled_programlisting_25">untitled_programlisting_25</a></p><p><a href="#SendpredreqtoKFS_RecInfServ">Example 8-32. Sending a prediction request to your KFServing Recommender InferenceService</a></p><p><a href="#untitled_programlisting_26">untitled_programlisting_26</a></p><p><a href="#target_concurrency_annos_KFSInfServ">Example 8-33. Custom target concurrency via annotations in KFServing InferenceService</a></p><p><a href="#untitled_programlisting_27">untitled_programlisting_27</a></p><p><a href="#KafkaS_sends_events_KFS_Rec_InfServ">Example 8-34. KafkaSource that sends events to a KFServing Recommender InferenceService</a></p><p><a href="#Ltwt_Pythonf_converts_DICOMs">Example 9-1. Lightweight Python function converts DICOMs to tensors</a></p><p><a href="#decompose_scala_mahout">Example 9-2. Decomposing a CT scan with Spark and Mahout</a></p><p><a href="#download_dir_helper">Example 9-3. Helper function to download a directory from GCS</a></p><p><a href="#mahout_helper">Example 9-4. Helper function to read Mahout DRMs into NumPy matrices</a></p><p><a href="#image_writer">Example 9-5. A loop to write several images</a></p><p><a href="#spark_manifest">Example 9-6. Spark operation manifest</a></p><p><a href="#ct_scan_denoise">Example 9-7. CT scan denoising pipeline</a></p><p><a href="#untitled_programlisting_28">untitled_programlisting_28</a></p><p><a href="#untitled_programlisting_29">untitled_programlisting_29</a></p><p><a href="#untitled_programlisting_30">untitled_programlisting_30</a></p><p><a href="#untitled_programlisting_31">untitled_programlisting_31</a></p><p><a href="#untitled_programlisting_32">untitled_programlisting_32</a></p><p><a href="#untitled_programlisting_33">untitled_programlisting_33</a></p><p><a href="#Examp_experimen_spec">Example 10-1. Example experiment spec</a></p><p><a href="#untitled_programlisting_34">untitled_programlisting_34</a></p><p><a href="#untitled_programlisting_35">untitled_programlisting_35</a></p><p><a href="#Ex_exper_output">Example 10-2. Example experiment output</a></p><p><a href="#distr_train_example">Example 10-3. Distributed training example</a></p><p><a href="#Example_NAS_experiment_spec">Example 10-4. Example NAS experiment spec</a></p><p><a href="#use_csv_examples_dataflow">Example B-1. Changing the pipeline to use Dataflow</a></p></div><div id="menuText_FIGURE" style="display: none;"><p><a href="#timbit_pic">Figure P-1. Timbit the dog</a></p><p><a href="#tina_pic">Figure P-2. Tina the cat</a></p><p><a href="#apache_meowska">Figure P-3. Apache and Meowska</a></p><p><a href="#mdlc_figure">Figure 1-1. Model development life cycle</a></p><p><a href="#ch1_jupyter_notebook">Figure 1-2. Jupyter notebook running in Kubeflow</a></p><p><a href="#ch1_kubeflow_pipeline">Figure 1-3. A Kubeflow pipeline</a></p><p><a href="#img-kubeflow-ui">Figure 2-1. Kubeflow web UI</a></p><p><a href="#img-argo">Figure 2-2. Pipelines web UI</a></p><p><a href="#img-pipeline-detail">Figure 2-3. Pipeline detail page</a></p><p><a href="#handwritten_3">Figure 2-4. Handwritten 3</a></p><p><a href="#ch3_kf_arch">Figure 3-1. Kubeflow architecture</a></p><p><a href="#central_dashboard">Figure 3-2. The central dashboard</a></p><p><a href="#metadata_dia">Figure 3-3. Metadata diagram</a></p><p><a href="#ex_minio_ui">Figure 3-4. MinIO dashboard</a></p><p><a href="#fig_istio_arch">Figure 3-5. Istio architecture</a></p><p><a href="#fig_knative_arch">Figure 3-6. Knative architecture</a></p><p><a href="#kubeflow_pipelines_ui_prepackage_pipelines">Figure 4-1. Kubeflow pipelines UI: prepackaged pipelines</a></p><p><a href="#kubeflow_pipelines_ui_pipeline_graph_view">Figure 4-2. Kubeflow pipelines UI: pipeline graph view</a></p><p><a href="#kubeflow_pipelines_ui_pipeline_view">Figure 4-3. Kubeflow pipelines UI: pipeline view</a></p><p><a href="#pipeline_execution">Figure 4-4. Pipeline execution</a></p><p><a href="#argo_ui_for_pipeline_execution">Figure 4-5. Argo UI for pipeline execution</a></p><p><a href="#argo_ui_execution_graph">Figure 4-6. Argo UI execution graph</a></p><p><a href="#viewing_kuberflow_pipelines_in_argo_ui">Figure 4-7. Viewing Kubeflow pipelines in Argo UI</a></p><p><a href="#execution_recomm_pipelines_ex">Figure 4-8. Execution of recommender pipelines example</a></p><p><a href="#fig_conditional_pipelines_example">Figure 4-9. Execution of conditional pipelines example</a></p><p><a href="#fig_conditional_pipelines_log">Figure 4-10. Viewing conditional pipeline log</a></p><p><a href="#setting_up_periodic_execution_of_a_pipeline">Figure 4-11. Setting up periodic execution of a pipeline</a></p><p><a href="#metadata_ui">Figure 6-1. Accessing Metadata UI</a></p><p><a href="#artifact_store_ui">Figure 6-2. List of artifacts in the Artifact Store UI</a></p><p><a href="#artifact_view">Figure 6-3. Artifact view</a></p><p><a href="#mlflow_arch">Figure 6-4. Overall architecture of MLflow components deployment</a></p><p><a href="#mlflow_mainpage">Figure 6-5. MLflow main page</a></p><p><a href="#indi_run">Figure 6-6. View of the individual run</a></p><p><a href="#multirun">Figure 6-7. Run comparison view</a></p><p><a href="#metrics">Figure 6-8. Run metrics comparison view</a></p><p><a href="#tfserving_figure">Figure 8-1. TFServing architecture</a></p><p><a href="#seldoninference_figure">Figure 8-2. Seldon inference graph example</a></p><p><a href="#seldongraph_figure">Figure 8-3. Seldon graph examples</a></p><p><a href="#seldonexplainer_figure">Figure 8-4. Seldon explainer component</a></p><p><a href="#seldoneventing_figure">Figure 8-5. Data science monitoring of models with Seldon Core and Knative</a></p><p><a href="#kfservingdataplane_figure">Figure 8-6. KFServing data plane</a></p><p><a href="#kfservinginfra_figure">Figure 8-7. KFServing infrastructure stack</a></p><p><a href="#kfservingrequest_figure">Figure 8-8. KFServing request flow</a></p><p><a href="#ct_original">Figure 9-1. Original slice of DICOM</a></p><p><a href="#ct_15">Figure 9-2. 1% denoised DICOM slice (left); 5% denoised DICOM slice (right)</a></p><p><a href="#ct_1030">Figure 9-3. 10% denoised DICOM slice (left); .5% denoised DICOM slice (right)</a></p><p><a href="#katib-workflow">Figure 10-1. Katib system workflow</a></p><p><a href="#katib-main">Figure 10-2. Katib UI main page</a></p><p><a href="#katib-new-experiment">Figure 10-3. Configuring a new experiment, part 1</a></p><p><a href="#katib-new-experiment-2">Figure 10-4. Configuring a new experiment, part 2</a></p><p><a href="#katib-random-example-graph">Figure 10-5. Katib UI for an experiment</a></p><p><a href="#katib-random-example-table">Figure 10-6. Katib metrics for an experiment</a></p><p><a href="#katib-trial">Figure 10-7. Metrics for each trial</a></p><p><a href="#Cloudflow_arch">Figure C-1. Cloudflow architecture</a></p><p><a href="#dynamically_controlled_figure">Figure C-2. Dynamically controlled stream pattern</a></p><p><a href="#batch_processing_figure">Figure C-3. Using stream processing for batch serving implementation</a></p></div><div id="menuText_TABLE" style="display: none;"><p><a href="#mlmd_features">Table 3-1. Examples of ML Metadata operations</a></p><p><a href="#query_result">Table 6-1. List of models</a></p><p><a href="#untitled_table_1">untitled_table_1</a></p><p><a href="#query_table">Table 6-2. Query result as a table</a></p><p><a href="#comparing_embedded_with_model_serving_as_a_service">Table 8-1. Comparing embedded with MaaS</a></p><p><a href="#comparing_different_model_inference_approaches">Table 8-2. Comparing different model inference approaches</a></p><p><a href="#kfserving_v1_data_plane">Table 8-3. KFServing V1 data plane</a></p><p><a href="#argo_k8s_table">Table A-1. Argo and Kubernetes APIs</a></p></div><script>
          function updateMenuDiv() {
            let menuDiv = document.getElementById('menuDiv');
            let selectList = document.getElementById('selectList');
            switch(selectList.value) {
              case 'hide':
                menuDiv.style.display = 'none';
                break;
              default:
                menuDiv.innerHTML = document.getElementById('menuText_' + selectList.value).innerHTML;
                menuDiv.style.display = 'block';
            }
          }
          </script></body></html>