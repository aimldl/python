
Index
A
A/B testing, Model Updatingadministrator of Kubeflow cluster, Kubeflow Multiuser IsolationAmazon EMR, Spark operators in KubeflowAmbassador, Serving Requestsannotations on pipelines, Building a Simple Pipeline in PythonApache Beamabout, Distributed ToolingPython support, TensorFlow ExtendedTensorFlow Extended on top of, TensorFlow ExtendedApache Flink, Stream Processing Engines and LibrariesApache Mahout, The Denoising CT Scans Example, DS-SVD with Apache SparkApache Software Foundation (see mailing list data preparation)Apache SpamAssassin, Data Cleaning: Filtering Out the Junk, Putting It Together in a PipelineApache Sparkabout, Apache Spark, Distributed Tooling, Distributed Data Using Apache Sparkbasics, Spark operators in Kubeflowcloud-specific options for running, Spark operators in Kubeflowconfiguring, Spark operators in Kubeflowdata denoising example id=ch09-dd5, The Denoising CT Scans Exampledata denoising pipeline, The CT Scan Denoising Pipeline-The pipelinefeature preparation example, Distributed Feature Preparation Using Apache SparkJupyter notebooks, Distributed Data Using Apache SparkKubeflow native operator, Spark operators in Kubeflow-Spark operators in Kubeflowmailing list example, Distributed Feature Preparation Using Apache SparkMinIO configuration, Spark operators in KubeflowResourceOp request validation, Spark operators in Kubeflowresources on, Distributed Data Using Apache Sparkschema validation, Validating the schemaSQL, Filtering out bad datausing with Kubeflow, Data/Feature Preparation, Distributed Data Using Apache SparkApache Spark in Kubeflowcomponents, Apache SparkarchitectureDifferentiable Architecture Search (DARTS), Neural Architecture SearchEfficient Neural Architecture Search (ENAS), Neural Architecture Searchneural architecture search support and Katb, Neural Architecture SearchArgo executors APIs, Argo Executor Configurations and Trade-OffsArgo Workflowsdeleting a workflow, Argo: the Foundation of PipelinesDocker as executor, Argo: the Foundation of Pipelines, Argo Executor Configurations and Trade-Offsexecuting pipeline YAML files, Building a Simple Pipeline in Pythonexecution information, Argo: the Foundation of Pipelinesexecution logs, Argo: the Foundation of Pipelinesexecutors, Argo: the Foundation of Pipelines, Argo Executor Configurations and Trade-Offsflow execution graph details, Argo: the Foundation of Pipelinesinstalling, Argo: the Foundation of PipelinesKubeflow Pipelines built on, Kubeflow Pipelines, Introduction to Kubeflow Pipelines Components-Argo: the Foundation of PipelinesKubeflow Pipelines enhancing, What Kubeflow Pipelines Adds to Argo Workfloworchestration controller, Kubeflow Pipelinesparameter passing, Building a Pipeline Using Existing Imagespipelines visible, Argo: the Foundation of Pipelines, Argo: the Foundation of PipelinesUI for pipeline execution, Argo: the Foundation of Pipelines, Argo: the Foundation of PipelinesUI installation, Argo: the Foundation of PipelinesYAML files for pipelines, Exploring the Prepackaged Sample Pipelinesartifact store, for logged metadata events, Kubeflow Metadata UIattribution for code examples, Using Code Examplesauthor contact information, How to Contact the AuthorsAutoML (automated machine learning)about, Hyperparameter Tuning and Automated 
Machine Learningcontinuous learning as, Summary of Inference RequirementsKubeflow Katib, Hyperparameter Tuning, Hyperparameter Tuning with Kubeflow KatibBbatch applications via model serving, Building Batch Applications Leveraging Model Serving-Building Batch Applications Leveraging Model ServingBayesian optimizationin Katib, Hyperparameter Tuning, Katib Conceptsbeginners’ resources for ML, Our Assumption About You-Our Assumption About You(see also getting started)biases of machine learning, Your Responsibility as a Practitionerblue-green deployment, Model UpdatingKFServing endpoints, Data Plane, Model updatingCCaffe2 for distributed training, Using Other Frameworks for Distributed Trainingcanary deployment, Model Updatingcanary deploymentsKFServing endpoints, Data Plane, Model updatingCanonical resources, Code Examplescentral dashboard, Getting Around the Central Dashboardcentral storage distributed training, Distributed TrainingClipper, Clipper (RiseLabs)cloud native microservices, Deploying a TensorFlow Training JobCloudflow, Introducing Cloudflow-Introducing Cloudflowcode examples in bookdownload link, Code Examplespermission for use, Using Code Examplescollaborative filtering, Building a Recommender with TensorFlowcompeting models, Model Updatingcomponentsabout, Kubeflow’s Design and Core Components-Component Overview, Kubeflow Pipeline Componentscentral dashboard, Getting Around the Central Dashboardfile-fetching component, Kubeflow Pipeline ComponentsGoogle-specific, Google Cloudhyperparameter tuning, Hyperparameter Tuning-Hyperparameter Tuningload_component function warning, Kubeflow Pipeline Components, Keeping your data quality: TensorFlow data validationload_component_from_file function, Keeping your data quality: TensorFlow data validationmetadata management, Metadata(see also Kubeflow ML Metadata)model inference, Model Inferencemultiuser isolation, Kubeflow Multiuser Isolationpipelines, Kubeflow Pipelines, Kubeflow Pipelines(see also Kubeflow Pipelines)repositories, Kubeflow Multiuser Isolation, Kubeflow Pipeline Componentstraining operators, Training Operatorscomposability of Kubeflow, Kubeflow’s Design and Core Componentsconcept drift, Model Accuracy, Drift, and Explainabilityconditional execution of pipelines, Conditional Execution of Pipeline Stages-Conditional Execution of Pipeline Stagescontainer registry, Setting up Dockercontainersabout, Why Containerize?beginners’ resources, Our Assumption About Youbuilding example pipeline, Building a Simple Pipeline in Pythoncontainer registry, Setting up Dockercustom containers, Custom Containersoverhead, Why Containerize?pipeline custom code and tools, Building a Pipeline Using Existing Images, Custom Containersresource about, Why Containerize?serverless, Model InferenceSpamAssassin package, Data Cleaning: Filtering Out the Junk, Putting It Together in a Pipelinetag for pushing, Setting up Dockerwhy containerize, Why Containerize?continuous learning (CL), Summary of Inference Requirementscontrol plane, Serverless and the Service PlaneCOVID-19 pandemic, CT Scans, The Denoising CT Scans ExampleCRDs (see custom resource definitions)credentials for MinIO, MinIOCSV component in recommender system, Keeping your data quality: TensorFlow data validationCT scan data denoisedabout data, CT Scans, Case Study Using Multiple Tools, The Denoising CT Scans ExampleApache Spark, Filtering out bad data, The CT Scan Denoising Pipeline-The pipelinedata preparation, Data Prep with Pythondecomposing CT scan, DS-SVD with Apache Sparkdenoising pipeline, The CT Scan Denoising Pipeline-The pipelineopen source method, Case Study Using Multiple Tools, The Denoising CT Scans Exampleresource on math, The Denoising CT Scans Examplesharing the pipeline, Sharing the Pipelinesingular value decomposition, Case Study Using Multiple Tools, DS-SVD with Apache Sparkvisualizing denoised DICOMs, Visualization-Recomposing the matrix into denoised imagescustom containers, Custom Containerscustom resource definitions (CRDs)Knative Serving, KnativePipeline Service, Kubeflow Pipelinescustom resources on Kubernetes, Deploying a TensorFlow Training JobKubeflow Katib, Katib Concepts, Configuring an ExperimentDdatadata lineage, Artifact and Metadata StoreDICOM file format, Data Prep with Pythondistributed object storage server, MinIO, Storing Data Between Stepsenvironment variables for pipelines, Building a Pipeline Using Existing Imagesexploring new, Putting It Together in a Pipelinefile-fetching component, Kubeflow Pipeline ComponentsKubernetes Pods storing, Kubeflow Pipelinesmetadata definition, Artifact and Metadata Store(see also metadata)persistent volumes, Storing Data Between Steps-Storing Data Between StepsApache Spark output, Saving the outputfilesystem/get_file component, Keeping your data quality: TensorFlow data validationlocal data preparation, Fetching the Datapreparation of, Data/Feature Preparation, Data and Feature Preparation(see also data preparation)sources for datasets, Data Prep with Pythontracked by Kubeflow, Kubeflow Pipelinesvalidation via TensorFlow Extended, Keeping your data quality: TensorFlow data validation-Keeping your data quality: TensorFlow data validationdata cleaningabout CT scan data, CT Scans, The Denoising CT Scans ExampleApache Spark, The CT Scan Denoising Pipeline-The pipelinedenoising pipeline, The CT Scan Denoising Pipeline-The pipelineopen source method, Case Study Using Multiple Tools, The Denoising CT Scans Exampledata parallelism distributed training, Distributed Trainingdata plane of KFServing, Data Plane-Data Plane, Model servingdata preparationabout, Data and Feature PreparationAutoML for, AutoML: An OverviewCT scan data denoised, Data Prep with Pythondistributedabout, Distributed ToolingApache Spark feature preparation, Distributed Feature Preparation Using Apache SparkApache Spark for, Spark operators in Kubeflow-Distributed Feature Preparation Using Apache SparkApache Spark setup, Distributed Data Using Apache Spark-Spark operators in Kubeflowdata validation, Keeping your data quality: TensorFlow data validation-Keeping your data quality: TensorFlow data validation, Spark operators in Kubeflow-Distributed Feature Preparation Using Apache Sparkmissing data, Handling missing fieldsrejected records check, Keeping your data quality: TensorFlow data validationfeature preparationabout, Data and Feature Preparation, Feature PreparationApache Spark, Distributed Feature Preparation Using Apache SparkAutoML for, AutoML: An Overviewdata formatting and, Formatting the Datarecommendation system, Starting a New Notebook SessionTensorFlow Transform, TensorFlow Transform, with TensorFlow Extended on Beam-TensorFlow Transform, with TensorFlow Extended on Beamlocalabout, Local Data and Feature Preparationcustom containers, Custom Containersfetching the data, Fetching the Datafiltering out junk, Data Cleaning: Filtering Out the Junkformatting the data, Formatting the Datamissing data, Data Cleaning: Filtering Out the JunkSpamAssassin package, Data Cleaning: Filtering Out the Junkmissing datadistributed platform, Handling missing fieldslocal, Data Cleaning: Filtering Out the JunkScikit-learn and, Data Preparationputting together into a pipeline, Putting It Together in a Pipeline-Using an Entire Notebook as a Data Preparation 
Pipeline Stageentire notebook as pipeline stage, Using an Entire Notebook as a Data Preparation 
Pipeline StageRandom Forest algorithm, Data Preparation-Data PreparationScikit-learn and missing data, Data Preparationtools online resource, Deciding on the Correct Toolingtools, local versus distributed, Deciding on the Correct ToolingUS Census dataset, Data Preparation-Data PreparationDatabricks MLflowabout, MLflow (Databricks), Artifact and Metadata Storemetadata toolsabout, Using MLflow’s Metadata Tools with Kubeflowlogging data on runs, Logging Data on RunsMLflow Tracking, Using MLflow’s Metadata Tools with KubeflowMLflow Tracking Server, Using MLflow’s Metadata Tools with Kubeflow-Creating and Deploying an MLflow Tracking ServerUI, Using the MLflow UIdatasets (see data)debuggingKFServingInferenceService, Debugging an InferenceServiceperformance, Debugging performanceTFJob deployment, Deploying a TensorFlow Training Jobdeep learning, Building a Recommender with TensorFlowsharing a pipeline, Sharing the Pipelinedenoising dataabout CT scan data, CT Scans, Case Study Using Multiple Tools, The Denoising CT Scans ExampleApache Spark, Filtering out bad data, The CT Scan Denoising Pipeline-The pipelineCT scan case study, Case Study Using Multiple Tools-Sharing the Pipelinedata preparation, Data Prep with Pythondecomposing CT scan, DS-SVD with Apache Sparkdenoising pipeline, The CT Scan Denoising Pipeline-The pipelineopen source method, Case Study Using Multiple Tools, The Denoising CT Scans Exampleresource on math, The Denoising CT Scans Examplesharing the pipeline, Sharing the Pipelinesingular value decomposition, Case Study Using Multiple Tools, DS-SVD with Apache Sparkvisualizing denoised DICOMs, Visualization-Recomposing the matrix into denoised imagesdeployment of Kubeflowclick-to-deploy on Google Cloud, Getting Set Up with Kubeflowmodel serving options, Model Inferencenamespace, Creating Our First Kubeflow ProjectDICOM file format, Data Prep with Pythonsources for datasets, Data Prep with PythonDifferentiable Architecture Search (DARTS), Neural Architecture Searchdisk space for Minikube, Minikubedisplay_schema, Keeping your data quality: TensorFlow data validationdistributed stochastic singular value decomposition (DS-SVD), Case Study Using Multiple Tools, DS-SVD with Apache Sparkdistributed trainingabout, Distributed TrainingDockerApache Spark on Jupyter notebooks, Distributed Data Using Apache SparkArgo Workflows executor, Argo: the Foundation of Pipelines, Argo Executor Configurations and Trade-Offscontainer registry, Setting up Dockerdeploying recommender training code, Deploying a TensorFlow Training Jobentire notebook as pipeline stage, Using an Entire Notebook as a Data Preparation 
Pipeline Stageinstalling, Setting up Dockerinstalling Minikube, Minikubeparameters passed by value, Building a Pipeline Using Existing Imagesprebuilt Docker images, Building a Pipeline Using Existing Images-Building a Pipeline Using Existing ImagesSeldon Core local testing, Local testing with DockerEEfficient Neural Architecture Search (ENAS), Neural Architecture SearchEMR native Spark operator, Spark operators in Kubeflow-Spark operators in Kubeflowenvironment variables for pipelines, Building a Pipeline Using Existing Imagesevents via Knative EventingKafkaSource to send events, Knative EventingKFServing, Model Inference, Knative Eventingonline documentation, Knative EventingSeldon Core, Outlier and drift detectionexample generators in TensorFlow Extended, Keeping your data quality: TensorFlow data validationexecutors for Argo Workflows, Argo: the Foundation of Pipelines, Argo Executor Configurations and Trade-Offsexperiments, Exploring the Prepackaged Sample Pipelines, Building a Simple Pipeline in Python, Building a Pipeline Using Existing ImagesKubeflow Katib, Hyperparameter Tuning, Katib Conceptsconfiguring, Configuring an Experimentreproducibility by sharing pipeline, Sharing the Pipelineexplaining the modelScikit-learn, Scikit-Learn Training-Explaining the Modelexplaining the model, importance of, Scikit-Learn Training, Model Accuracy, Drift, and ExplainabilityFfailover, Training Operatorsfeature preparationabout, Data and Feature Preparation, Feature PreparationApache Spark, Distributed Feature Preparation Using Apache SparkAutoML for, AutoML: An Overviewdata formatting and, Formatting the Datarecommendation system, Starting a New Notebook SessionTensorFlow Transform, TensorFlow Transform, with TensorFlow Extended on Beam-TensorFlow Transform, with TensorFlow Extended on Beamfile-fetching component, Kubeflow Pipeline Componentsfilesystem/get_file component, Keeping your data quality: TensorFlow data validationfile_output mechanism, Storing Data Between Steps, Keeping your data quality: TensorFlow data validationGgetting startedgetting started guide, Going Beyond a Local Deploymentinstalling Kubeflow, Getting Set Up with Kubeflow-Installing Kubeflow and Its Dependencies, Creating Our First Kubeflow Project, Going Beyond a Local Deploymentfirst project, Creating Our First Kubeflow Project-Test Queryinstalling Kubeflow Katib, Installing Katibfirst experiment, Running Your First Katib Experiment-Running the Experimentmachine learning, Our Assumption About You-Our Assumption About YouGoogle BigQuery example generators, Keeping your data quality: TensorFlow data validationGoogle Cloud Platform (GCP)click-to-deploy Kubeflow app, Getting Set Up with KubeflowGoogle-specific components, Google CloudTPU-accelerated instances, TPU-Accelerated InstancesGoogle codelabs, ConclusionGoogle DataflowApache Beam for, Distributed ToolingTensorFlow Extended configured for, Dataflow for TFXGoogle Dataproc for Apache Spark, Spark operators in Kubeflow-Spark operators in KubeflowGoogle Kubernetes Engine (GKE), Exploring the Prepackaged Sample PipelinesGoogle Vizier, Hyperparameter Tuning, Hyperparameter Tuning with Kubeflow KatibGPUsautoscaling in KFServing, KFServing, Going layer by layer, Model servingautoscaling lacking in Seldon Core, Model servingresource marking in code, Building a Simple Pipeline in Python, Using an Entire Notebook as a Data Preparation 
Pipeline Stagetraining using, Using GPUsgrid search in Katib, Katib ConceptsHhandwriting recognition via RandomForestClassifier, Creating Our First Kubeflow Project-Test Queryhello world project, Creating Our First Kubeflow Project-Test QueryhyperparametersAutoML for tuning, AutoML: An Overviewdefinition, Hyperparameter Tuning, Hyperparameter Tuning with Kubeflow KatibKubeflow Katib for tuning, Hyperparameter Tuning-Hyperparameter Tuning, Hyperparameter Tuning with Kubeflow Katib, Advantages of Katib over Other Frameworks(see also Kubeflow Katib)neural architecture search, AutoML: An OverviewTensorFlow recommender, TensorFlow Trainingtuning supported by Kubeflow, Hyperparameter TuningIincome predictor model, US Census income predictor model example-US Census income predictor model exampleIstioabout, Istio, Going layer by layerKFServinginfrastructure, Going layer by layermodel inference, Model InferenceJJupyter notebooksadding system software, Data Cleaning: Filtering Out the JunkApache Spark via Dockerfile, Distributed Data Using Apache Sparkdata and feature preparationabout, Local Data and Feature Preparationadding system software, Data Cleaning: Filtering Out the Junkentire notebook as pipeline stage, Using an Entire Notebook as a Data Preparation 
Pipeline StageGPU resources, Building a Simple Pipeline in Python, Using an Entire Notebook as a Data Preparation 
Pipeline StageJupyterHub, Notebooks (JupyterHub), Training Operators, Using an Entire Notebook as a Data Preparation 
Pipeline Stagekubectl for Kubernetes management, Notebooks (JupyterHub)Kubeflow component support via, Notebooks (JupyterHub)Kubeflow support for, Data Exploration with Notebooks, Setting Up Your Kubeflow Development Environmentmultiuser isolation, Kubeflow Multiuser IsolationScikit-learn notebook setup, Training a Model Using Scikit-Learn-Training a Model Using Scikit-LearnTensorFlow recommender notebook setup, Building a Recommender with TensorFlow-Starting a New Notebook SessionKKafkaSource to send Knative events, Knative EventingKatib (Kubeflow)about, Hyperparameter Tuning, Hyperparameter Tuning with Kubeflow Katib, Katib Conceptsabout katib meaning, Hyperparameter Tuning with Kubeflow Katibadvantages of, Advantages of Katib over Other Frameworksdistributed training jobs, Tuning Distributed Training Jobsexperiments, Hyperparameter Tuning, Katib Conceptsconfiguring, Configuring an Experimentfirst experiment, Running Your First Katib Experiment-Running the Experimentfirst experimentabout, Running Your First Katib Experimentconfiguring experiment, Configuring an Experimentprepping training code, Prepping Your Training Coderunning, Running the Experiment-Running the Experimentinstalling, Installing Katibjobs, Hyperparameter Tuningmetrics collector, Katib Conceptsneural architecture search support, Neural Architecture Searchabout NAS, Neural Architecture Searchexample DARTS experiment, Neural Architecture Search-Neural Architecture Searchmodel manager, Neural Architecture Searchsearch algorithmsBayesian optimization, Katib Conceptsgrid search, Katib Conceptshyperbrand, Katib Conceptsrandom search, Katib Conceptssuggestions, Hyperparameter Tuning, Katib Conceptstrials, Hyperparameter Tuning, Katib ConceptsUI, Katib User InterfaceKeras API, Building a Recommender with TensorFlowkfctl repository, Kubeflow Multiuser IsolationKFServingabout, KFServing, ReviewAPI documentation, API documentation, API documentationcapabilities of, Additional featurescomparison chart, Model Inference in Kubeflowcurl 404 Not Found, Recommender exampledata plane, Data Plane-Data Planecomponent in, Data Planeendpoint in, Data Planeexplainer in, Data Planeprediction protocol in, Data Planepredictor in, Data Planetransformer in, Data Planedebugging InferenceService, Debugging an InferenceServicedebugging performance, Debugging performancedeployment strategies, Model updatingendpointsblue-green deployment, Data Planeinference, KFServing-SummaryInferenceServiceautoscaling via escape hatches, Escape hatches-Escape hatchesdebugging, Debugging an InferenceServiceescape hatches, Escape hatches-Escape hatchesexamples, Simplicity and extensibility-Simplicity and extensibility, Recommender exampleKafkaSource to send Knative events, Knative Eventingnamespace, Recommender examplerecommender, Recommender example-Recommender exampleinfrastructure stack, Peeling Back the Underlying Infrastructuredebugging InferenceService, Debugging an InferenceServicedebugging performance, Debugging performanceescape hatches, Escape hatches-Escape hatchesIstio, Going layer by layerKnative, Data Plane, Going layer by layer, Going layer by layer, Knative EventingKnative Eventing, Model Inference, Knative EventingKnative Serving, Going layer by layer, Knative Eventing, Model updatingKubernetes, Going layer by layermodel monitoring, Model monitoringmodel serving, Model servingmodel updating, Model updatingnetwork monitoring and telemetry, Model monitoringSDK documentation, Recommender exampleserverless inferencing, KFServing, Recommender example, Knative Eventingservice plane, Serverless and the Service Planesetting up, Setting up KFServing-Setting up KFServingnamespaces and, Setting up KFServingtroubleshooting guide online, Setting up KFServingKnativearchitecture, Knativecomponents in Kubeflow, KnativeEventingKafkaSource to send events, Knative EventingKFServing, Model Inference, Knative Eventingonline documentation, Knative EventingSeldon Core, Outlier and drift detectionKFServing infrastructure, Data Plane, Going layer by layer, Knative EventingServing, Knative, KFServingKFServing, Going layer by layer, Knative Eventing, Model updatingkubectldeployment of Kubeflow, Creating Our First Kubeflow Projectinstalling, Installing Kubeflow and Its DependenciesJupyter notebook incorporation, Notebooks (JupyterHub)Kubeflowabout, Kubeflow: What It Is and Who It Is For, Why Kubernetes?alternatives to, Alternatives to Kubeflow-Others, TensorFlow ExtendedApache 2 license, Code Examplescore components, Kubeflow’s Design and Core Components-Component Overview(see also components)dataset tools, Data/Feature Preparationfirst project, Creating Our First Kubeflow Project-Test Querygetting started guide, Going Beyond a Local Deploymenthyperparameter tuning, Hyperparameter Tuning(see also hyperparameters)installing, Getting Set Up with Kubeflow-Installing Kubeflow and Its Dependencies, Creating Our First Kubeflow Project, Training and Monitoring Progress, Going Beyond a Local Deploymentinstalling development environment, Setting Up Your Kubeflow Development Environmentlocal installation, Setting Up Local Kuberneteslocal to distributed with ease, Getting Set Up with Kubeflow, Going Beyond a Local Deploymentonline community, Conclusion, Conclusionoverhead, Why Containerize?training frameworks, Trainingweb UI installation, Training and Monitoring ProgressKubeflow Katibabout, Hyperparameter Tuning, Hyperparameter Tuning with Kubeflow Katib, Katib Conceptsabout katib meaning, Hyperparameter Tuning with Kubeflow Katibadvantages of, Advantages of Katib over Other Frameworksdistributed training jobs, Tuning Distributed Training Jobsexperiments, Hyperparameter Tuning, Katib Conceptsconfiguring, Configuring an Experimentfirst experiment, Running Your First Katib Experiment-Running the Experimentfirst experimentabout, Running Your First Katib Experimentconfiguring experiment, Configuring an Experimentprepping training code, Prepping Your Training Coderunning, Running the Experiment-Running the Experimentinstalling, Installing Katibjobs, Hyperparameter Tuningmetrics collector, Katib Conceptsneural architecture search support, Neural Architecture Searchabout NAS, Neural Architecture Searchexample DARTS experiment, Neural Architecture Search-Neural Architecture Searchmodel manager, Neural Architecture Searchsearch algorithmsBayesian optimization, Katib Conceptsgrid search, Katib Conceptshyperbrand, Katib Conceptsrandom search, Katib Conceptssuggestions, Hyperparameter Tuning, Katib Conceptstrials, Hyperparameter Tuning, Katib ConceptsUI, Katib User InterfaceKubeflow ML Metadataabout, Artifact and Metadata Store, Kubeflow ML Metadatadataset tracking, Kubeflow ML Metadatadefining a workspace, Kubeflow ML Metadatainformation about model and metrics, Kubeflow ML Metadatainformation organization, Kubeflow ML Metadatalimitations of, Kubeflow Metadata UIPython only APIs, Kubeflow ML Metadatarequired imports, Kubeflow ML MetadataKubeflow Pipelinesabout, Pipelines, Getting Set Up with Kubeflow, Kubeflow Pipelines, Kubeflow Pipelines, Kubeflow Pipelinesand TensorFlow Extended, TensorFlow Extendedannotations, Building a Simple Pipeline in PythonArgo alternative, Argo: the Foundation of PipelinesArgo Workflows enhanced by, What Kubeflow Pipelines Adds to Argo WorkflowArgo Workflows foundation, Kubeflow Pipelines, Introduction to Kubeflow Pipelines Components-Argo: the Foundation of Pipelinesbuilding examples in Python, Building a Simple Pipeline in Python-Building a Simple Pipeline in PythoncamelCase function name bug in DSL, Building a Simple Pipeline in Pythoncompiler, Kubeflow Pipelines, Building a Simple Pipeline in Pythoncomponents of, Kubeflow Pipelines, Kubeflow Pipeline Componentsconditional execution, Conditional Execution of Pipeline Stages-Conditional Execution of Pipeline Stagescustom code and tools inside, Building a Pipeline Using Existing Images, Custom Containersdata and feature preparation, Data and Feature Preparation, Putting It Together in a Pipeline-Using an Entire Notebook as a Data Preparation 
Pipeline Stageenvironment variables, Building a Pipeline Using Existing Imagesexperiments, Exploring the Prepackaged Sample Pipelines, Building a Simple Pipeline in Python, Building a Pipeline Using Existing Imagesreproducibility by sharing pipeline, Sharing the Pipelineexploring sample, Exploring the Prepackaged Sample Pipelinesgeneric versus Google Kubernetes Engine, Exploring the Prepackaged Sample PipelinesGPU resource marking, Using an Entire Notebook as a Data Preparation 
Pipeline StageGPU resource marking in DSL, Building a Simple Pipeline in Pythonlanguage capabilities, Custom Containers, Case Study Using Multiple Toolsdenoising CT scan case study, Case Study Using Multiple Tools-Sharing the Pipelineload_component function warning, Kubeflow Pipeline Components, Keeping your data quality: TensorFlow data validationload_component_from_file function, Keeping your data quality: TensorFlow data validationoperators chaining execution, Training Operatorsperiodic execution of, Running Pipelines on Scheduleprebuilt Docker images, Building a Pipeline Using Existing Images-Building a Pipeline Using Existing ImagesPython SDK, Kubeflow Pipelinesrunning, Exploring the Prepackaged Sample Pipelines, Building a Simple Pipeline in Pythonon a schedule, Running Pipelines on ScheduleSDK installation, Setting Up Your Kubeflow Development EnvironmentService, Kubeflow Pipelinesrepository, Kubeflow Multiuser Isolationshared storage, MinIOtraining first project, Training and Monitoring Progresstraining integrated into, Integration into Pipelinestransformation code, TensorFlow Transform, with TensorFlow Extended on BeamUI, Exploring the Prepackaged Sample PipelinesKubeflow Slack workspace, Conclusion, ConclusionKubernetesabout, Why Kubernetes?Argo Workflows, Kubeflow Pipelines, Introduction to Kubeflow Pipelines Componentsbeginners’ resources, Our Assumption About Youclient, Building a Pipeline Using Existing Imagescloud native microservices, Deploying a TensorFlow Training Jobcustom resourcesKubeflow Katib, Katib Concepts, Configuring an Experimentcustom resources APIs, Deploying a TensorFlow Training Job, Katib Conceptsinstalling Kubeflow, Installing Kubeflow and Its DependenciesKFServing infrastructure, Going layer by layerkubectldeployment of Kubeflow, Creating Our First Kubeflow Projectinstalling, Installing Kubeflow and Its DependenciesJupyter notebook incorporation, Notebooks (JupyterHub)local cluster via Minikube, Setting Up Local KubernetesPipeline Service custom resource definitions, Kubeflow PipelinesPodsdata stored by, Kubeflow Pipelinesdeployment of Kubeflow, Creating Our First Kubeflow Projectresource creation, Notebooks (JupyterHub)YAML configuration editing, Editing YAMLKubernetes custom resourcesLlanguage capabilities of pipelines, Custom Containers, Case Study Using Multiple Toolsdenoising CT scan case study, Case Study Using Multiple Tools-Sharing the Pipelinelibrariesdata validation via TensorFlow Extended, Keeping your data quality: TensorFlow data validation, Keeping your data quality: TensorFlow data validationimporting, Building a Simple Pipeline in PythonKubernetes Python library, Building a Pipeline Using Existing ImagesScikit-learn, Putting It Together in a Pipelinestream processing, Stream Processing Engines and Librarieslightweight Python functions, Building a Simple Pipeline in Python-Building a Simple Pipeline in Pythonload_component warning, Keeping your data quality: TensorFlow data validationload_component_from_file, Keeping your data quality: TensorFlow data validationMMaaS (model serving as a service), Model Serving-Model ServingAPIs, Model Servingmachine learning (ML)AutoMLabout, Hyperparameter Tuning and Automated 
Machine Learningcontinuous learning as, Summary of Inference RequirementsKubeflow Katib, Hyperparameter Tuning, Hyperparameter Tuning with Kubeflow Katibbeginners’ resources, Our Assumption About You-Our Assumption About Youbiases, Your Responsibility as a Practitionerexplainability importance, Scikit-Learn Training, Model Accuracy, Drift, and Explainabilityframework selection, Scikit-Learn Trainingno single model works best, Hyperparameter Tuning and Automated 
Machine Learningreproducibility importance, Artifact and Metadata Storemailing list data preparationabout mailing list data, Mailing List DataApache SpamAssassin package, Data Cleaning: Filtering Out the Junk, Putting It Together in a PipelineApache Sparkfiltering out bad data, Filtering out bad datahandling missing data, Handling missing fieldsreading input data, Reading the input datasaving the output, Saving the output, Putting It Together in a PipelineSQL, Filtering out bad datafetching the data, Fetching the Datafiltering out junk, Data Cleaning: Filtering Out the Junkparallelize for data fetching, Reading the input dataputting together into a pipeline, Putting It Together in a Pipeline-Using an Entire Notebook as a Data Preparation 
Pipeline Stagemanual profile creation, Getting Around the Central Dashboardmetadataabout storing model creation metadata, Kubeflow ML Metadataartifact store and, Kubeflow Metadata UIdefined, Artifact and Metadata StoreKubeflow Metadata (see Kubeflow ML Metadata)Kubernetes Pods, Kubeflow Pipelinesmanagement component, Metadatareproducibility importance, Artifact and Metadata Storeresource on, Artifact and Metadata Storetracked by Kubeflow, Kubeflow Pipelines, Metadatatracking tool in Kubeflow, Artifact and Metadata Store(see also Kubeflow ML Metadata)viewingMetadata UI, Kubeflow Metadata UIprogrammatic query, Programmatic Query-Programmatic QueryMinikubeabout, Setting Up Local Kuberneteslocal installation of Kubeflow, Setting Up Local Kubernetesmemory for, Minikuberesources online, MinikubeMinIO, MinIO-MinIOApache Spark configuration, Spark operators in KubeflowClient exporting a model, TensorFlow Trainingdata validation via TensorFlow Extended, Keeping your data quality: TensorFlow data validationdistributed object storage server, MinIO-MinIOfile_output, Storing Data Between Steps, Keeping your data quality: TensorFlow data validationHadoop version for, MinIOsecrets for credentials, MinIOmirrored distributed training strategy, Distributed TrainingML (see machine learning)ML Metadata TensorFlow Extended (TFX), Artifact and Metadata StoreMLflow (Databricks)about, MLflow (Databricks), Artifact and Metadata Storemetadata toolsabout, Using MLflow’s Metadata Tools with Kubeflowlogging data on runs, Logging Data on RunsMLflow Tracking, Artifact and Metadata Store, Using MLflow’s Metadata Tools with KubeflowMLflow Tracking Server, Using MLflow’s Metadata Tools with Kubeflow-Creating and Deploying an MLflow Tracking ServerUI, Using the MLflow UIMNIST (Modified National Institute of Standards and Technology)about, Modified National Institute of Standards and Technologydata registration example, Kubeflow ML Metadatadistributed training, Distributed Training-Distributed Traininghello world project, Creating Our First Kubeflow Project-Test QueryKubeflow Katib first experimentabout, Running Your First Katib Experimentconfiguring experiment, Configuring an Experimentprepping training code, Prepping Your Training Coderunning experiment, Running the Experiment-Running the ExperimentPython script for first project, Test Querymodel as data MaaS, Model Servingmodel development life cycle (MDLC), Model Development Life Cycle, Data Exploration with Notebooks-Inference/Prediction, Summary of Inference Requirementsmodel drift, Model Accuracy, Drift, and Explainability, Model Accuracy, Drift, and ExplainabilitySeldon Core, Outlier and drift detection, Outlier and drift detectionmodel explainabilityimportance of, Scikit-Learn Training, Model Accuracy, Drift, and ExplainabilityScikit-learn, Scikit-Learn Training-Explaining the Model, Model Accuracy, Drift, and ExplainabilitySeldon Core, Model explainabilitymodel inferenceabout, Model Inference, Summary of Inference Requirementsaccuracy, Model Accuracy, Drift, and Explainabilityas code Maas, Model Servingcomponents, Model Inferencecontinuous learning, Summary of Inference Requirementsdebugging TFJob deployment, Deploying a TensorFlow Training Jobdeployment of in distributed training MNIST example, Distributed Training-Distributed Trainingdeployment reproducibility importance, Artifact and Metadata Storedeployment strategies, Deploying a TensorFlow Training Jobfirst project test query, Test QueryIstio, Model InferenceKFServing, KFServing-Summaryabout, KFServing, Reviewautoscaling via escape hatches, Escape hatches-Escape hatchescapabilities of, Additional featurescomparison chart, Model Inference in Kubeflowdata plane, Data Plane-Data Planedebugging InferenceService, Debugging an InferenceServicedebugging performance, Debugging performancedeployment strategies, Model updatingInferenceService debugging, Debugging an InferenceServiceInferenceService escape hatches, Escape hatches-Escape hatchesInferenceService namespace, Recommender exampleInferenceService recommender, Recommender example-Recommender exampleinfrastructure stack, Peeling Back the Underlying Infrastructuremodel monitoring, Model monitoringmodel serving, Model servingmodel updating, Model updatingnetwork monitoring and telemetry, Model monitoringserverless inferencing, KFServing, Recommender exampleservice plane, Serverless and the Service Planesetting up, Setting up KFServing-Setting up KFServingsetting up and namespaces, Setting up KFServingKubeflow, Model InferenceKubeflow model inference, Inference/Prediction, Model Inference in Kubeflowmodel monitoring, Model Monitoringaccuracy, drift, explainability, Model Accuracy, Drift, and ExplainabilityKFServing, Model monitoringrequirements, Model Monitoring RequirementsSeldon Core, Monitoring Your Models, Outlier and drift detection, Model monitoringTensorFlow Serving, Model monitoringmodel serving, Model Servingembedded, Model ServingKFServing, Model servingmodel serving as a service, Model Serving-Model ServingSeldon Core, Model servingTensorFlow Serving, Model servingmodel serving requirements, Model Serving Requirements-Model Serving Requirementsmodel updating, Model UpdatingKFServing, Model updatingrequirements, Model Updating RequirementsSeldon Core, Model updatingTensorFlow Serving, Model updatingSeldon Core, Seldon Core-Summaryabout, Seldon Corecomparison chart, Model Inference in Kubeflowdeployment, Creating a SeldonDeploymentexample graphs, Creating a SeldonDeployment-Creating a SeldonDeploymentexplaining the model, Model explainabilityincome predictor model, US Census income predictor model example-US Census income predictor model exampleinference graph, Seldon Coremodel serving, Model servingmodel updating, Model updatingmonitoring models, Monitoring Your Models, Outlier and drift detection, Model monitoringpackaging the model, Packaging your modelSeldonMessage, Serving Requestssentiment prediction model, Sentiment prediction model-Sentiment prediction modelserverless primitives lacking, KFServingserving requests, Serving Requestssetting up, Setting up Seldon Coretesting the model, Testing Your Model, Local testing with DockerTensorFlow recommender deploymentwith TensorFlow Serving, TensorFlow Serving-ReviewTensorFlow Serving, TensorFlow Serving-Reviewabout, TensorFlow Servingcomparison chart, Model Inference in Kubeflowmodel monitoring, Model monitoringmodel serving, Model servingmodel updating, Model updatingrecommendation system, TensorFlow Serving-Reviewserverless primitives lacking, KFServingTensorFlow Extended and, Distributed Toolingupdating models, Model UpdatingTensorFlow Serving, Model updatingmodel servingabout, Model Servingcustom applicationsabout, Using Model Serving in Applicationsbatch applications, Building Batch Applications Leveraging Model Serving-Building Batch Applications Leveraging Model Servingstreaming applications, Building Streaming Applications Leveraging 
Model Serving-Introducing Cloudflowembedded, Model ServingKFServing, Model servingmodel serving as a service, Model Serving-Model Servingmodel updating, Model UpdatingKFServing, Model updatingrequirements, Model Updating RequirementsSeldon Core, Model updatingTensorFlow Serving, Model updatingmonitoring, Model MonitoringKFServing, Model monitoringKnative serving project, Model Inferencerequirements, Model Monitoring Requirementsresources on, Model Monitoring RequirementsSeldon Core, Monitoring Your Models, Outlier and drift detection, Model monitoringTensorFlow Serving, Model monitoringrequirements, Model Serving Requirements-Model Serving RequirementsSeldon Core, Model servingTensorFlow Serving, Model servingmodel serving as a service (MaaS), Model Serving-Model Servingmodel training (see training)modelsabout the impact of, Your Responsibility as a Practitionercontinuous learning, Summary of Inference Requirementsevaluating competing, Model Updatingexplainability importance, Scikit-Learn TrainingSeldon Core, Model explainabilityexportingScikit-learn, Exporting ModelTensorFlow, TensorFlow Trainingmonitoring, Model MonitoringKFServing, Model monitoringKnative serving project, Model Inferencerequirements, Model Monitoring Requirementsresources on, Model Monitoring RequirementsSeldon Core, Monitoring Your Models, Outlier and drift detection, Model monitoringTensorFlow Serving, Model monitoringserving options, Model Inferenceupdating, Model UpdatingKFServing, Model updatingrequirements, Model Updating RequirementsSeldon Core, Model updatingTensorFlow Serving, Model updatingvalidation, Model Validation, Test Query, Model Accuracy, Drift, and ExplainabilityModified National Institute of Standards and Technology (see MNIST)monitoringabout, Model MonitoringKFServing, Model monitoringKnative serving project, Model Inferencemodel serving, Model Accuracy, Drift, and Explainabilityrequirements, Model Monitoring Requirementsresources on, Model Monitoring RequirementsSeldon Core, Monitoring Your Models, Outlier and drift detection, Model monitoringTensorFlow Serving, Model monitoringmulti-armed bandits, Model updatingmultiuser isolation, Kubeflow Multiuser Isolationmultiworker mirrored distributed training strategy, Distributed TrainingNnamespacesdeployment of Kubeflow, Creating Our First Kubeflow ProjectKFServingInferenceService, Recommender examplesetup, Setting up KFServingmanual profile creation, Getting Around the Central Dashboardprofile definition, Kubeflow Multiuser IsolationSeldon Core installation, Setting up Seldon CoreNAS (see neural architecture search)natural language processing (NLP), Why Containerize?neural architecture search (NAS)about, AutoML: An Overview, Neural Architecture SearchAutoML, AutoML: An OverviewDifferentiable Architecture Search, Neural Architecture SearchEfficient Neural Architecture Search, Neural Architecture Searchgeneration versus mutation methods, Neural Architecture SearchKubeflow Katib supporting, Neural Architecture Searchexample DARTS experiment, Neural Architecture Search-Neural Architecture Searchmodel manager, Neural Architecture Searchnotebooks (see Jupyter notebooks)Oobject storesdistributed object storage server, MinIO, Storing Data Between Steps(see also MinIO)using with Apache Spark, Saving the outputobservability automated by operators, Training Operatorsone-hot encoding in Scikit-learn, Data Preparationonline community for Kubeflow, Conclusion, Conclusiononline resources (see resources)orchestration controllers, Kubeflow Pipelines, Model InferencePparallelize for data fetching, Reading the input dataparameter server distributed training, Distributed Trainingpersistent volume storageabout, Storing Data Between Steps-Storing Data Between StepsApache Spark output, Saving the outputfilesystem/get_file component, Keeping your data quality: TensorFlow data validationlocal data preparation, Fetching the Datapinned deployments, Model UpdatingKFServing endpoints, Data Plane, Model updatingPods (Kubernetes)data stored by, Kubeflow Pipelinesdeployment of Kubeflow, Creating Our First Kubeflow Projectportability of KubeflowKubernetes foundation, Kubeflow’s Design and Core Components, Deploying a TensorFlow Training Jobobject storage and, Storing Data Between Stepsprediction (see model inference)product recommender (see recommendation systems)profilesautomatic creation, Kubeflow Multiuser Isolationdefinition, Kubeflow Multiuser Isolationmanual creation, Getting Around the Central Dashboardmultiuser isolation, Kubeflow Multiuser IsolationPythonApache Beam support of, TensorFlow ExtendedApache Sparkbasics, Spark operators in Kubeflowreading input data, Reading the input databuilding example pipelines, Building a Simple Pipeline in Python-Building a Simple Pipeline in PythoncamelCase function name bug, Building a Simple Pipeline in Pythonclient for Python-wrapped models, Python client for Python language wrapped modelsDSL compiler, Kubeflow PipelinesGPU resource marking, Building a Simple Pipeline in Python, Using an Entire Notebook as a Data Preparation 
Pipeline Stageinstalling, Setting up the Pipeline SDKKFServing API documentation, API documentationKubeflow ML Metadata, Kubeflow ML MetadataKubeflow native Spark operator, Spark operators in KubeflowKubernetes client, Building a Pipeline Using Existing Imageslibrary imports, Building a Simple Pipeline in Pythonlightweight Python functions, Building a Simple Pipeline in Python-Building a Simple Pipeline in PythonMNIST image script, Test QueryPandas and, Keeping your data quality: TensorFlow data validationpipeline components, Kubeflow Pipelinespipeline custom code and tools, Building a Pipeline Using Existing Images, Custom ContainersScikit-learn, Training a Model Using Scikit-LearnTensorFlow Extended as Python tool, TensorFlow Extendedvirtual environments for projects, Setting up the Pipeline SDKPyTorch for distributed training, Using Other Frameworks for Distributed Trainingjob spec example, Using Other Frameworks for Distributed TrainingQquality of data maintained, Keeping your data quality: TensorFlow data validationRRandom Forest algorithmabout, Training a Model Using Scikit-Learndata preparation, Data Preparation-Data Preparationrunning, Scikit-Learn Trainingrandom search in Katib, Katib Conceptsrecommendation systemsabout, Product Recommender, Building a Recommender with TensorFlowcollaborative filtering, Building a Recommender with TensorFlowKFServing InferenceService, Recommender example-Recommender exampleTensorFlowdeployment with TFServing, TensorFlow Serving-Reviewrepositoriescomponents, Kubeflow Multiuser Isolation, Kubeflow Pipeline ComponentsCOVID-19 CT scans, Data Prep with Pythonkfctl, Kubeflow Multiuser IsolationPipelines Service, Kubeflow Multiuser Isolationreproducibility in machine learning, Artifact and Metadata StoreResourceOp request validation, Spark operators in Kubeflowresources for Kubeflowalternatives, Othersclick-to-deploy Kubeflow app, Getting Set Up with Kubeflowcode examples for download, Code Examplescomponent repositories, Kubeflow Multiuser Isolation, Kubeflow Pipeline Componentsgetting started guide, Going Beyond a Local Deploymentinstallation guide, Creating Our First Kubeflow Projectonline community, Conclusion, ConclusionSscalability of KubeflowApache Spark feature preparation, Distributed Feature Preparation Using Apache Sparkinference component, Model InferenceKFServing for inferencing, KFServingautoscaling via escape hatches, Escape hatches-Escape hatchesKubernetes foundation, Kubeflow’s Design and Core Components, Going Beyond a Local Deployment, Deploying a TensorFlow Training Joboperators automating, Training Operatorsschemadata validation via TensorFlow Extended, Keeping your data quality: TensorFlow data validation, Keeping your data quality: TensorFlow data validation-Keeping your data quality: TensorFlow data validationinferred by TensorFlow Data Validation, Keeping your data quality: TensorFlow data validationinspecting, Keeping your data quality: TensorFlow data validationrejected records check, Keeping your data quality: TensorFlow data validationsaved to catch changes, Keeping your data quality: TensorFlow data validationtool for modifying, Keeping your data quality: TensorFlow data validationvalidation by Apache Spark, Validating the schemaScikit-learnabout, Training a Model Using Scikit-Learnexplaining the model, Scikit-Learn Training-Explaining the Modelexporting the model, Exporting Modellibrary, Putting It Together in a Pipelinemissing data and, Data Preparationone-hot encoding, Data PreparationRandomForestClassifier, Training and Monitoring Progress-Test QueryScikit-learn Jupyter notebook setup, Training a Model Using Scikit-Learn-Training a Model Using Scikit-LearnSeldon Core, Serving Requestsabout, Seldon Corecomparison chart, Model Inference in Kubeflowdeployment, Creating a SeldonDeploymentexample graphs, Creating a SeldonDeployment-Creating a SeldonDeploymentexplaining the model, Model explainabilityincome predictor model, US Census income predictor model example-US Census income predictor model exampleinference, Seldon Core-Summaryinference graph, Seldon CoreIstio ingress gateway and, Serving Requestsmodel serving, Model servingmodel updating, Model updatingmonitoring models, Monitoring Your Models, Outlier and drift detection, Model monitoringoutlier and drift detection in, Outlier and drift detectionpackaging the model, Packaging your modelSeldonMessage, Serving Requestssentiment prediction model, Sentiment prediction model-Sentiment prediction modelserverless primitives lacking, KFServingserving requests, Serving Requestssetting up, Setting up Seldon Coretesting the model, Testing Your Modellocal testing with Docker, Local testing with Dockersentiment prediction model, Sentiment prediction model-Sentiment prediction modelserverlessabout, Serverless and the Service Planecontainers on Kubernetes, Model InferenceKFServing, KFServing, Recommender example, Knative EventingKnative Serving, KFServingKnative serving and, Model Inferenceserverless applicationsKnative Serving, Knativeservice meshabout, Going layer by layercomponents, with Istio, Istiowith Istio, Going layer by layerservice plane, Serverless and the Service Planeshadow models, Model Updatingsingle-worker TensorFlow jobs, Distributed Trainingsingular value decomposition (SVD), Case Study Using Multiple Tools, DS-SVD with Apache SparkSpamAssassin package, Data Cleaning: Filtering Out the Junk, Putting It Together in a PipelineSQL in Apache Spark, Filtering out bad datastoragedistributed object storage server, MinIO, Storing Data Between StepsMinikube requirements, Minikubepersistent volumes, Storing Data Between Steps-Storing Data Between StepsApache Spark output, Saving the outputfilesystem/get_file component, Keeping your data quality: TensorFlow data validationlocal data preparation, Fetching the Datastorage classes, Storing Data Between Stepsstoring data between steps, Storing Data Between Steps-Storing Data Between StepsUI to explore, MinIOstreaming applicationsabout, Building Streaming Applications Leveraging 
Model ServingCloudflow, Introducing Cloudflow-Introducing Cloudflowprocessing engines versus libraries, Stream Processing Engines and Librariessuggestions (Kubeflow Katib), Katib ConceptsTtag for pushing containers, Setting up DockerTekton for running pipelines, Argo: the Foundation of PipelinesTensorFlowabout, Building a Recommender with TensorFlowdistributed trainingabout, Distributed Trainingdistribution strategies, Distributed TrainingMNIST example, Distributed Training-Distributed Trainingjobs as Kubernetes custom resources, Deploying a TensorFlow Training Jobrecommenderabout, Product Recommender, Building a Recommender with TensorFlowcreating TensorFlow session, TensorFlow Trainingdeployment, Deploying a TensorFlow Training Job-Deploying a TensorFlow Training Jobdeployment with TFServing, TensorFlow Serving-Reviewexporting model, TensorFlow Traininghyperparameters, TensorFlow TrainingKeras API, Building a Recommender with TensorFlowmodel selection, Building a Recommender with TensorFlownotebook setup, Building a Recommender with TensorFlow-Starting a New Notebook Sessionrunning training code, TensorFlow Trainingsingle-worker jobs, Distributed TrainingTensorFlow Data Validation (TFDV), Keeping your data quality: TensorFlow data validation-Keeping your data quality: TensorFlow data validationinstalling, Keeping your data quality: TensorFlow data validationschema inferred by, Keeping your data quality: TensorFlow data validationschema inspection, Keeping your data quality: TensorFlow data validationTensorFlow Extended (TFX)about, Distributed Tooling, TensorFlow ExtendedApache Beam Python support and, TensorFlow Extendedexample generators, Keeping your data quality: TensorFlow data validationGoogle Dataflow, Dataflow for TFXinstalling, Keeping your data quality: TensorFlow data validationinstalling components, Keeping your data quality: TensorFlow data validation, Keeping your data quality: TensorFlow data validationKubeflow pipelines and TFX pipelines, TensorFlow ExtendedML Metadata, Artifact and Metadata StorePandas dataframes accepted by, Keeping your data quality: TensorFlow data validationschema inferred by TFDV, Keeping your data quality: TensorFlow data validationTensorFlow Data Validation, Keeping your data quality: TensorFlow data validation-Keeping your data quality: TensorFlow data validationTransform feature preparation, TensorFlow Transform, with TensorFlow Extended on Beam-TensorFlow Transform, with TensorFlow Extended on BeamTensorFlow Model Analysis, TensorFlow Transform, with TensorFlow Extended on BeamTensorFlow Serving (TFServing)about, TensorFlow Servingbatch applications, Building Batch Applications Leveraging Model Servingcomparison chart, Model Inference in Kubeflowinference, TensorFlow Serving-Reviewmodel monitoring, Model monitoringmodel serving, Model servingmodel updating, Model updatingrecommendation system, TensorFlow Serving-Reviewserverless primitives lacking, KFServingTensorFlow Extended integrating with, Distributed ToolingTensorFlow Transform (TFT)feature preparation, TensorFlow Transform, with TensorFlow Extended on Beam-TensorFlow Transform, with TensorFlow Extended on BeamKubeflow support for, Data/Feature PreparationModel Analysis integration, TensorFlow Transform, with TensorFlow Extended on BeamtestingArgo installation, Argo: the Foundation of PipelinesDocker installation, Setting up Dockerfirst project test query, Test QuerySeldon Core inference model, Testing Your Modellocal testing with Docker, Local testing with DockerPython-wrapped models, Python client for Python language wrapped modelsTFJob for deploymentmultiworker distributed training, Distributed Trainingspecifications, Deploying a TensorFlow Training JobTensorFlow recommender, Deploying a TensorFlow Training Job-Deploying a TensorFlow Training JobTFServing (see TensorFlow Serving)TFX (see TensorFlow Extended)tfx/Transform component, TensorFlow Transform, with TensorFlow Extended on BeamTPU distributed training strategy, Distributed TrainingTPU-accelerated instances, TPU-Accelerated Instancestracking data and metadata, Kubeflow Pipelinestrainingabout, Training a Machine Learning Modeldeep learning, Building a Recommender with TensorFlowsharing a pipeline, Sharing the Pipelinedistributed trainingabout, Distributed Trainingdata versus model parallelism, Distributed TrainingGPUs for, Using GPUsKubeflow Katib, Tuning Distributed Training JobsMNIST example, Distributed Training-Distributed Trainingother frameworks for, Using Other Frameworks for Distributed Trainingfirst Kubeflow project, Training and Monitoring Progressframeworks supported, Trainingimpact of using more data, Data and Feature PreparationKubeflow components, Training Operatorsmodel selection, Building a Recommender with TensorFlowAutoML for, AutoML: An Overviewoperators, Training Operators, Integration into Pipelinespipeline integration, Integration into PipelinesScikit-learnabout, Training a Model Using Scikit-Learnabout Random Forest, Training a Model Using Scikit-Learndata preparation, Data Preparation-Data Preparationexplaining the model, Scikit-Learn Training-Explaining the Modelexporting the model, Exporting Modelrunning Random Forest, Scikit-Learn Trainingtraining with and evaluation, Scikit-Learn TrainingTensorFlow recommenderabout TensorFlow, Building a Recommender with TensorFlowcreating TensorFlow session, TensorFlow Trainingdeployment, Deploying a TensorFlow Training Job-Deploying a TensorFlow Training Jobdeployment with TFServing, TensorFlow Serving-Reviewexporting model, TensorFlow Traininghyperparameters, TensorFlow TrainingKeras API, Building a Recommender with TensorFlowmodel selection, Building a Recommender with TensorFlownotebook setup, Building a Recommender with TensorFlow-Starting a New Notebook Sessionrunning training code, TensorFlow Trainingsingle-worker jobs, Distributed Trainingtrials (Kubeflow Katib), Katib ConceptsUUS Census datasetabout dataset, Training a Model Using Scikit-Learnabout Random Forest, Training a Model Using Scikit-Learndata preparation, Data Preparation-Data Preparationexplaining the model, Scikit-Learn Training-Explaining the Modelexporting the model, Exporting Modelincome predictor model, US Census income predictor model example-US Census income predictor model exampletraining, Scikit-Learn Trainingtraining with and evaluation, Scikit-Learn Traininguser interfaces (UI)Argo UI, Argo: the Foundation of Pipelinesinstallation, Argo: the Foundation of Pipelinescentral dashboard, Getting Around the Central Dashboarddisplay_schema, Keeping your data quality: TensorFlow data validationinstallation of Kubeflow web UI, Training and Monitoring ProgressKatib, Katib User InterfaceKubeflow Pipelines UI, Exploring the Prepackaged Sample Pipelines, Building a Simple Pipeline in PythonMetadata UI, Kubeflow Metadata UIMinIO to explore storage, MinIOMLflow (Databricks), Using the MLflow UIuser of Kubeflow cluster, Kubeflow Multiuser IsolationVvalidation of dataApache Spark, Spark operators in Kubeflow-Distributed Feature Preparation Using Apache SparkTensorFlow Extended, Keeping your data quality: TensorFlow data validation-Keeping your data quality: TensorFlow data validationschema inferred, Keeping your data quality: TensorFlow data validationvalidation of modelsimportance of, Test QueryKubeflow support for, Model Validationmodel accuracy, Model Accuracy, Drift, and Explainabilityvirtual environments in Python, Setting up the Pipeline SDKWweb UI for Pipeline, Exploring the Prepackaged Sample Pipelines, Building a Simple Pipeline in PythonYYAMLcomponent options, Kubeflow Pipeline ComponentsDSL compiler producing, Kubeflow Pipelines, Exploring the Prepackaged Sample Pipelines, Building a Simple Pipeline in Pythonediting, Editing YAMLKafkaSource to send Knative events, Knative EventingKFServing InferenceService, Recommender exampleresource creation, Notebooks (JupyterHub)secrets for MinIO credentials, MinIOTensorFlow distributed training job, Distributed Training
hideTOCEQUATIONinline_EQEXAMPLEFIGURETABLEForewordPreface_Our Assumption About You_Your Responsibility as a Practitioner_Conventions Used in This Book_Code Examples__Using Code Examples_O’Reilly Online Learning_How to Contact the Authors_How to Contact Us_Acknowledgments_Grievances1. Kubeflow: What It Is and Who It Is For_1.1. Model Development Life Cycle_1.2. Where Does Kubeflow Fit In?_1.3. Why Containerize?_1.4. Why Kubernetes?_1.5. Kubeflow’s Design and Core Components__1.5.1. Data Exploration with Notebooks__1.5.2. Data/Feature Preparation__1.5.3. Training__1.5.4. Hyperparameter Tuning__1.5.5. Model Validation__1.5.6. Inference/Prediction__1.5.7. Pipelines__1.5.8. Component Overview_1.6. Alternatives to Kubeflow__1.6.1. Clipper (RiseLabs)__1.6.2. MLflow (Databricks)__1.6.3. Others_1.7. Introducing Our Case Studies__1.7.1. Modified National Institute of Standards and Technology__1.7.2. Mailing List Data__1.7.3. Product Recommender__1.7.4. CT Scans_1.8. Conclusion2. Hello Kubeflow_2.1. Getting Set Up with Kubeflow__2.1.1. Installing Kubeflow and Its Dependencies__2.1.2. Setting Up Local Kubernetes___2.1.2.1. Minikube__2.1.3. Setting Up Your Kubeflow Development Environment___2.1.3.1. Setting up the Pipeline SDK___2.1.3.2. Setting up Docker___2.1.3.3. Editing YAML__2.1.4. Creating Our First Kubeflow Project_2.2. Training and Deploying a Model__2.2.1. Training and Monitoring Progress__2.2.2. Test Query_2.3. Going Beyond a Local Deployment_2.4. Conclusion3. Kubeflow Design: Beyond the Basics_3.1. Getting Around the Central Dashboard__3.1.1. Notebooks (JupyterHub)__3.1.2. Training Operators__3.1.3. Kubeflow Pipelines__3.1.4. Hyperparameter Tuning__3.1.5. Model Inference__3.1.6. Metadata__3.1.7. Component Summary_3.2. Support Components__3.2.1. MinIO__3.2.2. Istio__3.2.3. Knative__3.2.4. Apache Spark__3.2.5. Kubeflow Multiuser Isolation_3.3. Conclusion4. Kubeflow Pipelines_4.1. Getting Started with Pipelines__4.1.1. Exploring the Prepackaged Sample Pipelines__4.1.2. Building a Simple Pipeline in Python__4.1.3. Storing Data Between Steps_4.2. Introduction to Kubeflow Pipelines Components__4.2.1. Argo: the Foundation of Pipelines__4.2.2. What Kubeflow Pipelines Adds to Argo Workflow__4.2.3. Building a Pipeline Using Existing Images__4.2.4. Kubeflow Pipeline Components_4.3. Advanced Topics in Pipelines__4.3.1. Conditional Execution of Pipeline Stages__4.3.2. Running Pipelines on Schedule_4.4. Conclusion5. Data and Feature Preparation_5.1. Deciding on the Correct Tooling_5.2. Local Data and Feature Preparation__5.2.1. Fetching the Data__5.2.2. Data Cleaning: Filtering Out the Junk__5.2.3. Formatting the Data__5.2.4. Feature Preparation__5.2.5. Custom Containers_5.3. Distributed Tooling__5.3.1. TensorFlow Extended___5.3.1.1. Keeping your data quality: TensorFlow data validation___5.3.1.2. TensorFlow Transform, with TensorFlow Extended on Beam__5.3.2. Distributed Data Using Apache Spark___5.3.2.1. Spark operators in Kubeflow___5.3.2.2. Reading the input data___5.3.2.3. Validating the schema___5.3.2.4. Handling missing fields___5.3.2.5. Filtering out bad data___5.3.2.6. Saving the output__5.3.3. Distributed Feature Preparation Using Apache Spark_5.4. Putting It Together in a Pipeline_5.5. Using an Entire Notebook as a Data Preparation 
Pipeline Stage_5.6. Conclusion6. Artifact and Metadata Store_6.1. Kubeflow ML Metadata__6.1.1. Programmatic Query__6.1.2. Kubeflow Metadata UI_6.2. Using MLflow’s Metadata Tools with Kubeflow__6.2.1. Creating and Deploying an MLflow Tracking Server__6.2.2. Logging Data on Runs__6.2.3. Using the MLflow UI_6.3. Conclusion7. Training a Machine Learning Model_7.1. Building a Recommender with TensorFlow__7.1.1. Getting Started__7.1.2. Starting a New Notebook Session__7.1.3. TensorFlow Training_7.2. Deploying a TensorFlow Training Job_7.3. Distributed Training__7.3.1. Using GPUs__7.3.2. Using Other Frameworks for Distributed Training_7.4. Training a Model Using Scikit-Learn__7.4.1. Starting a New Notebook Session__7.4.2. Data Preparation__7.4.3. Scikit-Learn Training__7.4.4. Explaining the Model__7.4.5. Exporting Model__7.4.6. Integration into Pipelines_7.5. Conclusion8. Model Inference_8.1. Model Serving__8.1.1. Model Serving Requirements_8.2. Model Monitoring__8.2.1. Model Accuracy, Drift, and Explainability__8.2.2. Model Monitoring Requirements_8.3. Model Updating__8.3.1. Model Updating Requirements_8.4. Summary of Inference Requirements_8.5. Model Inference in Kubeflow_8.6. TensorFlow Serving__8.6.1. Review___8.6.1.1. Model serving___8.6.1.2. Model monitoring___8.6.1.3. Model updating___8.6.1.4. Summary_8.7. Seldon Core__8.7.1. Designing a Seldon Inference Graph___8.7.1.1. Setting up Seldon Core___8.7.1.2. Packaging your model___8.7.1.3. Creating a SeldonDeployment__8.7.2. Testing Your Model___8.7.2.1. Python client for Python language wrapped models___8.7.2.2. Local testing with Docker__8.7.3. Serving Requests__8.7.4. Monitoring Your Models___8.7.4.1. Model explainability___8.7.4.2. Sentiment prediction model___8.7.4.3. US Census income predictor model example___8.7.4.4. Outlier and drift detection__8.7.5. Review___8.7.5.1. Model serving___8.7.5.2. Model monitoring___8.7.5.3. Model updating___8.7.5.4. Summary_8.8. KFServing__8.8.1. Serverless and the Service Plane__8.8.2. Data Plane__8.8.3. Example Walkthrough___8.8.3.1. Setting up KFServing___8.8.3.2. Simplicity and extensibility___8.8.3.3. Recommender example__8.8.4. Peeling Back the Underlying Infrastructure___8.8.4.1. Going layer by layer___8.8.4.2. Escape hatches___8.8.4.3. Debugging an InferenceService___8.8.4.4. Debugging performance___8.8.4.5. Knative Eventing___8.8.4.6. Additional features___8.8.4.7. API documentation__8.8.5. Review___8.8.5.1. Model serving___8.8.5.2. Model monitoring___8.8.5.3. Model updating___8.8.5.4. Summary_8.9. Conclusion9. Case Study Using Multiple Tools_9.1. The Denoising CT Scans Example__9.1.1. Data Prep with Python__9.1.2. DS-SVD with Apache Spark__9.1.3. Visualization___9.1.3.1. Downloading DRMs___9.1.3.2. Recomposing the matrix into denoised images__9.1.4. The CT Scan Denoising Pipeline___9.1.4.1. Spark operation manifest___9.1.4.2. The pipeline_9.2. Sharing the Pipeline_9.3. Conclusion10. Hyperparameter Tuning and Automated 
Machine Learning_10.1. AutoML: An Overview_10.2. Hyperparameter Tuning with Kubeflow Katib_10.3. Katib Concepts_10.4. Installing Katib_10.5. Running Your First Katib Experiment__10.5.1. Prepping Your Training Code__10.5.2. Configuring an Experiment__10.5.3. Running the Experiment__10.5.4. Katib User Interface_10.6. Tuning Distributed Training Jobs_10.7. Neural Architecture Search_10.8. Advantages of Katib over Other Frameworks_10.9. ConclusionA. Argo Executor Configurations and Trade-OffsB. Cloud-Specific Tools and Configuration_B.1. Google Cloud__B.1.1. TPU-Accelerated Instances__B.1.2. Dataflow for TFXC. Using Model Serving in Applications_C.1. Building Streaming Applications Leveraging 
Model Serving__C.1.1. Stream Processing Engines and Libraries__C.1.2. Introducing Cloudflow_C.2. Building Batch Applications Leveraging Model ServingIndexExample 2-1. Install kubectl with snapExample 2-2. Install kubectl with HomebrewExample 2-3. Install KubeflowExample 2-4. Create a virtual environmentExample 2-5. Install Kubeflow Pipeline SDKExample 2-6. Clone the Kubeflow Pipelines repoExample 2-7. Specify the new container is built on top of Kubeflow’s containerExample 2-8. Build the new container and push to a registry for useExample 2-9. Create first example projectExample 2-10. Create training workflow exampleExample 2-11. Model query exampleuntitled_programlisting_1untitled_programlisting_2Example 3-1. Setting up port-forwardingExample 3-2. Install MinIO on MacExample 3-3. Install MinIO on LinuxExample 3-4. Configure MinIO client to talk to Kubeflow’s MinIOExample 3-5. Create a bucket with MinIOExample 3-6. Sample MinIO secretuntitled_programlisting_3untitled_programlisting_4untitled_programlisting_5untitled_programlisting_6Example 4-1. A simple Python functionExample 4-2. A less-simple Python functionExample 4-3. A simple pipelineuntitled_programlisting_7untitled_programlisting_8untitled_programlisting_9Example 4-4. Mailing list data prepExample 4-5. File output exampleExample 4-6. Argo installationExample 4-7. Listing Argo executionsExample 4-8. Getting Argo execution detailsExample 4-9. Getting the log of Argo executionExample 4-10. Argo execution logExample 4-11. Deleting Argo executionuntitled_programlisting_10Example 4-12. Exporting Kubernetes clientExample 4-13. Obtaining pipeline experimentExample 4-14. Example recommender pipelineExample 4-15. Pipeline releaseExample 4-16. Load GCS download componentExample 4-17. Loading pipeline storage component from relative path and web linkExample 4-18. Importing required componentsExample 4-19. Functions implementationExample 4-20. Pipeline implementationExample 5-1. Downloading the mailing list dataExample 5-2. Data cleaningExample 5-3. Installing SpamAssassinExample 5-4. Writing and combining text-processing functions into featuresExample 5-5. Installing TFX and TFDVExample 5-6. Loading the componentsExample 5-7. Download recommender dataExample 5-8. Using CSV componentExample 5-9. Creating the schemaExample 5-10. Download the schema locallyExample 5-11. Display the schemaExample 5-12. Validating the dataExample 5-13. TFT importsExample 5-14. Creating the entry pointExample 5-15. Compute the vocabularyExample 5-16. Using the TFT componentExample 5-17. Adding SparkExample 5-18. Sample service definitionExample 5-19. Installing requirements and copying the applicationExample 5-20. Using the ResourceOp to launch a Spark jobExample 5-21. Launching your Spark sessionExample 5-22. Configuring your Spark sessionExample 5-23. Installing Python 3.6 in Spark’s worker containerExample 5-24. Configuring Spark to use MinIOExample 5-25. Reading our data’s Parquet-formatted outputExample 5-26. Specifying the schemaExample 5-27. Dropping recordsExample 5-28. Filtering out bad dataExample 5-29. Using Spark SQLExample 5-30. Saving to a persistent volumeExample 5-31. Writing to ParquetExample 5-32. Preparing features for the mailing listExample 5-33. Putting the functions togetherExample 5-34. Installing Scikit-learnExample 5-35. Specifying a containerExample 5-36. Using an entire notebook as data preparationExample 5-37. Using RUN to add Python dependencies to the containerExample 6-1. Required importsExample 6-2. Define a workspaceExample 6-3. Metadata exampleExample 6-4. Another metadata exampleExample 6-5. List all modelsExample 6-6. Basic lineageExample 6-7. Find the executionExample 6-8. Getting all related eventsExample 6-9. MLflow Tracking ServerExample 6-10. MLflow startup scriptExample 6-11. Installing MLflow server with HelmExample 6-12. Install requiredExample 6-13. Import required librariesExample 6-14. Set environment variablesExample 6-15. Create experimentExample 6-16. Sample KNN modelExample 6-17. Getting the runs for a given experimentExample 7-1. Setting up prerequisitesExample 7-2. Creating a TensorFlow sessionExample 7-3. DeepCollaborativeFiltering learningExample 7-4. Model creationExample 7-5. Setting Training configurationExample 7-6. Fitting modelExample 7-7. Model training resultsExample 7-8. Setting export destinationExample 7-9. Exporting the modelExample 7-10. TFJob Dockerfileuntitled_programlisting_11Example 7-11. Single-container TFJob exampleExample 7-12. Deploying TFJobExample 7-13. Viewing the state of TFJobExample 7-14. TF Recommender job descriptionExample 7-15. Distributed TFJob exampleuntitled_programlisting_12untitled_programlisting_13Example 7-16. TFJob execution resultExample 7-17. TFJob with GPU exampleExample 7-18. Pytorch Distributed Training ExampleExample 7-19. Feature preparationExample 7-20. Combining columns using column transformerExample 7-21. Data transformerExample 7-22. Using RandomForestClassifierExample 7-23. Evaluating training resultsExample 7-24. Training resultsExample 7-25. Defining the tabular anchorExample 7-26. Tabular anchorExample 7-27. Prediction calculationExample 7-28. Prediction calculation resultExample 7-29. Model explanationExample 7-30. Model explanation resultExample 7-31. Model explanationExample 7-32. Model explanation resultExample 7-33. Exporting modeluntitled_programlisting_14Example 8-1. Port-forwarding TFServing servicesuntitled_programlisting_15Example 8-2. TFServing Recommender model version statusuntitled_programlisting_16Example 8-3. Sending a request to your TFServing Recommender serviceExample 8-4. Output from your TFServing Recommender serviceExample 8-5. Helm install for a custom Seldon Core versionuntitled_programlisting_17Example 8-6. Seldon Core Istio GatewayExample 8-7. Simple Seldon Core prepackaged model serverExample 8-8. Simple Seldon Core custom language wrapperExample 8-9. Seldon Core Python model classuntitled_programlisting_18Example 8-10. Sending a request to your Seldon Core custom microserviceExample 8-11. Exposing Seldon Core microservice in a local Docker clientExample 8-12. Sending a request to your local Seldon Core microserviceuntitled_programlisting_19Example 8-13. SeldonMessage containing an ndarrayExample 8-14. SeldonMessage containing JSON dataExample 8-15. SeldonDeployment with Anchor ExplainersExample 8-16. Sending a prediction request to your Seldon Core movie sentiment modelExample 8-17. Prediction response from your Seldon Core movie sentiment modelExample 8-18. Sending an explanation request to your Seldon Core movie sentiment modelExample 8-19. Explanation response from your Seldon Core movie sentiment modelExample 8-20. SeldonDeployment for income predictorExample 8-21. Sending a prediction request to your Seldon Core income predictor modelExample 8-22. Prediction response from your Seldon Core income predictor modelExample 8-23. Sending a explanation request to your Seldon Core income predictor modelExample 8-24. Explanation response from your Seldon Core income predictor modeluntitled_programlisting_20untitled_programlisting_21untitled_programlisting_22untitled_programlisting_23untitled_programlisting_24Example 8-25. Simple sklearn KFServing InferenceServiceExample 8-26. Simple TensorFlow KFServing InferenceServiceExample 8-27. Simple PyTorch KFServing InferenceServiceExample 8-28. Sophisticated Canary KFServing InferenceServiceExample 8-29. KFServing-annotated MinIO secretExample 8-30. Service Account with attached MinIO secretExample 8-31. KFServing Recommender InferenceServiceuntitled_programlisting_25Example 8-32. Sending a prediction request to your KFServing Recommender InferenceServiceuntitled_programlisting_26Example 8-33. Custom target concurrency via annotations in KFServing InferenceServiceuntitled_programlisting_27Example 8-34. KafkaSource that sends events to a KFServing Recommender InferenceServiceExample 9-1. Lightweight Python function converts DICOMs to tensorsExample 9-2. Decomposing a CT scan with Spark and MahoutExample 9-3. Helper function to download a directory from GCSExample 9-4. Helper function to read Mahout DRMs into NumPy matricesExample 9-5. A loop to write several imagesExample 9-6. Spark operation manifestExample 9-7. CT scan denoising pipelineuntitled_programlisting_28untitled_programlisting_29untitled_programlisting_30untitled_programlisting_31untitled_programlisting_32untitled_programlisting_33Example 10-1. Example experiment specuntitled_programlisting_34untitled_programlisting_35Example 10-2. Example experiment outputExample 10-3. Distributed training exampleExample 10-4. Example NAS experiment specExample B-1. Changing the pipeline to use DataflowFigure P-1. Timbit the dogFigure P-2. Tina the catFigure P-3. Apache and MeowskaFigure 1-1. Model development life cycleFigure 1-2. Jupyter notebook running in KubeflowFigure 1-3. A Kubeflow pipelineFigure 2-1. Kubeflow web UIFigure 2-2. Pipelines web UIFigure 2-3. Pipeline detail pageFigure 2-4. Handwritten 3Figure 3-1. Kubeflow architectureFigure 3-2. The central dashboardFigure 3-3. Metadata diagramFigure 3-4. MinIO dashboardFigure 3-5. Istio architectureFigure 3-6. Knative architectureFigure 4-1. Kubeflow pipelines UI: prepackaged pipelinesFigure 4-2. Kubeflow pipelines UI: pipeline graph viewFigure 4-3. Kubeflow pipelines UI: pipeline viewFigure 4-4. Pipeline executionFigure 4-5. Argo UI for pipeline executionFigure 4-6. Argo UI execution graphFigure 4-7. Viewing Kubeflow pipelines in Argo UIFigure 4-8. Execution of recommender pipelines exampleFigure 4-9. Execution of conditional pipelines exampleFigure 4-10. Viewing conditional pipeline logFigure 4-11. Setting up periodic execution of a pipelineFigure 6-1. Accessing Metadata UIFigure 6-2. List of artifacts in the Artifact Store UIFigure 6-3. Artifact viewFigure 6-4. Overall architecture of MLflow components deploymentFigure
 6-5. MLflow main pageFigure 6-6. View of the individual runFigure 6-7. Run comparison viewFigure 6-8. Run metrics comparison viewFigure 8-1. TFServing architectureFigure 8-2. Seldon inference graph exampleFigure 8-3. Seldon graph examplesFigure 8-4. Seldon explainer componentFigure 8-5. Data science monitoring of models with Seldon Core and KnativeFigure 8-6. KFServing data planeFigure 8-7. KFServing infrastructure stackFigure 8-8. KFServing request flowFigure 9-1. Original slice of DICOMFigure 9-2. 1% denoised DICOM slice (left); 5% denoised DICOM slice (right)Figure 9-3. 10% denoised DICOM slice (left); .5% denoised DICOM slice (right)Figure 10-1. Katib system workflowFigure 10-2. Katib UI main pageFigure 10-3. Configuring a new experiment, part 1Figure 10-4. Configuring a new experiment, part 2Figure 10-5. Katib UI for an experimentFigure 10-6. Katib metrics for an experimentFigure 10-7. Metrics for each trialFigure C-1. Cloudflow architectureFigure C-2. Dynamically controlled stream patternFigure C-3. Using stream processing for batch serving implementationTable 3-1. Examples of ML Metadata operationsTable 6-1. List of modelsuntitled_table_1Table 6-2. Query result as a tableTable 8-1. Comparing embedded with MaaSTable 8-2. Comparing different model inference approachesTable 8-3. KFServing V1 data planeTable A-1. Argo and Kubernetes APIs
